<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3.2. 模型转换 &mdash; Horizon J5 AI Toolchain User Guide  文档</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom-style.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="3.3. 自定义算子开发" href="custom_op.html" />
    <link rel="prev" title="3.1. 概述" href="general_description.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Horizon J5 AI Toolchain User Guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">本手册目录结构:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../product_introduction.html">1. 产品介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_prepare.html">2. 环境部署</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../ptq_solution.html">3. PTQ浮点定点模型转换方案</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="general_description.html">3.1. 概述</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.2. 模型转换</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">3.2.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fp-model-preparation">3.2.2. 浮点模型准备</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-check">3.2.3. 验证模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hb-mapper-checker">3.2.3.1. 使用 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具验证模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">3.2.3.2. 检查异常处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="#check-result">3.2.3.3. 检查结果解读</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">3.2.3.4. 检查结果的调优指导</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-conversion">3.2.4. 转换模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hb-mapper-makertbin">3.2.4.1. 使用 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> 工具转换模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conversion-interpretation">3.2.4.2. 转换内部过程解读</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prepare-calibration-data">3.2.4.3. 准备校准数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">3.2.4.4. 转换结果解读</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conversion-output">3.2.4.5. 转换产出物解读</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#performance-evaluation">3.2.5. 模型性能分析与调优</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hb-perf">3.2.5.1. 使用 <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> 工具估计性能</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">3.2.5.2. 开发板实测性能</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-performance-optimization">3.2.5.3. 模型性能优化</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#accuracy-evaluation">3.2.6. 模型精度分析与调优</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id18">3.2.6.1. 模型精度分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="#accuracy-optimization">3.2.6.2. 精度调优</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qat">3.2.6.3. 使用QAT量化感知训练方案进一步提升模型精度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#op-restrictions">3.2.7. 算子约束</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-tools">3.2.8. 其他模型工具（可选）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id27">3.2.8.1. 模型打包</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id28">3.2.8.2. 模型信息查看</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bin">3.2.8.3. bin模型节点修改</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="custom_op.html">3.3. 自定义算子开发</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../application_development.html">4. runtime应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dsp.html">5. DSP开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix.html">6. 附录</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Horizon J5 AI Toolchain User Guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../ptq_solution.html"><span class="section-number">3. </span>PTQ浮点定点模型转换方案</a> &raquo;</li>
      <li><span class="section-number">3.2. </span>模型转换</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1><span class="section-number">3.2. </span>模型转换<a class="headerlink" href="#id1" title="永久链接至标题"></a></h1>
<section id="id2">
<h2><span class="section-number">3.2.1. </span>简介<a class="headerlink" href="#id2" title="永久链接至标题"></a></h2>
<p>模型转换是指将原始浮点模型转换为地平线混合异构模型的过程。
原始浮点模型（文中部分地方也称为浮点模型）是指您通过TensorFlow/PyTorch等等DL框架训练得到的
可用模型，这个模型的计算精度为float32；混合异构模型是一种适合在地平线芯片上运行的模型格式。
本章节将反复使用到这两种模型名词，为避免理解歧义，请先理解这个概念再阅读下文。</p>
<p>配合地平线工具链的模型完整开发过程，需要经过 <strong>浮点模型准备</strong>、<strong>模型检查</strong>、<strong>模型转换</strong>、
<strong>性能评估</strong> 和 <strong>精度评估</strong> 共五个重要阶段，如下图。</p>
<img alt="../_images/model_conversion_flowchart.png" class="align-center" src="../_images/model_conversion_flowchart.png" />
<p><strong>浮点模型准备</strong> 阶段的产出是输入到模型转换工具的浮点模型，
这些模型一般都是基于公开DL训练框架得到的，
需要您注意的是将模型导出为地平线工具支持的格式。
具体要求与建议请参考 <a class="reference internal" href="#fp-model-preparation"><span class="std std-ref">浮点模型准备</span></a>。</p>
<p><strong>模型检查</strong> 阶段用来确保算法模型是符合工具链要求的。
地平线提供了指定工具完成此阶段检查，对于不符合要求的情况，
检查工具会明确给出不符合要求的具体算子信息，方便您结合算子约束的说明将模型调整过来。
具体使用请参考 <a class="reference internal" href="#model-check"><span class="std std-ref">验证模型</span></a>。</p>
<p><strong>模型转换</strong> 阶段将完成浮点模型到地平线混合异构模型的转换。
为了模型能在地平线芯片上高效运行，地平线转换工具内部会完成模型优化、量化和编译等关键步骤，
地平线的量化方法经过了长期的技术与生产验证，在大部分典型模型上可以达到99%以上的精度保持效果。
具体使用请参考 <a class="reference internal" href="#model-conversion"><span class="std std-ref">转换模型</span></a>。</p>
<p><strong>性能评估</strong> 阶段提供了系列评估模型性能的工具。
在应用部署前，您可以使用这些工具验证模型性能是否达到应用要求。
对于部分性能不及预期的情况，也可以参考地平线提供的性能优化建议进行调优。
具体评估请参考 <a class="reference internal" href="#performance-evaluation"><span class="std std-ref">模型性能分析与调优</span></a>。</p>
<p><strong>精度评估</strong> 阶段提供了系列评估模型精度的工具。
大部分情况下，地平线转换后模型可以保持与原始浮点模型基本一致的精度效果，
在应用部署前，您可以使用地平线工具验证模型的精度是否符合预期。
对于部分精度不及预期的情况，也可以参考地平线提供的性能优化建议进行调优。
具体评估请参考 <a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">模型精度分析与调优</span></a>。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>通常在模型转换后就已经得到了可以上板的模型，
但是为了确保您得到的模型性能和精度都是符合应用要求的，
地平线强烈建议每次转换后都完成后续的性能评估与精度评估步骤。</p>
<p>模型转换过程会生成onnx模型，该模型均为中间产物，只是便于用户验证模型精度情况，因此不保证其在版本间的兼容性。若使用事例中的评测脚本对onnx模型单张或在测试集上进行评测时，请使用当前版本工具生成的onnx模型进行操作。</p>
</div>
</section>
<section id="fp-model-preparation">
<span id="id3"></span><h2><span class="section-number">3.2.2. </span>浮点模型准备<a class="headerlink" href="#fp-model-preparation" title="永久链接至标题"></a></h2>
<p>基于公开DL框架训练得到的浮点模型是转换工具的输入，目前转换工具支持的DL框架如下：</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 27%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 18%" />
<col style="width: 10%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>框架</strong></p></th>
<th class="head"><p>Caffe</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>TensorFlow</p></th>
<th class="head"><p>MXNet</p></th>
<th class="head"><p>其他框架</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>地平线工具链</strong></p></td>
<td><p>支持</p></td>
<td colspan="3"><p>支持（转ONNX）</p></td>
<td><p>请联系地平线</p></td>
</tr>
</tbody>
</table>
<p>以上框架中，Caffe导出的caffemodel是直接支持的；
PyTorch、TensorFlow和MXNet是通过转到ONNX实现间接支持，
ONNX目前主要支持的opset版本是opset10和opset11。</p>
<p>对于不同框架到ONNX的转换，目前都有对应的标准化方案，参考如下：</p>
<dl class="simple">
<dt>🔗 Pytorch2Onnx：PytTorch官方API支持直接将模型导出为ONNX模型，参考链接：</dt><dd><p><a class="reference external" href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html</a>。</p>
</dd>
<dt>🔗 Tensorflow2Onnx：基于ONNX社区的onnx/tensorflow-onnx 进行转换，参考链接：</dt><dd><p><a class="reference external" href="https://github.com/onnx/tensorflow-onnx">https://github.com/onnx/tensorflow-onnx</a>。</p>
</dd>
<dt>🔗 MXNet2Onnx：MXNet官方API支持直接将模型导出为ONNX模型，参考链接：</dt><dd><p><a class="reference external" href="https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Tests/OnnxConversionTest.cs">https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Tests/OnnxConversionTest.cs</a>。</p>
</dd>
<dt>🔗 更多框架的ONNX转换支持，参考链接：</dt><dd><p><a class="reference external" href="https://github.com/onnx/tutorials#converting-to-onnx-format">https://github.com/onnx/tutorials#converting-to-onnx-format</a>。</p>
</dd>
</dl>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<ul class="simple">
<li><p>目前转换工具仅支持输出个数小于或等于32的模型进行转换。</p></li>
<li><p>原始模型限制：ir_version≤7, opset=10或11，ir_version与onnx版本的对应关系请参考 <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/Versioning.md">onnx官方文档</a> 。</p></li>
</ul>
</div>
</section>
<section id="model-check">
<span id="id4"></span><h2><span class="section-number">3.2.3. </span>验证模型<a class="headerlink" href="#model-check" title="永久链接至标题"></a></h2>
<p>为了确保模型能顺利在地平线平台高效运行，模型中所使用的算子需要符合平台的算子约束。
算子约束部分给出了我们支持的具体算子，每个算子都给出了具体的参数限制，
具体详细信息请参考 <a class="reference internal" href="#op-restrictions"><span class="std std-ref">算子约束列表</span></a> 的内容
考虑到地平线支持的算子较多，为了避免人工逐条校对的麻烦，
我们提供了 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具用于验证模型所使用算子的支持情况。</p>
<section id="hb-mapper-checker">
<h3><span class="section-number">3.2.3.1. </span>使用 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具验证模型<a class="headerlink" href="#hb-mapper-checker" title="永久链接至标题"></a></h3>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hb_mapper</span> <span class="pre">checker</span> <span class="pre">工具的使用方式如下：</span></span></dt>
<dd><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_mapper checker --model-type <span class="si">${</span><span class="nv">model_type</span><span class="si">}</span> <span class="se">\</span>
                  --march <span class="si">${</span><span class="nv">march</span><span class="si">}</span> <span class="se">\</span>
                  --proto <span class="si">${</span><span class="nv">proto</span><span class="si">}</span> <span class="se">\</span>
                  --model <span class="si">${</span><span class="nv">caffe_model</span><span class="p">/onnx_model</span><span class="si">}</span> <span class="se">\</span>
                  --input-shape <span class="si">${</span><span class="nv">input_node</span><span class="si">}</span> <span class="si">${</span><span class="nv">input_shape</span><span class="si">}</span> <span class="se">\</span>
                  --output <span class="si">${</span><span class="nv">output</span><span class="si">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hb_mapper</span> <span class="pre">checker</span> <span class="pre">参数解释：</span></span></dt>
<dd><dl class="option-list">
<dt><kbd><span class="option">--model-type</span></kbd></dt>
<dd><p>用于指定检查输入的模型类型，目前只支持设置 <code class="docutils literal notranslate"><span class="pre">caffe</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">onnx</span></code>。</p>
</dd>
<dt><kbd><span class="option">--march</span></kbd></dt>
<dd><p>用于指定需要适配的AI芯片类型，可设置值为 <code class="docutils literal notranslate"><span class="pre">bernoulli2</span></code> 和 <code class="docutils literal notranslate"><span class="pre">bayes</span></code>，
分别对应X3&amp;J3和J5芯片，根据您需要适配的平台选择即可。</p>
</dd>
<dt><kbd><span class="option">--proto</span></kbd></dt>
<dd><p>此参数仅在 <code class="docutils literal notranslate"><span class="pre">model-type</span></code> 指定 <code class="docutils literal notranslate"><span class="pre">caffe</span></code> 时有效，取值为Caffe模型的prototxt文件名称。</p>
</dd>
<dt><kbd><span class="option">--model</span></kbd></dt>
<dd><p>在 <code class="docutils literal notranslate"><span class="pre">model-type</span></code> 被指定为 <code class="docutils literal notranslate"><span class="pre">caffe</span></code> 时，取值为Caffe模型的caffemodel文件名称。
在 <code class="docutils literal notranslate"><span class="pre">model-type</span></code>  被指定为 <code class="docutils literal notranslate"><span class="pre">onnx</span></code> 时，取值为ONNX模型文件名称。</p>
</dd>
<dt><kbd><span class="option">--input-shape</span></kbd></dt>
<dd><p>可选参数，明确指定模型的输入shape。
取值为 <code class="docutils literal notranslate"><span class="pre">{input_name}</span> <span class="pre">{NxHxWxC/NxCxHxW}</span></code> ，<code class="docutils literal notranslate"><span class="pre">input_name</span></code> 与shape之间以空格分隔。
例如模型输入名称为 <code class="docutils literal notranslate"><span class="pre">data1</span></code>，输入shape为 <code class="docutils literal notranslate"><span class="pre">[1,224,224,3]</span></code>，
则配置应该为 <code class="docutils literal notranslate"><span class="pre">--input_shape</span> <span class="pre">data1</span> <span class="pre">1x224x224x3</span></code>。
如果此处配置shape与模型内shape信息不一致，以此处配置为准。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>注意一个 <code class="docutils literal notranslate"><span class="pre">--input-shape</span></code> 只接受一个name和shape组合，如果您的模型有多个输入节点，
在命令中多次配置 <code class="docutils literal notranslate"><span class="pre">--input-shape</span></code> 参数即可。</p>
</div>
</dd>
<dt><kbd><span class="option">--output</span></kbd></dt>
<dd><p>该参数已经废弃, log信息默认存储于 <code class="docutils literal notranslate"><span class="pre">hb_mapper_checker.log</span></code> 中。</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id5">
<h3><span class="section-number">3.2.3.2. </span>检查异常处理<a class="headerlink" href="#id5" title="永久链接至标题"></a></h3>
<p>如果模型检查不通过，<code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具会报出ERROR。
在当前工作目录下会生成hb_mapper_checker.log文件，从文件中可以查看到具体的报错。
例如以下配置中含不可识别算子类型 <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code>：</p>
<div class="highlight-ProtoBuf notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;data&quot;</span>
<span class="w">  </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Input&quot;</span>
<span class="w">  </span><span class="n">top</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;data&quot;</span>
<span class="w">  </span><span class="n">input_param</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">shape</span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">dim</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">dim</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">dim</span><span class="o">:</span><span class="w"> </span><span class="mi">224</span><span class="w"> </span><span class="n">dim</span><span class="o">:</span><span class="w"> </span><span class="mi">224</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Convolution1&quot;</span>
<span class="w">  </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Convolution&quot;</span>
<span class="w">  </span><span class="n">bottom</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;data&quot;</span>
<span class="w">  </span><span class="n">top</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Convolution1&quot;</span>
<span class="w">  </span><span class="n">convolution_param</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">num_output</span><span class="o">:</span><span class="w"> </span><span class="mi">128</span>
<span class="w">    </span><span class="n">bias_term</span><span class="o">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">    </span><span class="n">pad</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="n">kernel_size</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">group</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">stride</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">weight_filler</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;msra&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;accuracy&quot;</span>
<span class="w">  </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Accuracy&quot;</span>
<span class="w">  </span><span class="n">bottom</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Convolution3&quot;</span>
<span class="w">  </span><span class="n">top</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;accuracy&quot;</span>
<span class="w">  </span><span class="n">include</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">phase</span><span class="o">:</span><span class="w"> </span><span class="n">TEST</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>使用 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 检查这个模型，您会在hb_mapper_checker.log中得到如下信息：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ValueError: Not support layer <span class="nv">name</span><span class="o">=</span>accuracy <span class="nv">type</span><span class="o">=</span>Accuracy
</pre></div>
</div>
</section>
<section id="check-result">
<span id="id6"></span><h3><span class="section-number">3.2.3.3. </span>检查结果解读<a class="headerlink" href="#check-result" title="永久链接至标题"></a></h3>
<p>如果不存在ERROR，则顺利通过校验。<code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具将直接输出如下信息：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">==============================================</span>
<span class="n">Node</span>         <span class="n">ON</span>   <span class="n">Subgraph</span>  <span class="n">Type</span>
<span class="o">----------------------------------------------</span>
<span class="n">conv1</span>        <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">conv2_1</span><span class="o">/</span><span class="n">dw</span>   <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">conv2_1</span><span class="o">/</span><span class="n">sep</span>  <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">conv2_2</span><span class="o">/</span><span class="n">dw</span>   <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">conv2_2</span><span class="o">/</span><span class="n">sep</span>  <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">conv3_1</span><span class="o">/</span><span class="n">dw</span>   <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">conv3_1</span><span class="o">/</span><span class="n">sep</span>  <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="o">...</span>
</pre></div>
</div>
<p>结果中每行都代表一个模型节点的check情况，每行含Node、ON、Subgraph和Type四列，
分别为节点名称、执行节点计算的硬件、节点所属子图和节点映射到的地平线内部实现名称。
如果模型在非输入和输出部分出现了CPU计算的算子，工具将把这个算子前后连续在BPU计算的部分拆分为两个Subgraph（子图）。</p>
</section>
<section id="id7">
<h3><span class="section-number">3.2.3.4. </span>检查结果的调优指导<a class="headerlink" href="#id7" title="永久链接至标题"></a></h3>
<p>在最理想的情况下，非输入和输出部分都应该在BPU上运行，也就是只有一个子图。
如果出现了CPU算子导致拆分多个子图， <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具会给出导致CPU算子出现的具体原因。
例如以下Caffe模型的出现了Reshape + Pow + Reshape 的结构, 从算子约束列表中我们可以看到, Reshape 算子目前为在CPU上运行的算子, 而POW的shape也是非4维的。</p>
<img alt="../_images/model_reshape.png" class="align-center" src="../_images/model_reshape.png" />
<p>因此模型最终检查结果也会出现分段情况, 如下:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">2022</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">25</span> <span class="mi">15</span><span class="p">:</span><span class="mi">16</span><span class="p">:</span><span class="mi">14</span><span class="p">,</span><span class="mi">667</span> <span class="n">INFO</span> <span class="n">The</span> <span class="n">converted</span> <span class="n">model</span> <span class="n">node</span> <span class="n">information</span><span class="p">:</span>
<span class="o">====================================================================================</span>
<span class="n">Node</span>                                    <span class="n">ON</span>   <span class="n">Subgraph</span>  <span class="n">Type</span>
<span class="o">-------------------------------------------------------------------------------------</span>
<span class="n">conv68</span>                                  <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">sigmoid16</span>                               <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzLut</span>
<span class="n">axpy_prod16</span>                             <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedMul</span>
<span class="n">UNIT_CONV_FOR_eltwise_layer16_add_1</span>     <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">prelu49</span>                                 <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzPRelu</span>
<span class="n">fc1</span>                                     <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">fc1_reshape_0</span>                           <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Reshape</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">square</span>                        <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Pow</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">sum_pre_reshape</span>               <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Reshape</span>
<span class="n">fc_output</span><span class="o">/</span><span class="nb">sum</span>                           <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">sum_reshape_0</span>                 <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Reshape</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">sqrt</span>                          <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Pow</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">expand_pre_reshape</span>            <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Reshape</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">expand</span>                        <span class="n">BPU</span>  <span class="nb">id</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>     <span class="n">HzSQuantizedConv</span>
<span class="n">fc1_reshape_1</span>                           <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Reshape</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">expand_reshape_0</span>              <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Reshape</span>
<span class="n">fc_output</span><span class="o">/</span><span class="n">op</span>                            <span class="n">CPU</span>  <span class="o">--</span>        <span class="n">Mul</span>
</pre></div>
</div>
<p>根据 hb_mapper checker 给出的提示，一般来说算子运行在BPU上会有更好的性能表现。
<strong>当然，多个子图也不会影响整个转换流程，但会较大程度地影响模型性能，建议尽量调整至全BPU执行。</strong></p>
</section>
</section>
<section id="model-conversion">
<span id="id8"></span><h2><span class="section-number">3.2.4. </span>转换模型<a class="headerlink" href="#model-conversion" title="永久链接至标题"></a></h2>
<p>转换模型阶段会完成浮点模型到地平线混合异构模型的转换，经过这个阶段，您将得到一个可以在地平线芯片上运行的模型。
在进行转换之前，请确保已经顺利通过了验证模型小节的过程。</p>
<p>模型转换使用 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> 工具完成，转换期间会完成模型优化和校准量化等重要过程，校准需要依照模型预处理要求准备校准数据。
为了方便您全面了解模型转换，本节将依次介绍转换工具使用、校准数据准备、转换内部过程解读、转换结果解读和转换产出物解读。</p>
<section id="hb-mapper-makertbin">
<span id="makertbin"></span><h3><span class="section-number">3.2.4.1. </span>使用 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> 工具转换模型<a class="headerlink" href="#hb-mapper-makertbin" title="永久链接至标题"></a></h3>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hb_mapper</span> <span class="pre">makertbin命令使用方式如下：</span></span></dt>
<dd><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_mapper makertbin --config <span class="si">${</span><span class="nv">config_file</span><span class="si">}</span>  <span class="se">\</span>
                    --model-type  <span class="si">${</span><span class="nv">model_type</span><span class="si">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hb_mapper</span> <span class="pre">makertbin参数解释：</span></span></dt>
<dd><dl class="option-list">
<dt><kbd><span class="option">--model-type</span></kbd></dt>
<dd><p>用于指定转换输入的模型类型，目前支持设置 <code class="docutils literal notranslate"><span class="pre">caffe</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">onnx</span></code>。</p>
</dd>
<dt><kbd><span class="option">--config</span></kbd></dt>
<dd><p>模型编译的配置文件，内容采用yaml格式，文件名使用.yaml后缀。一份完整的配置文件模板如下：</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>此处配置文件仅作展示，在实际模型配置文件中 <code class="docutils literal notranslate"><span class="pre">caffe_model</span></code> 与 <code class="docutils literal notranslate"><span class="pre">onnx_model</span></code> 两个参数只存在其中之一。</p>
<p>即，要么是Caffe模型，要么是ONNX模型。</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 模型参数组</span>
<span class="n">model_parameters</span><span class="p">:</span>
  <span class="c1"># 原始Caffe浮点模型描述文件</span>
  <span class="n">prototxt</span><span class="p">:</span> <span class="s1">&#39;***.prototxt&#39;</span>

  <span class="c1"># 原始Caffe浮点模型数据模型文件</span>
  <span class="n">caffe_model</span><span class="p">:</span> <span class="s1">&#39;****.caffemodel&#39;</span>

  <span class="c1"># 原始Onnx浮点模型文件</span>
  <span class="n">onnx_model</span><span class="p">:</span> <span class="s1">&#39;****.onnx&#39;</span>

  <span class="c1"># 转换的目标AI芯片架构</span>
  <span class="n">march</span><span class="p">:</span> <span class="s1">&#39;bayes&#39;</span>

  <span class="c1"># 模型转换输出的用于上板执行的模型文件的名称前缀</span>
  <span class="n">output_model_file_prefix</span><span class="p">:</span> <span class="s1">&#39;mobilenetv1&#39;</span>

  <span class="c1"># 模型转换输出的结果的存放目录</span>
  <span class="n">working_dir</span><span class="p">:</span> <span class="s1">&#39;./model_output_dir&#39;</span>

  <span class="c1"># 指定转换后混合异构模型是否保留输出各层的中间结果的能力</span>
  <span class="n">layer_out_dump</span><span class="p">:</span> <span class="kc">False</span>

<span class="c1"># 输入信息参数组</span>
<span class="n">input_parameters</span><span class="p">:</span>
  <span class="c1"># 原始浮点模型的输入节点名称</span>
  <span class="n">input_name</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span>

  <span class="c1"># 原始浮点模型的输入数据格式（数量/顺序与input_name一致）</span>
  <span class="n">input_type_train</span><span class="p">:</span> <span class="s1">&#39;bgr&#39;</span>

  <span class="c1"># 原始浮点模型的输入数据排布（数量/顺序与input_name一致）</span>
  <span class="n">input_layout_train</span><span class="p">:</span> <span class="s1">&#39;NCHW&#39;</span>

  <span class="c1"># 原始浮点模型的输入数据尺寸</span>
  <span class="n">input_shape</span><span class="p">:</span> <span class="s1">&#39;1x3x224x224&#39;</span>

  <span class="c1"># 网络实际执行时，输入给网络的batch_size, 默认值为1</span>
  <span class="n">input_batch</span><span class="p">:</span> <span class="mi">1</span>

  <span class="c1"># 在模型中添加的输入数据预处理方法</span>
  <span class="n">norm_type</span><span class="p">:</span> <span class="s1">&#39;data_mean_and_scale&#39;</span>

  <span class="c1"># 预处理方法的图像减去的均值, 如果是通道均值，value之间必须用空格分隔</span>
  <span class="n">mean_value</span><span class="p">:</span> <span class="s1">&#39;103.94 116.78 123.68&#39;</span>

  <span class="c1"># 预处理方法的图像缩放比例，如果是通道缩放比例，value之间必须用空格分隔</span>
  <span class="n">scale_value</span><span class="p">:</span> <span class="s1">&#39;0.017&#39;</span>

  <span class="c1"># 转换后混合异构模型需要适配的输入数据格式（数量/顺序与input_name一致）</span>
  <span class="n">input_type_rt</span><span class="p">:</span> <span class="s1">&#39;yuv444&#39;</span>

  <span class="c1"># 输入数据格式的特殊制式</span>
  <span class="n">input_space_and_range</span><span class="p">:</span> <span class="s1">&#39;regular&#39;</span>

  <span class="c1"># 转换后混合异构模型需要适配的输入数据排布（数量/顺序与input_name一致），若input_type_rt配置为nv12，则此处参数不需要配置</span>
  <span class="n">input_layout_rt</span><span class="p">:</span> <span class="s1">&#39;NHWC&#39;</span>

<span class="c1"># 校准参数组</span>
<span class="n">calibration_parameters</span><span class="p">:</span>
  <span class="c1"># 模型校准使用的标定样本的存放目录</span>
  <span class="n">cal_data_dir</span><span class="p">:</span> <span class="s1">&#39;./calibration_data&#39;</span>

  <span class="c1"># 开启图片校准样本自动处理（skimage read; resize到输入节点尺寸）</span>
  <span class="c1">#preprocess_on: False</span>

  <span class="c1"># 校准使用的算法类型</span>
  <span class="n">calibration_type</span><span class="p">:</span> <span class="s1">&#39;kl&#39;</span>

  <span class="c1"># max 校准方式的参数</span>
  <span class="n">max_percentile</span><span class="p">:</span> <span class="mf">1.0</span>

  <span class="c1"># 强制指定OP在CPU上运行</span>
  <span class="n">run_on_cpu</span><span class="p">:</span>  <span class="p">{</span><span class="n">OP_name</span><span class="p">}</span>

  <span class="c1"># 强制指定OP在BPU上运行</span>
  <span class="n">run_on_bpu</span><span class="p">:</span>  <span class="p">{</span><span class="n">OP_name</span><span class="p">}</span>

<span class="c1"># 编译参数组</span>
<span class="n">compiler_parameters</span><span class="p">:</span>
  <span class="c1"># 编译策略选择</span>
  <span class="n">compile_mode</span><span class="p">:</span> <span class="s1">&#39;latency&#39;</span>

  <span class="c1"># 是否打开编译的debug信息</span>
  <span class="n">debug</span><span class="p">:</span> <span class="kc">False</span>

  <span class="c1"># 模型运行核心数</span>
  <span class="c1"># core_num: 1</span>

  <span class="c1"># 模型编译的优化等级选择</span>
  <span class="n">optimize_level</span><span class="p">:</span> <span class="s1">&#39;O2&#39;</span>

<span class="n">custom_op</span><span class="p">:</span>
  <span class="c1"># 自定义op的校准方式, 推荐使用注册方式 register</span>
  <span class="n">custom_op_method</span><span class="p">:</span> <span class="n">register</span>

  <span class="c1"># 自定义OP的实现文件, 多个文件可用&quot;;&quot;分隔, 该文件可由模板生成, 详情见自定义OP相关文档</span>
  <span class="n">op_register_files</span><span class="p">:</span> <span class="n">sample_custom</span><span class="o">.</span><span class="n">py</span>

  <span class="c1"># 自定义OP实现文件所在的文件夹, 请使用相对路径</span>
  <span class="n">custom_op_dir</span><span class="p">:</span> <span class="o">./</span><span class="n">custom_op</span>
</pre></div>
</div>
<p>配置文件主要包含模型参数组、输入信息参数组、校准参数组和编译参数组。
在您的配置文件中，四个参数组位置都需要存在，具体参数分为可选和必选，可选参数可以不配置。
具体参数的设置形式为：<code class="docutils literal notranslate"><span class="pre">param_name:</span>&#160; <span class="pre">'param_value'</span></code> ，参数存在多个值时使用 <code class="docutils literal notranslate"><span class="pre">';'</span></code> 符号分隔：
<code class="docutils literal notranslate"><span class="pre">param_name:</span>&#160; <span class="pre">'param_value1;</span> <span class="pre">param_value2;</span> <span class="pre">param_value3'</span></code> 。</p>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p>当模型为多输入模型时, 强烈建议用户将可选参数们(<code class="docutils literal notranslate"><span class="pre">input_name</span></code>, <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> 等)显式的写出, 以免造成参数对应顺序上的错误。</p>
</div>
<p>以下是具体参数信息，参数会比较多，我们依照上述的参数组次序介绍。</p>
<p>🛠️ <strong>模型参数组</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 28%" />
<col style="width: 60%" />
<col style="width: 8%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>编</p>
<p>号</p>
</td>
<td><p>参数名称</p></td>
<td><p>参数配置说明</p></td>
<td><blockquote>
<div><p>可选/</p>
</div></blockquote>
<p>必选</p>
</td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">prototxt</span></code></p></td>
<td><p><strong>参数作用</strong>：指定Caffe浮点模型的prototxt文件名称。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：在 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> 的</p>
<p><code class="docutils literal notranslate"><span class="pre">model-type</span></code> 为 <code class="docutils literal notranslate"><span class="pre">caffe</span></code> 时必须配置。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">caffe_model</span></code></p></td>
<td><p><strong>参数作用</strong>：指定Caffe浮点模型的caffemodel文件名称。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：在 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> 的</p>
<p><code class="docutils literal notranslate"><span class="pre">model-type</span></code> 为 <code class="docutils literal notranslate"><span class="pre">caffe</span></code> 时必须配置。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">onnx_model</span></code></p></td>
<td><p><strong>参数作用</strong>：指定ONNX浮点模型的onnx文件名称。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：在 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> 的</p>
<p><code class="docutils literal notranslate"><span class="pre">model-type</span></code> 为 <code class="docutils literal notranslate"><span class="pre">onnx</span></code> 时必须配置。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">march</span></code></p></td>
<td><p><strong>参数作用</strong>：指定产出混合异构模型需要支持的平台架构。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">bernoulli2</span></code> 或 <code class="docutils literal notranslate"><span class="pre">bayes</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">bayes</span></code>。</p>
<p><strong>参数说明</strong>： 两个可选配置值依次对应X3&amp;J3和J5芯片，</p>
<p>根据您使用的平台选择。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">output_model_file_prefix</span></code></p></td>
<td><p><strong>参数作用</strong>：指定转换产出混合异构模型的名称前缀。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：输出的定点模型文件的名称前缀。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">working_dir</span></code></p></td>
<td><p><strong>参数作用</strong>：指定模型转换输出的结果的存放目录。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：<code class="docutils literal notranslate"><span class="pre">model_output</span></code>。</p>
<p><strong>参数说明</strong>：若该目录不存在, 则工具会自动创建目录。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">layer_out_dump</span></code></p></td>
<td><p><strong>参数作用</strong>：指定混合异构模型是否保留输出中间层值的能力。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">True</span></code> 、 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>默认配置</strong>：<code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>参数说明</strong>：输出中间层的值是调试需要用到的手段，</p>
<p>常规状态下请不要开启。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">output_nodes</span></code></p></td>
<td><p><strong>参数作用</strong>：指定模型的输出节点。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：一般情况下，转换工具会自动识别模型的输出节点。</p>
<p>此参数用于支持您指定一些中间层次作为输出。</p>
<p>设置值为模型中的具体节点名称，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
<p>需要您注意的是，一旦设置此参数后，工具将不再自动识别输出节点，</p>
<p>您通过此参数指定的节点就是全部的输出。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remove_node_type</span></code></p></td>
<td><p><strong>参数作用</strong>：设置删除节点的类型。</p>
<p><strong>取值范围</strong>：”Quantize”, “Transpose”,
“Dequantize”, “Cast”, “Reshape”, “Softmax”。不同类型用”;”分割。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：该参数为隐藏参数，
不设置或设置为空不影响模型转换过程。
此参数用于支持您设置待删除节点的类型信息。</p>
<p>被删除的节点必须在模型的开头或者末尾, 与模型的输入或输出连接。</p>
<p>注意：待删除节点会按顺序依次删除，并动态更新模型结构；</p>
<p>同时在节点删除前还会判断该节点是否位于模型的输入输出处。</p>
<p>因此节点的删除顺序很重要。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remove_node_name</span></code></p></td>
<td><p><strong>参数作用</strong>：设置删除节点的名称。</p>
<p><strong>取值范围</strong>：无。不同类型用”;”分割。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：该参数为隐藏参数，
不设置或设置为空不影响模型转换过程。
此参数用于支持您设置待删除节点的名称。</p>
<p>被删除的节点必须在模型的开头或者末尾, 与模型的输入或输出连接。</p>
<p>注意：待删除节点会按顺序依次删除，并动态更新模型结构；</p>
<p>同时在节点删除前还会判断该节点是否位于模型的输入输出处。</p>
<p>因此节点的删除顺序很重要。</p>
</td>
<td><p>可选</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>输入信息参数组</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 3%" />
<col style="width: 23%" />
<col style="width: 68%" />
<col style="width: 7%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>编</p>
<p>号</p>
</td>
<td><p>参数名称</p></td>
<td><p>参数配置说明</p></td>
<td><blockquote>
<div><p>可选/</p>
</div></blockquote>
<p>必选</p>
</td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_name</span></code></p></td>
<td><p><strong>参数作用</strong>：指定原始浮点模型的输入节点名称。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：浮点模型只有一个输入节点情况时不需要配置，</p>
<p>多于一个输入节点时必须配置以保证后续类型及校准数据输入顺序的准确性。</p>
<p>多个值的配置方法请参考前文对param_value配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_type_train</span></code></p></td>
<td><p><strong>参数作用</strong>：指定原始浮点模型的输入数据类型。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">rgb</span></code> 、 <code class="docutils literal notranslate"><span class="pre">bgr</span></code> 、 <code class="docutils literal notranslate"><span class="pre">yuv444</span></code> 、 <code class="docutils literal notranslate"><span class="pre">gray</span></code> 、 <code class="docutils literal notranslate"><span class="pre">featuremap</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：每一个输入节点都需要配置一个确定的输入数据类型，</p>
<p>存在多个输入节点时，设置的节点顺序需要与</p>
<p><code class="docutils literal notranslate"><span class="pre">input_name</span></code> 里的顺序严格保持一致。</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
<p>数据类型的选择请参考下文</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a></p>
<p>部分的介绍。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code></p></td>
<td><p><strong>参数作用</strong>：指定原始浮点模型的输入数据排布。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">NHWC</span></code> 、 <code class="docutils literal notranslate"><span class="pre">NCHW</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：每一个输入节点都需要配置一个确定的输入数据排布，</p>
<p>这个排布必须与原始浮点模型所采用的数据排布相同。存在多个输入节点时，</p>
<p>设置的节点顺序需要与 <code class="docutils literal notranslate"><span class="pre">input_name</span></code> 里的顺序严格保持一致。</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
<p>什么是数据排布请参考下文</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a></p>
<p>部分的介绍。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code></p></td>
<td><p><strong>参数作用</strong>：转换后混合异构模型需要适配的输入数据格式。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">rgb</span></code> 、 <code class="docutils literal notranslate"><span class="pre">bgr</span></code> 、 <code class="docutils literal notranslate"><span class="pre">yuv444</span></code> 、</p>
<p><code class="docutils literal notranslate"><span class="pre">nv12</span></code> 、 <code class="docutils literal notranslate"><span class="pre">gray</span></code> 、 <code class="docutils literal notranslate"><span class="pre">featuremap</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：这里是指明您需要使用的数据格式，</p>
<p>不要求与原始模型的数据格式一致，</p>
<p>但是需要注意在边缘平台喂给模型的数据是使用这个格式。</p>
<p>每一个输入节点都需要配置一个确定的输入数据类型，存在多个输入节点时，</p>
<p>设置的节点顺序需要与 <code class="docutils literal notranslate"><span class="pre">input_name</span></code> 里的顺序严格保持一致。</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
<p>数据类型的选择请参考下文</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a></p>
<p>部分的介绍。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code></p></td>
<td><p><strong>参数作用</strong>：转换后混合异构模型需要适配的输入数据排布。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> 、 <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：每一个输入节点都需要配置一个确定的输入数据排布，</p>
<p>这个输入是您希望给混合异构模型指定的排布。</p>
<p>不合适的输入数据的排布设置将会影响性能，</p>
<blockquote>
<div><p>J5平台建议用户使用 NHWC 格式输入。</p>
</div></blockquote>
<p>存在多个输入节点时，设置的节点顺序需要与</p>
<p><code class="docutils literal notranslate"><span class="pre">input_name</span></code> 里的顺序严格保持一致。</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
<p>什么是数据排布请参考下文</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a></p>
<p>部分的介绍。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_space_and_range</span></code></p></td>
<td><p><strong>参数作用</strong>：指定输入数据格式的特殊制式。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">regular</span></code> , <code class="docutils literal notranslate"><span class="pre">bt601_video</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">regular</span></code>。</p>
<p><strong>参数说明</strong>：这个参数是为了适配不同ISP输出的yuv420格式，</p>
<p>在相应 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 为 <code class="docutils literal notranslate"><span class="pre">nv12</span></code> 时，该配置才有效。</p>
<p><code class="docutils literal notranslate"><span class="pre">regular</span></code> 就是常见的yuv420格式，数值范围为 <code class="docutils literal notranslate"><span class="pre">[0,255]</span></code>；</p>
<p><code class="docutils literal notranslate"><span class="pre">bt601_video</span></code> 是另一种视频制式yuv420，数值范围为 <code class="docutils literal notranslate"><span class="pre">[16,235]</span></code>。</p>
<p>更多信息可以通过网络资料了解bt601，</p>
<p>在没有明确需要的情况下，您不要配置此参数。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code></p></td>
<td><p><strong>参数作用</strong>：指定原始浮点模型的输入数据尺寸。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：shape的几个维度以 <code class="docutils literal notranslate"><span class="pre">x</span></code> 连接，例如 <code class="docutils literal notranslate"><span class="pre">1x3x224x224</span></code>。</p>
<p>原始浮点模型只有一个输入节点情况时可以不配置，</p>
<p>工具会自动读取模型文件中的尺寸信息。</p>
<p>配置多个输入节点时，设置的节点顺序需要与 <code class="docutils literal notranslate"><span class="pre">input_name</span></code></p>
<p>里的顺序严格保持一致，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_batch</span></code></p></td>
<td><p><strong>参数作用</strong>：指定转换后混合异构模型需要适配的输入batch数量。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">1-4096</span></code>。</p>
<p><strong>默认配置</strong>：<code class="docutils literal notranslate"><span class="pre">1</span></code>。</p>
<p><strong>参数说明</strong>：这里input_batch为转换后混合异构bin模型输入batch数量，</p>
<p>但不影响转换后onnx的模型的输入batch数量。</p>
<p>此参数不配置时默认为1。</p>
<p>配置多个输入节点时，设置的节点顺序需要与 <code class="docutils literal notranslate"><span class="pre">input_name</span></code></p>
<p>里的顺序严格保持一致，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">norm_type</span></code></p></td>
<td><p><strong>参数作用</strong>：在模型中添加的输入数据预处理方法。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code> 、 <code class="docutils literal notranslate"><span class="pre">data_mean</span></code> 、</p>
<p><code class="docutils literal notranslate"><span class="pre">data_scale</span></code> 、 <code class="docutils literal notranslate"><span class="pre">no_preprocess</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>： <code class="docutils literal notranslate"><span class="pre">no_preprocess</span></code> 表示不添加任何数据预处理；</p>
<p><code class="docutils literal notranslate"><span class="pre">data_mean</span></code> 表示提供减均值预处理；</p>
<p><code class="docutils literal notranslate"><span class="pre">data_scale</span></code> 表示提供乘scale系数预处理；</p>
<p><code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code> 表示提供先减均值再乘scale系数前处理。</p>
<p>输入节点时多于一个时，设置的节点顺序需要与 <code class="docutils literal notranslate"><span class="pre">input_name</span></code></p>
<p>里的顺序严格保持一致，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
<p>配置该参数的影响请参考下文</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a></p>
<p>部分的介绍。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mean_value</span></code></p></td>
<td><p><strong>参数作用</strong>：指定预处理方法的图像减去的均值。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：当 <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> 存在 <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code></p>
<p>或 <code class="docutils literal notranslate"><span class="pre">data_mean</span></code> 时需要配置该参数。</p>
<p>对于每一个输入节点而言，存在两种配置方式。</p>
<p>第一种是仅配置一个数值，表示所有通道都减去这个均值；</p>
<p>第二种是提供与通道数量一致的数值（这些数值以空格分隔开），</p>
<p>表示每个通道都会减去不同的均值。</p>
<p>配置的输入节点数量必须与 <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> 配置的节点数量一致，</p>
<p>如果存在某个节点不需要 <code class="docutils literal notranslate"><span class="pre">mean</span></code> 处理，则为该节点配置 <code class="docutils literal notranslate"><span class="pre">'None'</span></code>。</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">scale_value</span></code></p></td>
<td><p><strong>参数作用</strong>：指定预处理方法的数值scale系数。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：当 <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> 存在 <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code> 或</p>
<p><code class="docutils literal notranslate"><span class="pre">data_scale</span></code> 时需要配置该参数。</p>
<p>对于每一个输入节点而言，存在两种配置方式。</p>
<p>第一种是仅配置一个数值，表示所有通道都乘以这个系数；</p>
<p>第二种是提供与通道数量一致的数值（这些数值以空格分隔开），</p>
<p>表示每个通道都会乘以不同的系数。</p>
<p>配置的输入节点数量必须与 <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> 配置的节点数量一致，</p>
<p>如果存在某个节点不需要 <code class="docutils literal notranslate"><span class="pre">scale</span></code> 处理，则为该节点配置 <code class="docutils literal notranslate"><span class="pre">'None'</span></code>。</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>校准参数组</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 21%" />
<col style="width: 67%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>编</p>
<p>号</p>
</th>
<th class="head"><p>参数名称</p></th>
<th class="head"><p>参数配置说明</p></th>
<th class="head"><blockquote>
<div><p>可选/</p>
</div></blockquote>
<p>必选</p>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cal_data_dir</span></code></p></td>
<td><p><strong>参数作用</strong>：指定模型校准使用的标定样本的存放目录。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：目录内校准数据需要符合输入配置的要求，</p>
<p>具体请参考 <a class="reference internal" href="#prepare-calibration-data"><span class="std std-ref">准备校准数据</span></a></p>
<p>部分的介绍。配置多个输入节点时，</p>
<p>设置的节点顺序需要与 <code class="docutils literal notranslate"><span class="pre">input_name</span></code> 里的顺序严格保持一致，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code></p></td>
<td><p><strong>参数作用</strong>：开启图片校准样本自动处理。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">True</span></code> 、 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>参数说明</strong>：在启动该功能时，</p>
<p><cite>cal_data_dir</cite> 目录下存放的都是jpg/bmp/png等图片数据，</p>
<p>工具会使用skimage读取图片，</p>
<p>并resize到输入节点需要的尺寸。</p>
<p>为了保证校准的效果，建议您保持该参数关闭。</p>
<p>使用的影响请参考 <a class="reference internal" href="#prepare-calibration-data"><span class="std std-ref">准备校准数据</span></a></p>
<p>部分的介绍。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">calibration_type</span></code></p></td>
<td><p><strong>参数作用</strong>：校准使用的算法类型。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">default</span></code>、 <code class="docutils literal notranslate"><span class="pre">kl</span></code> 、 <code class="docutils literal notranslate"><span class="pre">max</span></code> 和 <code class="docutils literal notranslate"><span class="pre">load</span></code>。</p>
<p><strong>默认配置</strong>：<code class="docutils literal notranslate"><span class="pre">default</span></code>。</p>
<p><strong>参数说明</strong>： <code class="docutils literal notranslate"><span class="pre">kl</span></code> 和 <code class="docutils literal notranslate"><span class="pre">max</span></code> 都是公开的校准量化算法，</p>
<p>其基本原理可以通过网络资料查阅。</p>
<p>使用 <code class="docutils literal notranslate"><span class="pre">load</span></code> 方式校准时, qat模型必须是通过horizon_nn提供的</p>
<p>export_to_onnx来导出的模型。详情参见</p>
<p><a class="reference internal" href="#qat-accuracy"><span class="std std-ref">使用QAT量化感知训练方案进一步提升模型精度</span></a> 。</p>
<p><code class="docutils literal notranslate"><span class="pre">default</span></code> 是一个自动搜索的策略，</p>
<p>会尝试从系列校准量化参数中获得一个相对效果较好的组合。</p>
<p>如果您使用的是QAT导出的模型，则应选择 <code class="docutils literal notranslate"><span class="pre">load</span></code>。</p>
<p>建议您先尝试 <code class="docutils literal notranslate"><span class="pre">default</span></code>，</p>
<p>如果最终的精度结果不满足预期</p>
<p>再根据 <a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">精度调优</span></a></p>
<p>部分建议配置不同的校准参数。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">max_percentile</span></code></p></td>
<td><p><strong>参数作用</strong>：该参数为 <code class="docutils literal notranslate"><span class="pre">max</span></code> 校准方法的参数，</p>
<p>用以调整 <code class="docutils literal notranslate"><span class="pre">max</span></code> 校准的截取点。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">0.0</span></code> ~ <code class="docutils literal notranslate"><span class="pre">1.0</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">1.0</span></code>。</p>
<p><strong>参数说明</strong>：此参数仅在 <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> 为 <code class="docutils literal notranslate"><span class="pre">max</span></code> 时有效，</p>
<p>常用配置选项有：0.99999/0.99995/0.99990/0.99950/0.99900。</p>
<p>建议您先尝试 <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> 配置 <code class="docutils literal notranslate"><span class="pre">default</span></code>，</p>
<p>如果最终的精度结果不满足预期</p>
<p>再根据 <a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">精度调优</span></a> 部分建议调整该参数。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">per_channel</span></code></p></td>
<td><p><strong>参数作用</strong>：控制是否针对featuremap的每个channel进行校准。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">True</span></code> 、 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>参数说明</strong>： <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> 设置非default时有效。</p>
<p>建议您先尝试 <code class="docutils literal notranslate"><span class="pre">default</span></code>，</p>
<p>如果最终的精度结果不满足预期</p>
<p>再根据 <a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">精度调优</span></a></p>
<p>部分建议调整该参数。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_on_cpu</span></code></p></td>
<td><p><strong>参数作用</strong>：强制指定算子在CPU上运行。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：CPU上虽然性能不及BPU，但是提供的是float精度计算，</p>
<p>如果您确定某些算子需要在CPU上计算，</p>
<p>可以通过该参数指定。</p>
<p>设置值为模型中的具体节点名称，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_on_bpu</span></code></p></td>
<td><p><strong>参数作用</strong>：强制指定OP在BPU上运行。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：为了保证最终量化模型的精度，少部分情况下，</p>
<p>转换工具会将一些具备BPU计算条件的算子放在CPU上运行。</p>
<p>如果您对性能有较高的要求，愿意以更多一些量化损失为代价，</p>
<p>则可以通过该参数明确指定算子运行在BPU上。</p>
<p>设置值为模型中的具体节点名称，</p>
<p>多个值的配置方法请参考前文对 <code class="docutils literal notranslate"><span class="pre">param_value</span></code> 配置描述。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cal_data_type</span></code></p></td>
<td><p><strong>参数作用</strong>：指定校准数据二进制文件的数据存储类型。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">float32</span></code>、<code class="docutils literal notranslate"><span class="pre">uint8</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：指定模型校准时使用的二进制文件的数据存储类型。</p>
<p>没有指定值的情况下将会使用文件夹名字后缀来做判断。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optimization</span></code></p></td>
<td><p><strong>参数作用</strong>：使模型以 int8/int16 格式输出。</p>
<p><strong>取值范围</strong>：[‘set_model_output_int8’]或者
[‘set_model_output_int16’]。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：</p>
<p>指定值为set_model_output_int8时，设置模型为 int8 格式低精度输出；</p>
<p>指定值为set_model_output_int16时，设置模型为 int16 格式低精度输出。</p>
</td>
<td><p>可选</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>编译参数组</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 23%" />
<col style="width: 66%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>编</p>
<p>号</p>
</th>
<th class="head"><p>参数名称</p></th>
<th class="head"><p>参数配置说明</p></th>
<th class="head"><p>可选/</p>
<p>必选</p>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">compile_mode</span></code></p></td>
<td><p><strong>参数作用</strong>：编译策略选择。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">latency</span></code>、 <code class="docutils literal notranslate"><span class="pre">bandwidth</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">latency</span></code>。</p>
<p><strong>参数说明</strong>： <code class="docutils literal notranslate"><span class="pre">latency</span></code> 以优化推理时间为目标；</p>
<p><code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> 以优化ddr的访问带宽为目标。</p>
<p>如果模型没有严重超过预期的带宽占用，建议您使用 <code class="docutils literal notranslate"><span class="pre">latency</span></code> 策略。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">debug</span></code></p></td>
<td><p><strong>参数作用</strong>：是否打开编译的debug信息。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">True</span></code> 、 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p><strong>参数说明</strong>：开启该参数情况下，</p>
<p>编译后模型将附带一些调试信息，</p>
<p>用于支持后续的调优分析过程。</p>
<p>默认情况下，建议您保持该参数关闭。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">core_num</span></code></p></td>
<td><p><strong>参数作用</strong>：模型运行核心数。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">1</span></code>、 <code class="docutils literal notranslate"><span class="pre">2</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">1</span></code>。</p>
<p><strong>参数说明</strong>：地平线平台支持利用多个AI加速器核心同时完成一个推理任务，</p>
<p>多核心适用于输入尺寸较大的情况，</p>
<p>理想状态下的双核速度可以达到单核的1.5倍左右。</p>
<p>如果您的模型输入尺寸较大，对于模型速度有极致追求，</p>
<p>可以配置 <code class="docutils literal notranslate"><span class="pre">core_num=2</span></code>。</p>
<p><span class="red">该选项在J5上尚不支持, 请勿配置!</span></p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optimize_level</span></code></p></td>
<td><p><strong>参数作用</strong>：模型编译的优化等级选择。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">O0</span></code> 、 <code class="docutils literal notranslate"><span class="pre">O1</span></code> 、 <code class="docutils literal notranslate"><span class="pre">O2</span></code> 、 <code class="docutils literal notranslate"><span class="pre">O3</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：优化等级可选范围为 <code class="docutils literal notranslate"><span class="pre">O0</span></code> ~ <code class="docutils literal notranslate"><span class="pre">O3</span></code>。</p>
<p><code class="docutils literal notranslate"><span class="pre">O0</span></code> 不做任何优化, 编译速度最快，优化程度最低,。</p>
<p><code class="docutils literal notranslate"><span class="pre">O1</span></code> - <code class="docutils literal notranslate"><span class="pre">O3</span></code> 随着优化等级提高，</p>
<p>预期编译后的模型的执行速度会更快，</p>
<p>但是所需编译时间也会变长。</p>
<p>正常用于生产和验证性能的模型，</p>
<p>必须使用 <code class="docutils literal notranslate"><span class="pre">O3</span></code> 级别优化才能保证得到最优性能。</p>
<p>某些流程验证或精度调试过程中，</p>
<p>可以尝试使用更低级别优化加快过程速度。</p>
</td>
<td><p>必选</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_source</span></code></p></td>
<td><p><strong>参数作用</strong>：设置上板bin模型的输入数据来源。</p>
<p><strong>取值范围</strong>： <code class="docutils literal notranslate"><span class="pre">ddr</span></code>, <code class="docutils literal notranslate"><span class="pre">pyramid</span></code>, <code class="docutils literal notranslate"><span class="pre">resizer</span></code>。</p>
<p><strong>默认配置</strong>： <code class="docutils literal notranslate"><span class="pre">{input_name}</span> <span class="pre">:</span> <span class="pre">ddr</span></code>。</p>
<p><strong>参数说明</strong>：这个参数是适配工程环境的选项，</p>
<p>建议您已经全部完成模型检查后再配置。</p>
<p><code class="docutils literal notranslate"><span class="pre">ddr</span></code> 表示数据来自内存，<code class="docutils literal notranslate"><span class="pre">pyramid</span></code> 和 <code class="docutils literal notranslate"><span class="pre">resizer</span></code></p>
<p>表示来自AI芯片上的固定硬件。</p>
<p>具体在工程环境中如何适配 <code class="docutils literal notranslate"><span class="pre">pyramid</span></code> 和 <code class="docutils literal notranslate"><span class="pre">resizer</span></code> 数据源，</p>
<p>请您参考第4章涉及到的</p>
<p><a class="reference external" href="../../bpu_sdk_api_doc/index.html">《BPU SDK API手册》</a>。</p>
<p>此参数配置有点特殊，例如模型输入名称为 data,</p>
<p>数据源为内存(ddr), 则此处应该配置值为 <code class="docutils literal notranslate"><span class="pre">&quot;data&quot;:</span> <span class="pre">&quot;ddr&quot;</span></code>。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">max_time_per_fc</span></code></p></td>
<td><p><strong>参数作用</strong>：指定模型的每个function call的最大可连续执行时间(单位ms)。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">0或1000-4294967295</span></code>。</p>
<p><strong>默认配置</strong>：<code class="docutils literal notranslate"><span class="pre">0</span></code>。</p>
<p><strong>参数说明</strong>：编译后的数据指令模型在BPU上进行推理计算时，</p>
<p>它将表现为1个或者多个function-call的调用,</p>
<p>其中function-call是BPU的执行粒度,</p>
<p>该参数用来限制每个function-call最大的执行时间,</p>
<p>设置达到后即使这一段function-call还未执行完也会被高优先级模型抢占。</p>
<p>当一个模型设置了 <code class="docutils literal notranslate"><span class="pre">max_time_per_fc</span></code> 编译参数后，即为低优先级模型，</p>
<p>它才可以被抢占。</p>
<p>详情参见 <a class="reference internal" href="../application_development.html#preemption"><span class="std std-ref">模型优先级控制</span></a> 部分的介绍。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<ul class="simple">
<li><p>此参数仅用于实现模型抢占功能，如无需实现该功能则可以忽略。</p></li>
<li><p>模型抢占功能仅支持在板端实现，不支持模拟器实现。</p></li>
</ul>
</div>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">jobs</span></code></p></td>
<td><p><strong>参数作用</strong>：设置编译bin模型时的进程数。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">机器支持的最大核心数范围内</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：在编译bin模型时，用于设置进程数。
一定程度上可提高编译速度。</p>
</td>
<td><p>可选</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>自定义算子参数组</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 24%" />
<col style="width: 63%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>编
号</p></th>
<th class="head"><p>参数名称</p></th>
<th class="head"><p>参数配置说明</p></th>
<th class="head"><p>可选/</p>
<p>必选</p>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">custom_op_method</span></code></p></td>
<td><p><strong>参数作用</strong>：自定义算子策略选择。</p>
<p><strong>取值范围</strong>：<code class="docutils literal notranslate"><span class="pre">register</span></code>。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：目前仅支持register策略，具体使用请参考</p>
<p><a class="reference internal" href="custom_op.html"><span class="doc">自定义算子开发</span></a>。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">op_register_files</span></code></p></td>
<td><p><strong>参数作用</strong>：自定义算子的Python实现文件名称。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：多个文件可用 <code class="docutils literal notranslate"><span class="pre">;</span></code> 分隔，算子如何实现请参考</p>
<p><a class="reference internal" href="custom_op.html"><span class="doc">自定义算子开发</span></a>。</p>
</td>
<td><p>可选</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">custom_op_dir</span></code></p></td>
<td><p><strong>参数作用</strong>：自定义算子的Python实现文件存放路径。</p>
<p><strong>取值范围</strong>：无。</p>
<p><strong>默认配置</strong>：无。</p>
<p><strong>参数说明</strong>：设置路径时，请使用相对路径。</p>
</td>
<td><p>可选</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>

</section>
<section id="conversion-interpretation">
<span id="id9"></span><h3><span class="section-number">3.2.4.2. </span>转换内部过程解读<a class="headerlink" href="#conversion-interpretation" title="永久链接至标题"></a></h3>
<p>模型转换完成浮点模型到地平线混合异构模型的转换。
为了使得这个异构模型能快速高效地在嵌入式端运行，
模型转换重点在解决 <strong>输入数据处理</strong> 和 <strong>模型优化编译</strong> 两个问题，本节会依次围绕这两个重点问题展开。</p>
<p><strong>输入数据处理</strong> 方面地平线的边缘AI计算平台会为某些特定类型的输入通路提供硬件级的支撑方案，
但是这些方案的输出不一定符合模型输入的要求。
例如视频通路方面就有视频处理子系统，为采集提供图像裁剪、缩放和其他图像质量优化功能，这些子系统的输出往往是yuv420格式图像，
而我们的算法模型往往是基于bgr/rgb等一般常用图像格式训练得到的。
地平线针对此种情况提供的固定解决方案是，每个转换模型都提供两份输入信息描述，
一份用于描述原始浮点模型输入（ <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 和 <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code>），
另一份则用于描述我们需要对接的边缘平台输入数据（ <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 和 <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code>）。</p>
<p>图像数据的mean/scale也是比较常见的操作，显然yuv420等边缘平台数据格式不再适合做这样的操作，
因此，我们也将这些常见图像前处理固化到了模型中。
经过以上两种处理后，转换产出的异构模型的输入部分将变成如下图状态。</p>
<a class="reference internal image-reference" href="../_images/input_data_process.png"><img alt="../_images/input_data_process.png" class="align-center" src="../_images/input_data_process.png" style="width: 899.2px; height: 229.60000000000002px;" /></a>
<p>上图中的数据排布就只有NCHW和NHWC两种数据排布格式，N代表数量、C代表channel、H代表高度、W代表宽度，
两种不同的排布体现的是不同的内存访问特性。在TensorFlow模型NHWC较常用，Caffe中就都使用NCHW，
地平线平台不会限制使用的数据排布，但是有两条要求：第一是 <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code> 必须与原始模型的数据排布一致；
第二是在边缘AI平台准备好与 <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code> 一致排布的数据，正确的数据排布指定是顺利解析数据的基础。</p>
<p>工具会根据 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 和 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 指定的数据格式自动添加数据转换节点，根据地平线的实际生产经验，
并不是任意type组合都是需要的，为了避免您误用，我们只开放了一些固定的type组合如下表。</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 51%" />
<col style="width: 7%" />
<col style="width: 9%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 14%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> \ <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code></p></td>
<td><p>nv12</p></td>
<td><p>yuv444</p></td>
<td><p>rgb</p></td>
<td><p>bgr</p></td>
<td><p>gray</p></td>
<td><p>featuremap</p></td>
</tr>
<tr class="row-even"><td><p>yuv444</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>rgb</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>bgr</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>gray</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>featuremap</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>表格中第一行是 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 中支持的类型，第一列是 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 支持的类型，
其中的 <strong>Y/N</strong> 表示是否支持相应的 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 到 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 的转换。
在转换得到的最终产出bin模型中，<code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 到 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 是一个内部的过程，
您只需要关注 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 的数据格式即可。
<strong>正确理解每种</strong> <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> <strong>的要求，对于嵌入式应用准备推理数据很重要，以下是对</strong>
<code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> <strong>每种格式的说明：</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>rgb、bgr和gray都是比较常见的图像数据，注意每个数值都采用UINT8表示。</p></li>
<li><p>yuv444是一种常见的图像格式，注意每个数值都采用UINT8表示。</p></li>
<li><p>nv12是常见的yuv420图像数据，每个数值都采用UINT8表示。</p></li>
<li><p>nv12有个比较特别的情况是 <code class="docutils literal notranslate"><span class="pre">input_space_and_range</span></code> 设置 <code class="docutils literal notranslate"><span class="pre">bt601_video</span></code>
（参考前文对 <code class="docutils literal notranslate"><span class="pre">input_space_and_range</span></code> 参数的介绍），较于常规nv12情况，它的数值范围由[0,255]变成了[16,235]，
每个数值仍然采用UINT8表示。</p></li>
<li><p>featuremap适用于以上列举格式不满足您需求的情况，此type只要求您的数据是四维的，每个数值采用float32表示。
例如雷达和语音等模型处理就常用这个格式。</p></li>
</ul>
</div></blockquote>
</div>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p>以上 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 与 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 是固化在工具链的处理流程中，如果您非常确定不需要转换，
将两个 <code class="docutils literal notranslate"><span class="pre">input_type</span></code> 设置成一样就可以了，一样的 <code class="docutils literal notranslate"><span class="pre">input_type</span></code> 会做直通处理，不会影响模型的实际执行性能。</p>
<p>同样的，数据前处理也是固化在流程中，如果您不需要做任何前处理，通过 <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> 配置关闭这个功能即可，不会影响模型的实际执行性能。</p>
</div>
<p><strong>模型优化编译</strong> 方面完成了模型解析、模型优化、模型校准与量化、模型编译几个重要阶段，其内部工作过程如下图所示。</p>
<a class="reference internal image-reference" href="../_images/model_optimization.png"><img alt="../_images/model_optimization.png" class="align-center" src="../_images/model_optimization.png" style="width: 896.0px; height: 515.1999999999999px;" /></a>
<p><strong>模型解析阶段</strong> 对于Caffe浮点模型会完成到ONNX浮点模型的转换。
在原始浮点模型上会根据转换配置中的配置参数决定是否加入数据预处理节点，此阶段产出一个original_float_model.onnx。
这个ONNX模型计算精度仍然是float32，在输入部分加入了一个数据预处理节点。</p>
<p>理想状态下，这个预处理节点应该完成 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 到 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 的完整转换，
实际情况是整个type转换过程会配合地平线AI芯片硬件完成，ONNX模型里面并没有包含硬件转换的部分。
因此ONNX的真实输入类型会使用一种中间类型，这种中间类型就是硬件对 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 的处理结果类型，
数据layout(NCHW/NHWC)会保持原始浮点模型的输入layout一致。
每种 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 都有特定的对应中间类型，如下表：</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 16%" />
<col style="width: 19%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>nv12</strong></p></td>
<td><p><strong>yuv444</strong></p></td>
<td><p><strong>rgb</strong></p></td>
<td><p><strong>bgr</strong></p></td>
<td><p><strong>gray</strong></p></td>
<td><p>featuremap</p></td>
</tr>
<tr class="row-even"><td><p>yuv444_128</p></td>
<td><p>yuv444_128</p></td>
<td><p>RGB_128</p></td>
<td><p>BGR_128</p></td>
<td><p>GRAY_128</p></td>
<td><p>featuremap</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>表格中第一行加粗部分是 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 指定的数据类型，第二行是特定 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 对应的中间类型，
这个中间类型就是original_float_model.onnx的输入类型。每个类型解释如下：</p>
<ul class="simple">
<li><p>yuv444_128 是yuv444数据减去128结果，每个数值采用int8表示。</p></li>
<li><p>RGB_128 是RGB数据减去128的结果，每个数值采用int8表示。</p></li>
<li><p>BGR_128 是BGR数据减去128的结果，每个数值采用int8表示。</p></li>
<li><p>GRAY_128 是gray数据减去128的结果，每个数值采用int8表示。</p></li>
<li><p>featuremap 是一个四维张量数据，每个数值采用float32表示。</p></li>
</ul>
</div>
<p><strong>模型优化阶段</strong> 实现模型的一些适用于地平线平台的算子优化策略，例如BN融合到Conv等。
此阶段的产出是一个optimized_float_model.onnx，这个ONNX模型的计算精度仍然是float32，经过优化后不会影响模型的计算结果。
模型的输入数据要求还是与前面的original_float_model一致。</p>
<p><strong>模型校准阶段</strong> 会使用您提供的校准数据来计算必要的量化阈值参数，这些参数会直接输入到量化阶段，不会产生新的模型状态。</p>
<p><strong>模型量化阶段</strong> 使用校准得到的参数完成模型量化，此阶段的产出是一个quantized_model.onnx。
这个模型的计算精度已经是int8，使用这个模型可以评估到模型量化带来的精度损失情况。
这个模型要求输入的基本数据格式仍然与 <code class="docutils literal notranslate"><span class="pre">original_float_model</span></code> 一样，不过layout和数值表示已经发生了变化，
整体较于 <code class="docutils literal notranslate"><span class="pre">original_float_model</span></code> 输入的变化情况描述如下：</p>
<ul class="simple">
<li><p>数据layout均使用NHWC。</p></li>
<li><p>当 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 的取值为非 <code class="docutils literal notranslate"><span class="pre">featuremap</span></code> 时，则输入的数据类型均使用INT8，
反之， 当 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 取值为 <code class="docutils literal notranslate"><span class="pre">featuremap</span></code> 时，则输入的数据类型则为float32。</p></li>
</ul>
<p><strong>模型编译阶段</strong> 会使用地平线模型编译器，将量化模型转换为地平线平台支持的计算指令和数据，
这个阶段的产出一个***.bin模型，这个bin模型是后续将在地平线边缘嵌入式平台运行的模型，也就是模型转换的最终产出结果。</p>
</section>
<section id="prepare-calibration-data">
<span id="id10"></span><h3><span class="section-number">3.2.4.3. </span>准备校准数据<a class="headerlink" href="#prepare-calibration-data" title="永久链接至标题"></a></h3>
<p>在进行模型转换时，校准阶段会需要100份左右标定样本输入，每一份样本都是一个独立的数据文件。
为了确保转换后模型的精度效果，我们希望这些校准样本来自于您训练模型使用的训练集或验证集，
不要使用非常少见的异常样本，例如纯色图片、不含任何检测或分类目标的图片等。</p>
<p>前文介绍了转换配置文件中的 <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> 参数，该参数启用和关闭状态下分别对应了两种不同的预处理样本要求。</p>
<p><code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> 关闭状态下，您需要把取自训练集/验证集的样本做与inference前一样的前处理，
处理完后的校准样本会与原始模型具备一样的数据类型(前文 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code>)、尺寸(前文 <code class="docutils literal notranslate"><span class="pre">input_shape</span></code>)和
layout(前文 <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code>)，对于featuremap输入的模型，您可以通过 <code class="docutils literal notranslate"><span class="pre">numpy.tofile</span></code> 命令将数据保存为float32格式的二进制文件，
工具链校准时会基于 <code class="docutils literal notranslate"><span class="pre">numpy.fromfile</span></code> 命令进行读取。
例如，有一个使用ImageNet训练的用于分类的原始浮点模型，它只有一个输入节点，输入信息描述如下：</p>
<ul class="simple">
<li><p>输入类型：<code class="docutils literal notranslate"><span class="pre">BGR</span></code></p></li>
<li><p>输入layout：<code class="docutils literal notranslate"><span class="pre">NCHW</span></code></p></li>
<li><p>输入尺寸：<code class="docutils literal notranslate"><span class="pre">1x3x224x224</span></code></p></li>
</ul>
<p>使用验证集做Inference时的数据预处理如下：</p>
<ol class="arabic simple">
<li><p>图像长宽等比scale,短边缩放到256。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">center_crop</span></code> 方法获取224x224大小图像。</p></li>
<li><p>按通道减mean</p></li>
<li><p>数据乘以scale系数</p></li>
</ol>
<p>依照 <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> 关闭状态下的样本文件制作原则，针对上述举例模型的样本处理代码如下
(为避免过长代码篇幅，各种简单transformer实现代码未贴出，请自行实现)：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># 本示例使用skimage，如果是opencv会有所区别
# 需要您特别注意的是，transformers中并没有体现减mean和乘scale的处理
# mean和scale操作已经融合到了模型中，参考前文norm_type/mean_values/scale_values配置
def data_transformer():
  transformers = [
  # 长宽等比scale，短边缩放至256
  ShortSideResizeTransformer(short_size=256),
  # CenterCrop获取224x224图像
  CenterCropTransformer(crop_size=224),
  # skimage读取结果为NHWC排布，转换为模型需要的NCHW
  HWC2CHWTransformer(),
  # skimage读取结果通道顺序为RGB，转换为模型需要的BGR
  RGB2BGRTransformer(),
  # skimage读取数值范围为[0.0,1.0]，调整为模型需要的数值范围
  ScaleTransformer(scale_value=255)
  ]

  return transformers

# src_image 标定集中的原图片
# dst_file 存放最终标定样本数据的文件名称
def convert_image(src_image, dst_file, transformers)：
  image = skimage.img_as_float(skimage.io.imread(src_file))
  for trans in transformers:
  image = trans(image)
  # 模型指定的input_type_train BGR数值类型是UINT8
  image = image.astype(np.uint8)
  # 二进制存储标定样本到数据文件
  image.tofile(dst_file)

if __name__ == &#39;__main__&#39;:
  # 此处表示原始标定图片集合，伪代码
  src_images = [&#39;ILSVRC2012_val_00000001.JPEG&#39;，...]
  # 此处表示最终标定文件名称（后缀名不限制），伪代码
  # calibration_data_bgr_f32是您在配置文件中指定的cal_data_dir
  dst_files = [&#39;./calibration_data_bgr_f32/ILSVRC2012_val_00000001.bgr&#39;，...]

  transformers = data_transformer()
  for src_image, dst_file in zip(src_images, dst_files):
  convert_image(src_image, dst_file, transformers)
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p><code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> 启用状态下，标定样本使用skimage支持read的图片格式文件即可。
转换工具读取这些图片后，会将其缩放到模型输入节点要求的尺寸大小，以此结果作为校准的输入。
这样的操作会简单，但是对于量化精度的没有保障，我们强烈建议您使用前文关闭 <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> 的方式。</p>
</div>
</section>
<section id="id11">
<h3><span class="section-number">3.2.4.4. </span>转换结果解读<a class="headerlink" href="#id11" title="永久链接至标题"></a></h3>
<p>本节将依次介绍模型转换成功状态的解读、转换不成功的分析方式。
确认模型转换成功，需要您从 <code class="docutils literal notranslate"><span class="pre">makertbin</span></code> 状态信息、相似度信息和 <cite>working_dir</cite> 产出三个方面确认。
<code class="docutils literal notranslate"><span class="pre">makertbin</span></code> 状态信息方面，转换成功将在控制台输出信息尾部给出明确的提示信息如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">2021</span>-04-21 <span class="m">11</span>:13:08,337 INFO Convert to runtime bin file successfully!
<span class="m">2021</span>-04-21 <span class="m">11</span>:13:08,337 INFO End Model Convert
</pre></div>
</div>
<p>相似度信息也存在于 <code class="docutils literal notranslate"><span class="pre">makertbin</span></code> 的控制台输出内容中，在 <code class="docutils literal notranslate"><span class="pre">makertbin</span></code> 状态信息之前，其内容形式如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">======================================================================</span>
Node    ON   Subgraph  Type     Cosine Similarity  Threshold
----------------------------------------------------------------------
...    ...     ...     ...       <span class="m">0</span>.999936           <span class="m">127</span>.000000
...    ...     ...     ...       <span class="m">0</span>.999868           <span class="m">2</span>.557209
...    ...     ...     ...       <span class="m">0</span>.999268           <span class="m">2</span>.133924
...    ...     ...     ...       <span class="m">0</span>.996023           <span class="m">3</span>.251645
...    ...     ...     ...       <span class="m">0</span>.996656           <span class="m">4</span>.495638
</pre></div>
</div>
<p>上面列举的输出内容中，Node、ON、Subgraph、Type与 <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> 工具的解读是一致的，
请参考前文 <a class="reference internal" href="#check-result"><span class="std std-ref">检查结果解读</span></a>；
Threshold是每个层次的校准阈值，用于异常状态下向地平线技术支持反馈信息，正常状况下不需要关注；
Cosine Similarity反映的Node指示的节点中，原始浮点模型与量化模型输出结果的余弦相似度。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>需要您特别注意的是，Cosine Similarity只是指明量化后数据稳定性的一种参考方式，对于模型精度的影响不存在明显的直接关联关系。
一般情况下，输出节点的相似度低于0.8就有了较明显的精度损失，当然由于与精度不存在绝对的直接关联，
完全准确的精度情况还需要您参考 <a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">模型精度分析与调优</span></a> 的介绍。</p>
</div>
<p>转换产出存放在转换配置参数 <code class="docutils literal notranslate"><span class="pre">working_dir</span></code> 指定的路径中，成功完成模型转换后，
您可以在该目录下得到以下文件(***部分是您通过转换配置参数 <code class="docutils literal notranslate"><span class="pre">output_model_file_prefix</span></code> 指定的内容)：</p>
<ul class="simple">
<li><p>***_original_float_model.onnx</p></li>
<li><p>***_optimized_float_model.onnx</p></li>
<li><p>***_quantized_model.onnx</p></li>
<li><p>***.bin</p></li>
</ul>
<p><a class="reference internal" href="#conversion-output"><span class="std std-ref">转换产出物解读</span></a> 介绍了每个产出物的用途。
不过在上板运行前，我们强烈建议您完成 <a class="reference internal" href="#model-check"><span class="std std-ref">验证模型</span></a> 和 <a class="reference internal" href="#performance-evaluation"><span class="std std-ref">模型性能分析与调优</span></a>
介绍的性能&amp;精度评测过程，避免将模型转换问题延伸到后续嵌入式端。</p>
<p>如果以上验证模型转换成功的三个方面中，有任一个出现缺失都说明模型转换出现了错误。
一般情况下，<code class="docutils literal notranslate"><span class="pre">makertbin</span></code> 工具会在出现错误时将错误信息输出至控制台，
例如我们在Caffe模型转换时不配置 <code class="docutils literal notranslate"><span class="pre">prototxt</span></code> 和 <code class="docutils literal notranslate"><span class="pre">caffe_model</span></code> 参数，工具给出如下提示。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">2021</span>-04-21 <span class="m">14</span>:45:34,085 ERROR Key <span class="s1">&#39;model_parameters&#39;</span> error:
Missing keys: <span class="s1">&#39;caffe_model&#39;</span>, <span class="s1">&#39;prototxt&#39;</span>
<span class="m">2021</span>-04-21 <span class="m">14</span>:45:34,085 ERROR yaml file parse failed. Please double check your input
<span class="m">2021</span>-04-21 <span class="m">14</span>:45:34,085 ERROR exception <span class="k">in</span> command: makertbin
</pre></div>
</div>
<p>如果以上步骤不能帮助您发现问题，欢迎在地平线唯一官方技术社区（<a class="reference external" href="https://developer.horizon.ai/">https://developer.horizon.ai/</a>）提出您的问题，
我们将在24小时内给您提供支持。</p>
</section>
<section id="conversion-output">
<span id="id12"></span><h3><span class="section-number">3.2.4.5. </span>转换产出物解读<a class="headerlink" href="#conversion-output" title="永久链接至标题"></a></h3>
<p>上文提到模型成功转换的产出物包括以下四个部分，本节将介绍每个产出物的用途：</p>
<ul class="simple">
<li><p>***_original_float_model.onnx</p></li>
<li><p>***_optimized_float_model.onnx</p></li>
<li><p>***_quantized_model.onnx</p></li>
<li><p>***.bin</p></li>
</ul>
<p>***_original_float_model.onnx的产出过程可以参考 <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a> 的介绍，
这个模型计算精度与转换输入的原始浮点模型是一模一样的，有个重要的变化就是为了适配地平线平台添加了一些数据预处理计算。
一般情况下，您不需要使用这个模型，在转换结果出现异常时，如果能把这个模型提供给地平线的技术支持，将有助于帮助您快速解决问题。</p>
<p>***_optimized_float_model.onnx的产出过程可以参考 <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a> 的介绍，
这个模型经过一些算子级别的优化操作，常见的就是算子融合。
通过与original_float模型的可视化对比，您可以明显看到一些算子结构级别的变化，不过这些都不影响模型的计算精度。
一般情况下，您不需要使用这个模型，在转换结果出现异常时，如果能把这个模型提供给地平线的技术支持，将有助于帮助您快速解决问题。</p>
<p>***_quantized_model.onnx的产出过程可以参考 <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a> 的介绍，
这个模型已经完成了校准和量化过程，量化后的精度损失情况可以从这里查看。
这个模型是精度验证过程中必须要使用的模型，具体使用方式请参考 <a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">模型精度分析与调优</span></a> 部分的介绍。</p>
<p>***.bin就是可以用于在地平线AI芯片上加载运行的模型，
配合 <a class="reference internal" href="../application_development.html"><span class="doc">应用开发</span></a> 部分介绍的内容，
您就可以将模型快速在芯片部署运行。不过为了确保模型的性能与精度效果是符合您的预期的，
我们强烈建议完成 <a class="reference internal" href="#model-conversion"><span class="std std-ref">转换模型</span></a> 和 <a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">模型精度分析与调优</span></a>
介绍的性能和精度分析过程后再进入到应用开发和部署。</p>
</section>
</section>
<section id="performance-evaluation">
<span id="id13"></span><h2><span class="section-number">3.2.5. </span>模型性能分析与调优<a class="headerlink" href="#performance-evaluation" title="永久链接至标题"></a></h2>
<p>本节介绍了如何使用地平线提供的工具评估模型性能，这些工具得到的都是与实际执行基本无异的性能效果，
如果此阶段发现评估结果不符合预期，强烈建议您尽量在此阶段根据地平线的优化建议解决性能问题，
不建议将模型的问题延伸到应用开发阶段。</p>
<section id="hb-perf">
<span id="id14"></span><h3><span class="section-number">3.2.5.1. </span>使用 <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> 工具估计性能<a class="headerlink" href="#hb-perf" title="永久链接至标题"></a></h3>
<p>地平线提供的 <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> 以模型转换得到的 ***.bin为输入，可以直接得到模型预期上板性能，工具使用方式如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_perf  ***.bin
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>如果分析的是 <code class="docutils literal notranslate"><span class="pre">pack</span></code> 后模型，需要加上一个 <code class="docutils literal notranslate"><span class="pre">-p</span></code> 参数，命令为 <code class="docutils literal notranslate"><span class="pre">hb_perf</span> <span class="pre">-p</span> <span class="pre">***.bin</span></code>。
关于模型 <code class="docutils literal notranslate"><span class="pre">pack</span></code>，请查看 <a class="reference internal" href="#other-tools"><span class="std std-ref">其他模型工具（可选）</span></a> 部分的介绍。</p>
</div>
<p>命令中的 ***.bin就是模型转换产出的bin模型，命令执行完成后，
在当前工作目录下会得到一个 <cite>hb_perf_result</cite> 目录，分析结果以html形式提供。
以下是我们分析一个MobileNet的示例结果，其中mobilenetv1_224x224_nv12.html就是查看分析结果的主页面。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_perf_result/
└── mobilenetv1_224x224_nv12
    ├── MOBILENET_subgraph_0.html
    ├── MOBILENET_subgraph_0.json
    ├── mobilenetv1_224x224_nv12
    ├── mobilenetv1_224x224_nv12.html
    ├── mobilenetv1_224x224_nv12.png
    └── temp.hbm
</pre></div>
</div>
<p>通过浏览器打开结果主页面，其内容如下图：</p>
<img alt="../_images/hb_mapper_perf_2.png" class="align-center" src="../_images/hb_mapper_perf_2.png" />
<p>分析结果主要由Model Performance Summary、Details和BIN Model Structure三个部分组成。
Model Performance Summary是整个bin模型的整体性能评估结果，其中各项指标为:</p>
<ul class="simple">
<li><p>Model Name——模型名称。</p></li>
<li><p>BPU Model Latency(ms)——模型整体单帧计算耗时(单位为ms)。</p></li>
<li><p>Model DDR Occupation(Mb per frame)——模型运行的整体内存占用情况(单位为Mb/frame)。</p></li>
<li><p>Loaded Bytes per Frame——模型运行每帧读取数据量。</p></li>
<li><p>Stored Bytes per Frame——模型运行每帧存储数据量。</p></li>
</ul>
<p>在了解Details和BIN Model Structure前，您需要了解子图（subgraph）的概念。
如果模型在非输入和输出部分出现了CPU计算的算子，模型转换工具将把这个算子前后连续在BPU计算的部分拆分为两个独立的子图（subgraph）。
具体可以参考 <a class="reference internal" href="#model-check"><span class="std std-ref">验证模型</span></a> 部分的介绍。</p>
<p>Details是每份模型BPU子图的具体信息，在主页面中，每个子图提供的指标解读如下：</p>
<ul class="simple">
<li><p>Model Subgraph Name——子图名称。</p></li>
<li><p>Model Subgraph Calculation Load (OPpf)——子图的单帧计算量。</p></li>
<li><p>Model Subgraph DDR Occupation(Mbpf)——子图的单帧读写数据量（单位为MB）。</p></li>
<li><p>Model Subgraph Latency(ms)——子图的单帧计算耗时（单位为ms）。</p></li>
</ul>
<p>每份子图结果提供了一个明细入口，以上指标都是明细页面提取到的，进入到明细页面可以给您更加细致的参考信息。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>需要特别注意的是，明细页面会根据您是否启用调试级转换而有所区别，
下图中的Layer Details仅当在配置文件中设置 <code class="docutils literal notranslate"><span class="pre">debug</span></code> 参数为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时才可以拿到，
这个 <code class="docutils literal notranslate"><span class="pre">debug</span></code> 参数配置方法请参考 <a class="reference internal" href="#makertbin"><span class="std std-ref">使用 hb_mapper makertbin 工具转换模型</span></a> 部分的介绍。</p>
</div>
<p>Layer Details提供到了具体算子级别的分析，在调试分析阶段也是比较不错的参考，
如果是某些BPU算子导致性能低，可以帮助您定位到这个具体算子。</p>
<img alt="../_images/layer_details.png" class="align-center" src="../_images/layer_details.png" />
<p>BIN Model Structure部分提供的是bin模型的子图级可视化结果，图中深色节点表示运行在BPU上的节点，灰色节点表示在CPU上计算的节点。</p>
<p>使用 <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> 的意义在于了解bin模型子图结构，对于BPU上计算部分，该工具也能提供较全面的静态分析指标。
不过 <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> 不含CPU部分的计算评估，如果CPU计算仅限于模型输入或输出部分的常规性处理，不含计算密集型计算节点，这个影响不大。
否则，您就一定需要利用开发板工具实测性能。</p>
</section>
<section id="id15">
<h3><span class="section-number">3.2.5.2. </span>开发板实测性能<a class="headerlink" href="#id15" title="永久链接至标题"></a></h3>
<p>开发板上实测模型性能使用的是开发板上 <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> 工具，
<code class="docutils literal notranslate"><span class="pre">hrt</span> <span class="pre">_model_exec</span></code> 是一个模型执行工具，可直接在开发板上评测模型的推理性能、获取模型信息。
一方面可以让用户拿到模型时实际了解模型真实性能；
另一方面也可以帮助用户了解模型可以做到的速度极限，对于应用调优的目标极限具有指导意义。</p>
<p>使用 <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> 工具前，有两个准备工作。</p>
<ol class="arabic simple">
<li><p>确保您已经参考 <a class="reference internal" href="../env_prepare.html"><span class="doc">环境部署</span></a> 介绍完成了开发板上工具安装。</p></li>
<li><p>第二是需要将Ubuntu/CentOS开发机上得到的bin模型拷贝到开发板上（建议放在/userdata目录），
开发板上是一个Linux系统，可以通过 <code class="docutils literal notranslate"><span class="pre">scp</span></code> 等Linux系统常用方式完成这个拷贝过程。</p></li>
</ol>
<p>使用 <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> 实测性能的参考命令如下（<strong>注意是在开发板上执行</strong>）：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./hrt_model_exec perf --model_file mobilenetv1_224x224_nv12.bin <span class="se">\</span>
                      --model_name<span class="o">=</span><span class="s2">&quot;&quot;</span> <span class="se">\</span>
                      --core_id<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
                      --frame_count<span class="o">=</span><span class="m">200</span> <span class="se">\</span>
                      --perf_time<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
                      --thread_num<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
                      --profile_path<span class="o">=</span><span class="s2">&quot;.&quot;</span>
</pre></div>
</div>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></span></dt>
<dd><dl class="simple">
<dt>model_file：</dt><dd><p>需要分析性能的bin模型名称。</p>
</dd>
<dt>model_name:</dt><dd><p>需要分析性能的bin模型名字。若 <code class="docutils literal notranslate"><span class="pre">model_file</span></code> 只含一个模型，则可以省略。</p>
</dd>
<dt>core_id</dt><dd><p>默认值 <code class="docutils literal notranslate"><span class="pre">0</span></code>，运行模型使用的核心id，<code class="docutils literal notranslate"><span class="pre">0</span></code> 代表任意核心，<code class="docutils literal notranslate"><span class="pre">1</span></code> 代表核心0，<code class="docutils literal notranslate"><span class="pre">2</span></code> 代表核心1。若要分析双核极限帧率，请将此处设为 <code class="docutils literal notranslate"><span class="pre">0</span></code>。</p>
</dd>
<dt>frame_count：</dt><dd><p>默认值 <code class="docutils literal notranslate"><span class="pre">200</span></code>，设置推理帧数，工具会执行指定次数后再分析平均耗时。 当 <code class="docutils literal notranslate"><span class="pre">perf_time</span></code> 为 <code class="docutils literal notranslate"><span class="pre">0</span></code> 时生效。</p>
</dd>
<dt>perf_time:</dt><dd><p>默认值 <code class="docutils literal notranslate"><span class="pre">0</span></code>，单位分钟。设置推理时间，工具会执行指定时间后再分析平均耗时。</p>
</dd>
<dt>thread_num：</dt><dd><p>默认值 <code class="docutils literal notranslate"><span class="pre">1</span></code>，设置运行的线程数，取值范围 <code class="docutils literal notranslate"><span class="pre">[1,8]</span></code>。若要分析极限帧率，请将线程数改大。</p>
</dd>
<dt>profile_path：</dt><dd><p>默认关闭，统计工具日志产生路径。该参数引入的分析结果会存放在指定目录下的profiler.log文件中。</p>
</dd>
</dl>
</dd></dl>

<p>命令执行完成后，您将在控制台得到如下结果。
最终的评估结果就是 <code class="docutils literal notranslate"><span class="pre">Average</span> <span class="pre">latency</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Frame</span> <span class="pre">rate</span></code>，分别表示平均单帧推理延时和模型极限帧率。
如果想获得模型在板子上运行的极限帧率，需将 <code class="docutils literal notranslate"><span class="pre">thread_num</span></code> 设置得足够大。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Running condition:
  Thread number is: <span class="m">1</span>
  Frame count   is: <span class="m">200</span>
  core number   is: <span class="m">1</span>
  Program run time: <span class="m">726</span>.604000  ms
Perf result:
  Frame totally latency is: <span class="m">714</span>.537781  ms
  Average    latency    is: <span class="m">3</span>.572689  ms
  Frame      rate       is: <span class="m">275</span>.253095  FPS
</pre></div>
</div>
<p>控制台得到的信息只有整体情况，通过 <code class="docutils literal notranslate"><span class="pre">profile_path</span></code> 控制产生的node_profiler.log文件记录了更加丰富的信息如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
  <span class="s2">&quot;model_latency&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;MOBILENET_subgraph_0&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">2</span>.889,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">2</span>.889,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">2</span>.889
    <span class="o">}</span>,
    <span class="s2">&quot;MOBILENET_subgraph_0_output_layout_convert&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.017265,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.038,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.015
    <span class="o">}</span>,
    <span class="s2">&quot;fc7_1_HzDequantize&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.07467,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.146,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.069
    <span class="o">}</span>,
    <span class="s2">&quot;prob&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.08839,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.172,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.052
    <span class="o">}</span>
  <span class="o">}</span>,
  <span class="s2">&quot;task_latency&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;TaskRunningTime&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">3</span>.43695,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">5</span>.883,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">3</span>.354
    <span class="o">}</span>,
    <span class="s2">&quot;TaskScheduleTime&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.07456,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.215,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.054
    <span class="o">}</span>,
    <span class="s2">&quot;TaskSubmitTime&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.00861,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.106,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.006
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>这里的内容会对应到 <a class="reference internal" href="#hb-perf"><span class="std std-ref">使用hb_perf工具估计性能</span></a> 中的BIN Model Structure部分介绍的bin可视化图中，
图中每个节点都有一个对应节点在profiler.log文件中，可以通过 <code class="docutils literal notranslate"><span class="pre">name</span></code> 对应起来。
profiler.log文件中记录了每个节点的执行时间，对优化节点有重要的参考意义。</p>
<p><code class="docutils literal notranslate"><span class="pre">profiler</span></code> 分析是经常使用的操作，前文 <a class="reference internal" href="#check-result"><span class="std std-ref">检查结果解读</span></a> 部分提到检查阶段不用过于关注CPU算子，
此阶段就能看到CPU算子的具体耗时情况了，如果根据这里的评估认为CPU耗时太长，那就值得优化了。</p>
</section>
<section id="model-performance-optimization">
<span id="id16"></span><h3><span class="section-number">3.2.5.3. </span>模型性能优化<a class="headerlink" href="#model-performance-optimization" title="永久链接至标题"></a></h3>
<p>根据以上性能分析结果，您可能发现性能结果不及预期，本章节内容介绍了地平线对提升模型性能的建议与措施，
包括： <strong>检查yaml配置参数</strong>、<strong>处理CPU算子</strong>、<strong>高性能模型设计建议</strong>、<strong>使用地平线平台友好结构&amp;模型</strong> 共四个方面。</p>
<p>部分修改可能会影响原始浮点模型的参数空间，意味着需要您重训模型，为了避免性能调优过程中反复调整并训练的代价，
在得到满意性能效果前，建议您使用随机参数导出模型来验证性能即可。</p>
<p><strong>检查影响模型性能的yaml参数</strong></p>
<blockquote>
<div><p>在模型转换的yaml配置文件中，部分参数会实际影响模型的最终性能，可以先检查下是否已正确按照预期配置，
各参数的具体含义和作用请参考 <a class="reference external" href="../../j5_ai_toolchain_tool_guide/hb_mapper/hb_mapper.html">hb_mapper 工具介绍</a> 中的 <strong>配置文件详细介绍</strong> 小节的内容。</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">layer_out_dump</span></code>：指定模型转换过程中是否输出模型的中间结果，一般仅用于调试功能。
如果将其配置为 <code class="docutils literal notranslate"><span class="pre">True</span></code>，则会为每个卷积算子增加一个反量化输出节点，它会显著的降低模型上板后的性能。
所以在性能评测时，务必要将该参数配置为 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compile_mode</span></code>：该参数用于选择模型编译时的优化方向为带宽还是时延，关注性能时请配置为 <code class="docutils literal notranslate"><span class="pre">latency</span></code>。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimize_level</span></code>：该参数用于选择编译器的优化等级，实际生产中应配置为 <code class="docutils literal notranslate"><span class="pre">O3</span></code> 获取最佳性能。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">debug</span></code>：配置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 将打开编译器的debug模式，能够输出性能仿真的相关信息，如帧率、DDR 带宽占用等。
一般用于性能评估阶段，在产品化交付时候，可关闭该参数减小模型大小，提高模型执行效率。</p></li>
</ul>
</div></blockquote>
<p><strong>处理CPU算子</strong></p>
<blockquote>
<div><p>根据 <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> 的评估，已经确认突出的性能瓶颈是CPU算子导致的。
此种情况下，我们建议您先查看 <a class="reference internal" href="#op-restrictions"><span class="std std-ref">算子约束列表</span></a> 的内容，确认当前运行在CPU上的算子是否具备BPU支持的能力。</p>
<p>如果算子不具备BPU支持能力，那么就是您的算子参数超过了BPU支持的参数约束范围，
将相应原始浮点模型计算参数调整到约束范围内即可。
为了方便您快速知晓超出约束的具体参数，建议您再使用 <a class="reference internal" href="#model-check"><span class="std std-ref">验证模型</span></a> 部分介绍的方法做一遍检查，
工具将会直接给出超出BPU支持范围的参数提示。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>修改原始浮点模型参数对模型计算精度的影响需要您自己把控，
例如Convolution的 <code class="docutils literal notranslate"><span class="pre">input_channel</span></code> 或 <code class="docutils literal notranslate"><span class="pre">output_channel</span></code> 超出范围就是一种较典型的情况，
减少channel快速使得该算子被BPU支持，单单只做这一处修改也预计会对模型精度产生影响。</p>
</div>
<p>如果算子并不具备BPU支持能力，就需要您在地平线支持的BPU算子中找一个替代算子，并将其替换到原始浮点模型中。
对于计算密集型的算子，地平线一般都具备BPU支持能力，少数只能在CPU上运行算子也都经过了极致优化。
所以，这种情况一般由于您使用了一种不被BPU支持的激活函数造成的，而且这个激活函数反复被使用，最终导致bin模型中出现很多子图分割情况。</p>
</div></blockquote>
<p><strong>高性能模型设计建议</strong></p>
<blockquote>
<div><p>根据性能评估结果，CPU上耗时占比可能很小，主要的性能瓶颈还是BPU推理时间过长。
这种情况下，我们已经把计算器件都用上了，发力的空间就在于提升计算资源的利用率。
每种AI芯片都有自己的硬件特性，算法模型的计算参数是否很好地符合了硬件特性，
直接决定了计算资源的利用率，符合度越高则利用率越高，反之则低。
本部分介绍重点在于阐明地平线的硬件特性。</p>
<p>首先，地平线的AI芯片是一款旨在加速CNN（卷积神经网络）的芯片，主要的计算资源都集中在处理各种卷积计算。
所以，我们希望您的模型是以卷积计算为主的模型，卷积之外的算子都会导致计算资源的利用率降低，不同OP的影响程度会有所不同。</p>
</div></blockquote>
<p><strong>其他建议</strong></p>
<p>地平线芯片上的depthwise convolution的计算效率接近100%，所以对于MobileNet类的模型，BPU芯片具有效率优势。</p>
<p>另外，在模型设计时，我们应尽量让模型BPU段的输入输出维度降低，以减少量化、反量化节点的耗时和硬件的带宽压力。
以典型的分割模型为例，我们可以将Argmax算子直接合入模型本身。
但需注意，只有满足以下条件，Argmax才支持BPU加速：</p>
<ol class="arabic simple">
<li><p>Caffe中的Softmax层默认axis=1，而ArgMax层则默认axis=0，算子替换时要保持axis的一致</p></li>
<li><p>Argmax的Channel需小于等于64，否则只能在CPU上计算</p></li>
</ol>
<p><strong>BPU面向高效率模型优化</strong></p>
<p>学术界在持续优化算法模型的计算效率（同样算法精度下所需的理论计算量越小越高效）、参数效率（同样算法精度下所用参数量越小越高效）。
这方面的代表工作有EfficientNet和ResNeXt，二者分别使用了Depthwise Convolution和Group Convolution。
面对这样的高效率模型，GPU/TPU支持效率很低，不能充分发挥算法效果，学术界被迫针对GPU/TPU分别优化了EfficientNet V2/X和NFNet，
优化过程主要是通过减少Depthwise Convolution的使用以及大幅扩大Group Convolution中的Group大小，
这些调整都降低了原本模型的计算效率和参数效率。</p>
<p>更多的模型结构和业务模型都在持续探索中，我们将提供更加丰富的模型给您作为直接的参考，
这些产出将不定期更新至 <a class="reference external" href="https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master">https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master</a>。
如果以上依然不能满足您的需要，欢迎在地平线唯一官方技术社区（<a class="reference external" href="https://developer.horizon.ai">https://developer.horizon.ai</a>）发帖与我们取得联系，
我们将根据您的具体问题提供更具针对性的指导建议。</p>
</section>
</section>
<section id="accuracy-evaluation">
<span id="id17"></span><h2><span class="section-number">3.2.6. </span>模型精度分析与调优<a class="headerlink" href="#accuracy-evaluation" title="永久链接至标题"></a></h2>
<p>基于几十或上百张校准数据实现浮点模型到定点模型转换的后量化方式，不可避免地会存在一定的精度损失。
但经过大量实际生产经验验证，如果能筛选出最优的量化参数组合，地平线的转换工具在大部分情况下，都可以将精度损失保持在1%以内。</p>
<p>本节先介绍了如何正确地进行模型精度分析，如果通过评估发现不及预期，则可以参考 <a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">精度调优</span></a> 小节的内容尝试调优，
实在无法解决可寻求地平线的技术支持。</p>
<section id="id18">
<h3><span class="section-number">3.2.6.1. </span>模型精度分析<a class="headerlink" href="#id18" title="永久链接至标题"></a></h3>
<p>在进入到此部分介绍前，我们希望您已经了解如何对一个模型进行精度评测。本节介绍的内容是如何使用模型转换的产出物进行推理。</p>
<p>前文提到模型成功转换的产出物包括以下四个部分：</p>
<ul class="simple">
<li><p>***_original_float_model.onnx</p></li>
<li><p>***_optimized_float_model.onnx</p></li>
<li><p>***_quantized_model.onnx</p></li>
<li><p>***.bin</p></li>
</ul>
<p>虽然最后的bin模型才是将部署到AI芯片的模型，考虑到方便在Ubuntu/CentOS开发机上完成精度评测，
我们提供了***_quantized_model.onnx完成这个精度评测的过程。
quantized模型已经完成了量化，与最后的bin模型具有一致的精度效果。
使用地平线开发库加载ONNX模型推理的基本流程如下所示，这份示意代码不仅适用于quantized模型，
对original和optimized模型同样适用，根据不同模型的输入类型和layout要求准备数据即可。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 加载地平线依赖库</span>
<span class="kn">from</span> <span class="nn">horizon_tc_ui</span> <span class="kn">import</span> <span class="n">HB_ONNXRuntime</span>

<span class="c1"># 准备模型运行的feed_dict</span>
<span class="k">def</span> <span class="nf">prepare_input_dict</span><span class="p">(</span><span class="n">input_names</span><span class="p">):</span>
  <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">input_name</span> <span class="ow">in</span> <span class="n">input_names</span><span class="p">:</span>
      <span class="c1"># your_custom_data_prepare代表您的自定义数据</span>
      <span class="c1"># 根据输入节点的类型和layout要求准备数据即可</span>
      <span class="n">feed_dict</span><span class="p">[</span><span class="n">input_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">your_custom_data_prepare</span><span class="p">(</span><span class="n">input_name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">feed_dict</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="c1"># 创建推理Session</span>
  <span class="n">sess</span> <span class="o">=</span> <span class="n">HB_ONNXRuntime</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="s1">&#39;***_quantized_model.onnx&#39;</span><span class="p">)</span>

  <span class="c1"># 获取输入节点名称</span>
  <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">sess</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()]</span>
  <span class="c1"># 或</span>
  <span class="n">input_names</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">input_names</span>

  <span class="c1"># 获取输出节点名称</span>
  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">sess</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()]</span>
  <span class="c1"># 或</span>
  <span class="n">output_names</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">output_names</span>

  <span class="c1"># 准备模型输入数据</span>
  <span class="n">feed_dict</span> <span class="o">=</span> <span class="n">prepare_input_dict</span><span class="p">(</span><span class="n">input_names</span><span class="p">)</span>
  <span class="c1"># 开始模型推理，推理的返回值是一个list，依次与output_names指定名称一一对应</span>
  <span class="c1"># 输入图像的类型范围为（RGB/BGR/NV12/YUV444/GRAY）</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output_names</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">input_offset</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
  <span class="c1"># 输入数据的类型范围为（FEATURE）</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run_feature</span><span class="p">(</span><span class="n">output_names</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">input_offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Modification  history:</span>
<span class="sd">    OE 1.3 ~ 1.6</span>
<span class="sd">        outputs = sess.run(output_names, feed_dict, input_type_rt=None, float_offset=0)</span>
<span class="sd">        outputs = sess.run_feature(output_names, feed_dict, {input_name: &quot;featuremap&quot;}, float_offset=0)</span>
<span class="sd">    OE 1.7</span>
<span class="sd">        outputs = sess.run(output_names, feed_dict, input_type_rt=None, float_offset=None, input_offset=128)</span>
<span class="sd">        outputs = sess.run_feature(output_names, feed_dict, {input_name: &quot;featuremap&quot;}, float_offset=0)</span>
<span class="sd">    OE 1.8 ~ 1.9</span>
<span class="sd">        outputs = sess.run(output_names, feed_dict, input_offset=128)</span>
<span class="sd">        outputs = sess.run_feature(output_names, feed_dict, input_offset=128)</span>

<span class="sd">    note: OE 1.5 后架构上的调整，如果更新 OE 需要重新编译模型</span>
<span class="sd">  &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>上述代码中，<code class="docutils literal notranslate"><span class="pre">input_offset</span></code> 参数可以不提供, 其默认值为128. 对于有前处理节点的模型, 这里都需要做-128的操作. 如果模型输入前并未添加前处理节点, 则需要将 <code class="docutils literal notranslate"><span class="pre">input_offset</span></code> 设置为0。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>对于多输入模型：</p>
<ul class="simple">
<li><p>如果输入 input_type 均属于 （ RGB/BGR/NV12/YUV444/GRAY ），可以采用 sess.run 方法做推理。</p></li>
<li><p>如果输入 input_type 均属于 （ FEATURE ），可以采用 sess.run_feature 方法做推理。</p></li>
<li><p>如果输入 input_type 为混合类型，暂不支持这种场景。</p></li>
</ul>
</div>
<p>此外, <code class="docutils literal notranslate"><span class="pre">your_custom_data_prepare</span></code> 所代表的输入数据准备过程是最容易出现误操作的部分。
较于您设计&amp;训练原始浮点模型的精度验证过程，我们需要您在数据预处理后将推理输入数据进一步调整，
这些调整主要是数据格式（RGB、NV12等）、数据精度（int8、float32等）和数据排布（NCHW或NHWC）。
至于具体怎么调整，这个是由您在模型转换时设置的 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 、 <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code> 、 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 和 <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code> 四个参数共同决定的，其详细规则请参考 <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a> 部分的介绍。</p>
<p>举个例子，有一个使用ImageNet训练的用于分类的原始浮点模型，它只有一个输入节点。
这个节点接受BGR顺序的三通道图片，输入数据排布为NCHW。原始浮点模型设计&amp;训练阶段，验证集推理前做的数据预处理如下：</p>
<ol class="arabic simple">
<li><p>图像长宽等比scale,短边缩放到256。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">center_crop</span></code> 方法获取224x224大小图像。</p></li>
<li><p>按通道减mean。</p></li>
<li><p>数据乘以scale系数。</p></li>
</ol>
<dl class="simple">
<dt>使用地平线转换这个原始浮点模型时，</dt><dd><p><code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 设置 <code class="docutils literal notranslate"><span class="pre">bgr</span></code> 、 <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code> 设置 <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> 、 <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 设置 <code class="docutils literal notranslate"><span class="pre">bgr</span></code> 、
<code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code> 设置 <code class="docutils literal notranslate"><span class="pre">NHWC</span></code> 。</p>
</dd>
</dl>
<p>根据 <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">转换内部过程解读</span></a> 部分介绍的规则，
***_quantized_model.onnx接受的输入应该为bgr_128、NHWC排布。
对应到前文的示例代码， <code class="docutils literal notranslate"><span class="pre">your_custom_data_prepare</span></code> 部分提供的数据处理应该一个这样的过程：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 本示例使用skimage，如果是opencv会有所区别</span>
<span class="c1"># 需要您特别注意的是，transformers中并没有体现减mean和乘scale的处理</span>
<span class="c1"># mean和scale操作已经融合到了模型中，参考前文norm_type/mean_values/scale_values配置</span>
<span class="k">def</span> <span class="nf">your_custom_data_prepare_sample</span><span class="p">(</span><span class="n">image_file</span><span class="p">):</span>
  <span class="c1"># skimage读取图片，已经是NHWC排布</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">img_as_float</span><span class="p">(</span><span class="n">skimage</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_file</span><span class="p">))</span>
  <span class="c1"># 长宽等比scale，短边缩放至256</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">ShortSideResize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">short_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
  <span class="c1"># CenterCrop获取224x224图像</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">CenterCrop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">crop_size</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
  <span class="c1"># skimage读取结果通道顺序为RGB，转换为bgr_128需要的BGR顺序</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">RGB2BGR</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
  <span class="c1"># skimage读取数值范围为[0.0,1.0]，调整为bgr需要的数值范围</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">*</span> <span class="mi">255</span>

  <span class="k">return</span> <span class="n">image</span>
</pre></div>
</div>
</section>
<section id="accuracy-optimization">
<span id="id19"></span><h3><span class="section-number">3.2.6.2. </span>精度调优<a class="headerlink" href="#accuracy-optimization" title="永久链接至标题"></a></h3>
<p>基于前文的精度分析工作，如果确定模型的量化精度不符合预期，则主要可分为以下两种情况进行解决：</p>
<ul class="simple">
<li><p>精度有较明显损失（损失大于4%）。
这种问题往往是由于yaml配置不当，校验数据集不均衡等导致的，可以根据我们接下来提供的建议逐一排查。</p></li>
<li><p>精度损失较小（1.5%~3%）。
排除1导致的精度问题后，如果仍然出现精度有小幅度损失，往往是由于模型自身的敏感性导致，可以使用我们提供的精度调优工具进行调优。</p></li>
</ul>
<p>整体精度问题解决流程示意如下图：</p>
<img alt="../_images/accuracy_problem.png" class="align-center" src="../_images/accuracy_problem.png" />
<p><strong>精度有明显损失（4%以上）</strong></p>
<p>通常情况下，明显的精度损失往往是由于各种配置不当引起的，我们建议您依次从pipeline、模型转换配置和一致性三个方面检查。</p>
<ol class="arabic simple">
<li><p>pipeline检查</p></li>
</ol>
<blockquote>
<div><p>pipeline是指您完成数据准备、模型推理、后处理、精度评测Metric的全过程。
在以往的实际问题跟进经验中，我们发现这些部分在原始浮点模型训练阶段中有变动，却没有及时更新到模型转换的精度验证过程来是比较常见的情况。</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>模型转换配置检查</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> 和 <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> 该参数用来区分转后混合异构模型与原始浮点模型需要的数据格式，
需要认真检查是否符合预期，尤其是BGR和RGB通道顺序是否正确。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">norm_type</span></code>、<code class="docutils literal notranslate"><span class="pre">mean_values</span></code>、<code class="docutils literal notranslate"><span class="pre">scale_values</span></code> 等参数是否配置正确。
通过转换配置可以直接在模型中插入mean和scale操作节点，需要确认是否对校验/测试图片进行了重复的mean和scale操作。
重复预处理是错误的易发区。</p></li>
<li><p>模型转换的 <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> 开关是否开启，决定了是否对校准图片进行resize以及颜色转换，我们建议您关闭该参数。</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>数据处理一致性检查</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">skimage.read</span></code> 和 <code class="docutils literal notranslate"><span class="pre">opencv.imread</span></code> 是两种常用图片读取方法，这两种方法在输出的范围和格式上都有所区别。
使用 <code class="docutils literal notranslate"><span class="pre">skimage</span></code> 的图片读取，得到的是RGB通道顺序，取值范围为0~1，数值类型为float；
而使用 <code class="docutils literal notranslate"><span class="pre">opencv</span></code>，得到的是BGR通道顺序，取值范围为0~255，数据类型为uint8。</p></li>
<li><p>在校准数据准备阶段、给应用程序准备应用样本时，我们常使用numpy的tofile序列化数据。
这种方式不会保存shape和类型信息，在加载时都需要手动指定，
需要您确保这些文件的序列化和反序列化过程的数据类型、数据尺寸和数据排布等信息都是一致的。</p></li>
<li><p>推荐您在地平线工具链使用过程中，依然使用原始浮点模型训练验证阶段依赖的数据处理库。
对于鲁棒性较差的模型，不同库实现的功能resize、crop等典型功能都可能引起扰动，进而影响模型精度。</p></li>
<li><p>校验图片集是否合理设置。校准图片集数量应该在百张左右，同时最好可以覆盖到数据分布的各种场合，
例如在多任务或多分类时，校验图片集可以覆盖到各个预测分支或者各个类别。
同时避免偏离数据分布的异常图片（过曝光等）。</p></li>
<li><p>使用 ***_original_float_model.onnx再验证一遍精度，
正常情况下，这个模型的精度应该是与原始浮点模型精度保持小数点后三到五位对齐。
如果验证发现不满足这种对齐程度，则表明您的数据处理需要再仔细检查。</p></li>
</ul>
</div></blockquote>
<p><strong>较小精度损失提升</strong></p>
<p>一般情况下，为降低模型精度调优的难度，我们默认建议您在转换配置中使用的是自动参数搜索功能。
如果发现自动搜索的精度结果仍与预期有一定的差距，较于原始浮点模型的精度损失在1.5%到3%范围左右。
可以分别尝试使用以下建议提高精度：</p>
<ul class="simple">
<li><p>尝试在配置转换中手动指定 <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> ，可以选择 <code class="docutils literal notranslate"><span class="pre">kl</span></code> / <code class="docutils literal notranslate"><span class="pre">max</span></code> 。</p></li>
<li><p>尝试在配置转换中启用 <code class="docutils literal notranslate"><span class="pre">perchannel</span></code> 。</p></li>
<li><p>在 <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> 设定为 <code class="docutils literal notranslate"><span class="pre">max</span></code> 时，
同时配置 <code class="docutils literal notranslate"><span class="pre">max_percentile</span></code> 参数分别为 <code class="docutils literal notranslate"><span class="pre">0.99999</span></code> 、 <code class="docutils literal notranslate"><span class="pre">0.99995</span></code> 、 <code class="docutils literal notranslate"><span class="pre">0.9999</span></code> 、 <code class="docutils literal notranslate"><span class="pre">0.9995</span></code> 、 <code class="docutils literal notranslate"><span class="pre">0.999</span></code> 进行尝试。</p></li>
</ul>
<p>根据以往的实际生产经验，以上策略已经可以应对各种实际问题。
如果经过以上尝试仍然未能解决您的问题，欢迎在地平线唯一官方技术社区（<a class="reference external" href="https://developer.horizon.ai">https://developer.horizon.ai</a>）发帖与我们取得联系，
我们将根据您的具体问题提供更具针对性的指导建议。</p>
</section>
<section id="qat">
<span id="qat-accuracy"></span><h3><span class="section-number">3.2.6.3. </span>使用QAT量化感知训练方案进一步提升模型精度<a class="headerlink" href="#qat" title="永久链接至标题"></a></h3>
<p>如果通过上述分析，并没有发现任何配置上的问题，但是精度仍不能满足要求，则可能是PTQ（即：Post-training Quantization，后量化训练）本身的限制。
这时候我们可以改用QAT（即Quantization Aware Training，量化感知训练）的方式来对模型进行量化。</p>
<p>本小节内容对QAT方案进行详细介绍：</p>
<ul class="simple">
<li><p>首先，<a class="reference internal" href="#about-quantization"><span class="std std-ref">关于量化</span></a> 介绍量化的概念和两种量化方法；</p></li>
<li><p>其次，<a class="reference internal" href="#about-conversion"><span class="std std-ref">关于模型转换</span></a> 介绍地平线模型转换、原始浮点模型和混合异构模型的概念；</p></li>
<li><p>接着，在理解了以上一些概念后，<a class="reference internal" href="#about-quantization-compile"><span class="std std-ref">关于模型量化编译流程</span></a> 一小节内容，让您理解地平线PTQ和QAT方案的关系，便于您可以在不同情况下选择更合适的模型处理方案；</p></li>
<li><p>最后，<a class="reference internal" href="#qat-quantzation-compile"><span class="std std-ref">QAT模型量化编译</span></a> 再展开介绍如何结合PyTorch社区的QAT方案完成量化模型编译。这一小节还给出了 <strong>API接口定义</strong> 和一个 <strong>完整量化示例</strong> 供开发者参考。</p></li>
</ul>
<section id="about-quantization">
<span id="id20"></span><h4><span class="section-number">3.2.6.3.1. </span>关于量化<a class="headerlink" href="#about-quantization" title="永久链接至标题"></a></h4>
<p>目前在GPU上训练的模型大部分都是浮点模型，即参数使用的是float类型存储。
地平线BPU架构的AI芯片使用的是int8的计算精度（业内AI芯片的通用精度），能运行定点量化模型。
那么 <strong>从训练出的浮点精度转为定点模型的过程，我们叫做量化。</strong></p>
<p><strong>量化方法有两种，分别为</strong>：</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>后量化（post training quantization，PTQ）</strong>：
先训练浮点模型，然后使用校准图片计算量化参数，将浮点模型转为量化模型。
该方法简单、快捷，但将浮点模型直接转为量化模型难免会有一些量化损失，地平线浮点转换工具链中提供的后量化工具能做到80%(估计)以上的模型量化误差小于1%。</p></li>
</ol>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">备注</p>
<p>关于PTQ模型的量化和编译流程，本章内容的上述部分已为您做出了详细介绍。</p>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>量化感知训练（quantization aware training，QAT）</strong>：
在浮点训练的时候，就先对浮点模型结构进行干预，增加量化误差，使得模型能够感知到量化带来的损失。
该方法需要用户在全量训练集上重新训练，能有效地降低量化部署的量化误差。
一些社区框架都提供QAT方案，例如pytorch的eager mode方案、pytorch的fx graph方案、tf-lite量化方案等。</p></li>
</ol>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">备注</p>
<p><strong>QAT训练与浮点训练的关系</strong></p>
<p>QAT训练是一种finetune方法，最好是在浮点结果已经拟合的情况下，再用QAT方法提升量化精度。
即用户的训练分为了两个步骤，先训练浮点模型，将模型精度提升到满意的指标；再通过QAT训练，提升量化精度。</p>
<p>为了让模型更好的感知到量化误差，QAT训练需要使用全量的训练数据集。
训练轮数和模型难度相关，大约是原来的浮点训练的1/10。
因为是在浮点模型上finetune，所以QAT训练的学习率尽量和浮点模型的最后几个epoch一致。</p>
</div>
</div></blockquote>
</div></blockquote>
</section>
<section id="about-conversion">
<span id="id21"></span><h4><span class="section-number">3.2.6.3.2. </span>关于模型转换<a class="headerlink" href="#about-conversion" title="永久链接至标题"></a></h4>
<p>模型转换是指将原始浮点模型转换为地平线混合异构模型的过程。
其中会包括模型前处理节点修改、原始模型图优化、模型量化和上板模型编译等过程。</p>
<p><strong>原始浮点模型</strong> （文中部分地方也称为浮点模型）是指您通过TensorFlow/PyTorch等等DL框架训练得到的可用模型，这个模型的计算精度为float32；
目前我们的QAT方案为PyTorch 社区QAT方案，因此只支持PyTorch格式的模型。
PTQ方案只支持Caffe&amp;ONNX模型格式，因此对于TensorFlow/PyTorch等格式的模型，需要先通过转换到ONNX模型后，才能够被地平线的工具进行量化&amp;编译。</p>
<p><strong>混合异构模型</strong> 是一种适合在地平线芯片上运行的模型格式，之所以被称为异构模型是因为它能够支持模型同时在ARM CPU和BPU上执行。
由于在BPU上的运算速度会远大于CPU上的速度，因此会尽可能的将算子放在BPU上运算。
对于BPU上暂时不支持的算子，则会放在CPU上进行运算。</p>
</section>
<section id="about-quantization-compile">
<span id="id22"></span><h4><span class="section-number">3.2.6.3.3. </span>关于模型量化编译流程<a class="headerlink" href="#about-quantization-compile" title="永久链接至标题"></a></h4>
<p>正常的模型量化编译流程如下图所示：</p>
<a class="reference internal image-reference" href="../_images/qat_compile_flow.png"><img alt="../_images/qat_compile_flow.png" class="align-center" src="../_images/qat_compile_flow.png" style="width: 1318.5px; height: 282.0px;" /></a>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p>由于PTQ方式的使用代价小，因此推荐用户首先尝试该方法进行模型量化编译。
若尝试并调优后的模型精度依然无法满足要求，则可以再改为尝试QAT方案。</p>
</div>
</section>
<section id="qat-quantzation-compile">
<span id="id23"></span><h4><span class="section-number">3.2.6.3.4. </span>QAT模型量化编译介绍<a class="headerlink" href="#qat-quantzation-compile" title="永久链接至标题"></a></h4>
<p><strong>社区QAT方案（PyTorch基于Fx Graph的量化方法）</strong></p>
<p>PyTorch在1.8版本以后推出了FX Graph追踪的技术以及基于该技术的量化方案。
相对于之前的Eager Mode方案，该方案全过程都是自动的、可配置的，无需用户修改代码，更适合用户使用。
关于详细介绍，用户可参考pytorch的官方介绍：
<a class="reference external" href="https://pytorch.org/docs/stable/quantization.html">Quantization ‒ PyTorch 1.9.0 documentation</a>。
目前该方案受到很多开发者的青睐，也渐渐变得稳定起来，地平线工具链也提供了部署该方案的能力。</p>
<p><strong>基于FX Graph的社区QAT使用方法</strong></p>
<p>用户调用 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 提供的 <code class="docutils literal notranslate"><span class="pre">prepare_qat_fx</span></code> 函数，设置对应的量化节点参数，即可自动的完成浮点模型到量化模型的转换，函数API：</p>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">prepare_qat_fx</span> <span class="pre">函数：</span></span></dt>
<dd><dl class="simple">
<dt><strong>作用：</strong></dt><dd><p>将浮点模型转为一个可以进行量化感知训练的Prepare模型。</p>
</dd>
<dt><strong>参数：</strong></dt><dd><dl class="simple">
<dt>model：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> 类型的模型，一定是train的状态。</p>
</dd>
<dt>qconfig_dict：</dt><dd><p>声明量化节点的量化方法，例如非对称、per-tensor等方法。</p>
</dd>
<dt>prepare_custom_config_dict：</dt><dd><p>自定义设置prepare过程的配置，例如指定不量化某一层、指定不使用FX追踪某一层、指定某些结构（avgpooling+relu）可以打包量化等。</p>
</dd>
</dl>
</dd>
<dt><strong>返回值：</strong></dt><dd><p>一个可以用于QAT训练的prepare模型。</p>
</dd>
</dl>
</dd></dl>

<p>当用户调用 <code class="docutils literal notranslate"><span class="pre">quantize_fx.prepare_qat_fx</span></code> 时，会进行以下步骤：</p>
<ul class="simple">
<li><p><strong>构建静态图：</strong> 使用fx工具trace整个网络结构（可使用 <code class="docutils literal notranslate"><span class="pre">prepare_custum_config_dict</span></code> 指定某一块不被trace），构建出一个静态的网络结构。</p></li>
<li><p><strong>融合特定网络结构：</strong> 加载默认的和用户定义的 <code class="docutils literal notranslate"><span class="pre">fuse</span> <span class="pre">pattern</span></code> 配置，对网络结构进行遍历、融合，例如PyTorch默认会把conv+bn+relu融合为 <code class="docutils literal notranslate"><span class="pre">instrice.ConvBnRelu</span></code>。</p></li>
<li><p><strong>转换网络结构：</strong> 加载默认的和用户定义的 <code class="docutils literal notranslate"><span class="pre">convert</span> <span class="pre">pattern</span></code> 配置，将网络中特定的网络结构转换为指定的网络结构。</p></li>
<li><p><strong>量化网络结构：</strong> 加载默认的和用户定义的 <code class="docutils literal notranslate"><span class="pre">quantize</span> <span class="pre">pattern</span></code>，在节点指定的位置插入量化感知节点，例如在 <code class="docutils literal notranslate"><span class="pre">quant.ConvBnRelu</span></code> 后面插入 <code class="docutils literal notranslate"><span class="pre">observer</span></code>。</p></li>
<li><p><strong>其他处理</strong>。</p></li>
</ul>
<p><strong>基于FX Graph的社区QAT加载地平线的量化配置</strong></p>
<p>PyTorch社区QAT默认的量化方法不能高效地运行在地平线AI芯片上，用户可以通过加载工具链提供的量化参数调整QAT训练配置。
为了使得QAT模型更容易拟合，地平线还提供了一些调整量化参数的策略，例如配置最后一层卷积为高精度输出等。</p>
<p>用户仅需从 <code class="docutils literal notranslate"><span class="pre">horizon_nn</span></code> 中导入 <code class="docutils literal notranslate"><span class="pre">HorizonQConfig</span></code> 用于设定量化策略。
若要求最后一层卷积进行高精度输出(float32), 则可以调用 <code class="docutils literal notranslate"><span class="pre">adjust_qat_bits</span></code> 接口对模型进行修改。</p>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p>QAT模型的评测、训练和浮点模型一致，但需要注意的是，QAT模型默认每一次推理（无论training还是eval）都会更新量化参数，所以会出现每次评测的结果不一致的现象。
我们需要在训练或者评测前，手动修改QAT模型的状态，让QAT模型仅在训练时更新量化参数。
用户可以通过导入工具链封装的 <code class="docutils literal notranslate"><span class="pre">set_qat_eval</span></code>、 <code class="docutils literal notranslate"><span class="pre">set_qat_training</span></code> 来完成修改。</p>
</div>
<p><strong>API接口定义</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 调整模型中的输入部分量化策略</span>
<span class="k">def</span> <span class="nf">disable_input_fake_quant</span><span class="p">(</span><span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disable input fake quantize of a given graph module.</span>

<span class="sd">    Args:</span>
<span class="sd">        graph_module: graph module</span>

<span class="sd">    Returns:</span>
<span class="sd">        Return graph module of which the input fake quantize is disabled</span>
<span class="sd">    &quot;&quot;&quot;</span>

<span class="c1"># 调整模型中的输出部分量化策略</span>
<span class="k">def</span> <span class="nf">disable_output_fake_quant</span><span class="p">(</span><span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disable output fake quantize of a given graph module.</span>

<span class="sd">    Args:</span>
<span class="sd">        graph_module: graph module</span>

<span class="sd">    Returns:</span>
<span class="sd">        Return graph module of which the output fake quantize is disabled</span>
<span class="sd">    &quot;&quot;&quot;</span>

<span class="c1">#  export_to_onnx 接口的api描述</span>
<span class="k">def</span> <span class="nf">export_to_onnx</span><span class="p">(</span>
  <span class="n">pytorch_model</span><span class="p">,</span>
  <span class="n">input_data</span><span class="p">,</span>
  <span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
  <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">export_name</span><span class="o">=</span><span class="s2">&quot;onnx_temp.onnx&quot;</span><span class="p">,</span>
  <span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Export ONNX model from pytorch model.</span>
<span class="sd">  Args:</span>
<span class="sd">      pytorch_model: pytorch model</span>
<span class="sd">      input_data: dummy input data for onnx model output</span>
<span class="sd">      opset_version: export onnx version, 11 is default.</span>
<span class="sd">      input_names: list of model input names</span>
<span class="sd">      output_names: list of model output names</span>
<span class="sd">      export_name: name of the onnx model generated</span>
<span class="sd">  Return:</span>
<span class="sd">      None</span>
<span class="sd">  &quot;&quot;&quot;</span>

<span class="c1"># 将qat模型转为定点onnx模型, 储存在OnnxModule中. 该接口仍在开发中,</span>
<span class="c1"># 更推荐用户将qat模型导出至onnx, 使用hb_mapper makertbin工具进行定点和编译</span>
<span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
            <span class="n">dummy_input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">march</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bernoulli2&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OnnxModule</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      desc:</span>
<span class="sd">          convert qat model into quantized model.</span>
<span class="sd">      :param model: torch qat model, onnx model, or onnx model path</span>
<span class="sd">      :param dummy_input: the example input, whose type could be dict, Tensor, list.</span>
<span class="sd">      :param march: on board march, default is bernoulli2, selected form[&#39;bernoulli2&#39;, &#39;bayes&#39;]</span>
<span class="sd">      :param preprocess_setting: if the data input is int8 type (bgr_128, yuv444_128 etc.), then preprocess_setting needs to be set. Please refer to the example below.</span>
<span class="sd">      :return: OnnxModule, which could run with torch dataloader as same as qat model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<span class="c1"># 将onnx模型编译为混合异构模型. 该接口仍在开发中,</span>
<span class="c1"># 更推荐用户将qat模型导出至onnx, 使用hb_mapper makertbin工具进行定点和编译</span>
<span class="k">def</span> <span class="nf">compile</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">OnnxModule</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="n">dummy_input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model.bin&quot;</span><span class="p">,</span>
    <span class="n">march</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bernoulli2&quot;</span><span class="p">,</span>
    <span class="n">rt_input_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;yuv444&quot;</span><span class="p">,</span>
    <span class="n">rt_input_layout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
    <span class="n">opt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;O0&quot;</span><span class="p">,</span>
    <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        desc:</span>
<span class="sd">            compile quantized model to bin model.</span>
<span class="sd">        :param model: onnx model, or onnx model file path.</span>
<span class="sd">        :param dummy_input: tensor for dump onnx model from torch.nn.Module</span>
<span class="sd">        :param output_name: export bin model filename</span>
<span class="sd">        :param march: on board march, default is bernoulli2, selected form[&#39;bernoulli2&#39;, &#39;bayes&#39;]</span>
<span class="sd">        :param rt_input_type: runtime input type, selected from [nv12, gray, rgb, bgr, feature_map, yuv444].</span>
<span class="sd">        :param rt_input_layout: runtime input layout, selected from [NHWC, NCHW].</span>
<span class="sd">        :param opt: optimized level, select from [&#39;O0&#39;, &#39;O1&#39;, &#39;O2&#39;, &#39;O3&#39;], high level mean long time and better performance.</span>
<span class="sd">        :param debug: debug model allow user to dump all output.</span>
<span class="sd">        :param compiler_parameters: compiler related parameters,</span>
<span class="sd">            only support dual-core option for now. Compile the dual-core model using compiler_parameters = {&quot;core_num&quot; : 2 }.</span>

<span class="sd">        :return: None</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>我们更推荐用户在训练完毕qat模型之后, 将模型导出为 qat_onnx 模型, 使用hb_mapper makertbin工具进行后续的量化&amp;编译操作。</p>
<p>如果您需要导出qat onnx模型，需要调用export_to_onnx api来进行，否则可能会出现导出模型不正确的情况。</p>
</div>
<p><strong>完整量化示例</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.quantization.quantize_fx</span> <span class="kn">import</span> <span class="n">prepare_qat_fx</span>
<span class="kn">from</span> <span class="nn">hemat.torch.quantization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HorizonQConfig</span><span class="p">,</span>
    <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
    <span class="n">disable_output_fake_quant</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">hemat.torch.quantization</span> <span class="kn">import</span> <span class="n">set_qat_eval</span><span class="p">,</span> <span class="n">set_qat_train</span>
<span class="kn">from</span> <span class="nn">hemat.torch.quantization</span> <span class="kn">import</span> <span class="n">export_to_onnx</span><span class="p">,</span> <span class="n">convert</span>
<span class="kn">from</span> <span class="nn">horizon_tc_ui.torch</span> <span class="kn">import</span> <span class="nb">compile</span>

<span class="k">def</span> <span class="nf">load_model</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">accuracy</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">prepare_data_loaders</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="n">prepare_data_loaders</span><span class="p">()</span>
<span class="n">float_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span><span class="c1"># 用户训练好的模型</span>

<span class="c1"># 按照HorizonQConfig配置量化策略</span>
<span class="n">qat_model</span> <span class="o">=</span> <span class="n">prepare_qat_fx</span><span class="p">(</span><span class="n">float_model</span><span class="p">,</span> <span class="n">HorizonQConfig</span><span class="p">,</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">)</span>

<span class="c1"># 设置最后一层卷积高精度输出 (若无此要求, 该步骤可省略)</span>
<span class="n">qat_model</span> <span class="o">=</span> <span class="n">disable_output_fake_quant</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>

<span class="c1"># 检查一下QAT模型结构是否正确</span>
<span class="c1"># print(qat_model)</span>

<span class="k">for</span> <span class="n">nepoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">):</span>
    <span class="c1"># 设置模型为训练模式, 开启量化参数更新</span>
    <span class="n">qat_model</span> <span class="o">=</span> <span class="n">set_qat_train</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>

    <span class="c1"># 设置模型为评测模式, 停止量化参数更新</span>
    <span class="n">qat_model</span> <span class="o">=</span> <span class="n">set_qat_eval</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>
    <span class="n">top1</span><span class="p">,</span> <span class="n">top5</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>

<span class="c1"># 将训练好的模型进行保存</span>
<span class="n">save_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;state_dict&#39;</span><span class="p">:</span><span class="n">qat_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()}</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_dict</span><span class="p">,</span><span class="s2">&quot;qat_best.pth&quot;</span><span class="p">)</span>

<span class="c1"># 将qat模型导出为 onnx 格式</span>
<span class="n">dummy_data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">img</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)))</span>
<span class="n">export_to_onnx</span><span class="p">(</span><span class="n">qat_model</span><span class="p">,</span> <span class="n">dummy_data</span><span class="p">,</span> <span class="n">export_name</span><span class="o">=</span><span class="s2">&quot;qat_model.onnx&quot;</span><span class="p">)</span>

<span class="c1"># 得到 qat_model.onnx 之后, 就可以使用 hb_mapper makertbin工具进行后续的定点化及编译流程了.</span>
<span class="c1"># 如果使用该方法的话, 流程至此就可以结束了.</span>

<span class="c1"># 上述储存的pytorch模型在储存后读取方式的介绍.</span>
<span class="n">float_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
<span class="n">qat_model</span> <span class="o">=</span> <span class="n">prepare_fx_qat</span><span class="p">(</span><span class="n">float_model</span><span class="p">,</span> <span class="n">HorizonQConfig</span><span class="p">,</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;qat_best.pth&#39;</span><span class="p">)[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">]</span>
<span class="n">qat_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

<span class="c1"># 若不打算导出onnx模型, 由hb_mapper makertbin工具进行后续转换, 则需要使用</span>
<span class="c1"># convert + compile 接口的组合.</span>
<span class="c1"># 如果输入数据为yuv444 这种定点输入的数据, 则需要配置preprocess_setting</span>
<span class="c1"># expected_input_type 为上板输入的数据类型, yuv444_128为yuv444数据减去128结果, 数据类型为int8</span>
<span class="n">preprocess_setting</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;img&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;means&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">128.0</span><span class="p">]),</span>
        <span class="s2">&quot;scales&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="o">/</span> <span class="mf">128.0</span><span class="p">]),</span>
        <span class="s2">&quot;original_input_type&quot;</span><span class="p">:</span> <span class="s2">&quot;yuv444&quot;</span><span class="p">,</span>
        <span class="s2">&quot;expected_input_type&quot;</span><span class="p">:</span> <span class="s2">&quot;yuv444_128&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span>
    <span class="n">qat_model</span><span class="p">,</span>  <span class="c1"># qat model</span>
    <span class="n">dummy_data</span><span class="p">,</span>  <span class="c1"># dummy data, which is the input data to feed the qat model</span>
    <span class="n">march</span><span class="o">=</span><span class="s1">&#39;bayes&#39;</span>  <span class="c1"># bayes for j5</span>
    <span class="n">preprocess_setting</span><span class="o">=</span><span class="n">preprocess_setting</span><span class="p">,</span>  <span class="c1">#preprocess settings for int inputs</span>
<span class="p">)</span>

<span class="c1"># 将定点onnx模型转为异构bin模型</span>
<span class="nb">compile</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span>
        <span class="s2">&quot;test.bin&quot;</span><span class="p">,</span>
        <span class="n">march</span><span class="o">=</span><span class="s2">&quot;bayes&quot;</span><span class="p">,</span>
        <span class="n">rt_input_type</span><span class="o">=</span><span class="s2">&quot;yuv444&quot;</span><span class="p">,</span>
        <span class="n">rt_input_layout</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
        <span class="n">opt</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="op-restrictions">
<span id="id24"></span><h2><span class="section-number">3.2.7. </span>算子约束<a class="headerlink" href="#op-restrictions" title="永久链接至标题"></a></h2>
<p>详见： <a class="reference external" href="../../supported_op_list_and_restrictions/supported_op_list_and_restrictions_release.xlsx">算子约束列表</a> Excel表格。</p>
</section>
<section id="other-tools">
<span id="id26"></span><h2><span class="section-number">3.2.8. </span>其他模型工具（可选）<a class="headerlink" href="#other-tools" title="永久链接至标题"></a></h2>
<p>本节将对上述常规流程中不涉及的工具做统一介绍，这些工具是为某些特定需要提供，如果您有类似的需求，可以酌情选用。</p>
<section id="id27">
<h3><span class="section-number">3.2.8.1. </span>模型打包<a class="headerlink" href="#id27" title="永久链接至标题"></a></h3>
<p>模型打包提供了将多个转换后bin模型整合成一个文件的功能，
应用开发阶段，我们也为打包后模型提供了相关的接口，在您业务场景中模型比较多的时候可以使用。
打包工具 <code class="docutils literal notranslate"><span class="pre">hb_pack</span></code> 使用命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_pack -o dst_name.bin  to_pack_1.bin to_pack_2.bin
</pre></div>
</div>
<p>工具使用 <code class="docutils literal notranslate"><span class="pre">-o</span></code> 参数指定打包后文件名称。</p>
<p>需要打包的bin模型依次在命令尾部添加，使用空格分隔即可。</p>
</section>
<section id="id28">
<h3><span class="section-number">3.2.8.2. </span>模型信息查看<a class="headerlink" href="#id28" title="永久链接至标题"></a></h3>
<p>模型信息查看工具可以提供模型转换时使用的配置参数信息，其使用命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_model_info model_name.bin
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">hb_model_info</span></code> 后加上模型名称即可，如果您使用的是打包后模型，需要带上一个在 <code class="docutils literal notranslate"><span class="pre">hb_model_info</span></code> 后先加上一个 <code class="docutils literal notranslate"><span class="pre">-p</span></code> 参数。
命令执行后会输出一些转换环境信息和转换配置中使用的配置参数信息，转换配置参数解读请参考
<a class="reference internal" href="#makertbin"><span class="std std-ref">使用 hb_mapper makertbin 工具转换模型</span></a> 部分介绍。</p>
</section>
<section id="bin">
<h3><span class="section-number">3.2.8.3. </span>bin模型节点修改<a class="headerlink" href="#bin" title="永久链接至标题"></a></h3>
<p>出于某些极大尺寸输入场景下的极致性能需求，部分输入的量化和转置操作可以融合在数据前处理中一并完成。
此时您可以选择使用 <code class="docutils literal notranslate"><span class="pre">hb_model_modifier</span></code> 工具移除这些节点，使用命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_model_modifier  bin_file  -r <span class="o">{</span>node_name<span class="o">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">-r</span></code> 参数指定需要删除的节点名称。</p>
<p><code class="docutils literal notranslate"><span class="pre">-o</span></code> 指定删除节点后产生的新模型文件名称。</p>
<p>需要删除的节点名称必须与bin模型中的名称完全一致，且每次调用工具只能删除一个节点。
在 <a class="reference internal" href="#hb-perf"><span class="std std-ref">使用hb_perf工具估计性能</span></a> 部分介绍的 <strong>BIN Model Structure</strong> 中可以查看节点名称。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>这种操作会使得前文讲述的模型输入格式要求等信息失效，一般不建议使用。
该节点中的量化信息会储存在model中, 可以用 <cite>hb_model_info</cite> 工具来查看被删除节点的 <cite>mean</cite> 和 <cite>scale</cite> 值, 并据此来自行实现处理逻辑。</p>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="general_description.html" class="btn btn-neutral float-left" title="3.1. 概述" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="custom_op.html" class="btn btn-neutral float-right" title="3.3. 自定义算子开发" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, Horizon Robotics.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>