

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>社区QAT &mdash; HAT 1.5.5 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Unet分割模型训练" href="unet.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> HAT
          

          
          </a>

          
            
            
              <div class="version">
                1.5.5
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/introduction.html">简介</a></li>
</ul>
<p class="caption"><span class="caption-text">Framework</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../framework/framework.html">框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="../framework/engine.html">执行引擎</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/registry.html">注册机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/config.html">config 文件介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/opts.html">通过命令行覆盖config参数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/calops.html">计算量工具</a></li>
</ul>
<p class="caption"><span class="caption-text">ModelZoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/model_zoo.html">ModelZoo</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/data.html">hat.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/callbacks.html">hat.callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/engine.html">hat.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/models.html">hat.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/metrics.html">hat.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/visualize.html">hat.visualize</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="scripts.html">执行脚本</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">VargConvNet分类模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="fcos.html">FCOS检测模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="deeplab.html">Deeplab分割模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="fastscnn.html">Fastscnn分割模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="unet.html">Unet分割模型训练</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">社区QAT</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">训练流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">配置文件</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#calibration">calibration配置</a></li>
<li class="toctree-l3"><a class="reference internal" href="#custom-config-dict">配置custom_config_dict的高阶技巧</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">qat配置</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id4">模型结构</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">编写模型注意事项</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">模型结构检查</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id7">调参技巧</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id8">转换流程</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">HAT</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>社区QAT</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/examples/community_qat.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="qat">
<h1>社区QAT<a class="headerlink" href="#qat" title="Permalink to this headline">¶</a></h1>
<p>QAT的基本原理是在模型中插入量化节点，使得模型在训练中可以感知到量化误差，减少量化损失的精度。社区QAT有eager模式和fx graph模式，eager模式需要用户自己编写fuse_model和set_qconfig方法，上手难度大。而fx graph模式不需要编写这两个方法，使用起来更加方便，所以HAT里的社区QAT功能是基于fx graph模式开发的。这篇教程以fcos-efficientnetb0为例，告诉大家如何使用HAT算法包训练一个社区QAT检测模型。同理，fcos-efficientnetb2，fcos-efficientnetb3，deeplabv3plus-efficientnetm2也可以用同样的方法训练QAT模型。默认用户已经熟悉如何使用HAT训练模型，如果有QAT以外的问题，请参考其他与模型训练相关的文档。</p>
<div class="section" id="id1">
<h2>训练流程<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>整个训练流程分为以下几步：</p>
<ol class="arabic simple">
<li><p>训练float模型：这一步请参考其他训练浮点模型的说明文档，这里不再赘述。</p></li>
<li><p>训练calibration模型：QAT插入伪量化节点的初始scale并不可靠，而calibration的主要作用是在训练开始前调整这些scale。虽然calibration不是必须的，但绝大多数情况下，calibration对QAT精度都是有一定提升的，有益无害。</p></li>
<li><p>训练qat模型。</p></li>
</ol>
<p>社区qat的入口为 <code class="docutils literal notranslate"><span class="pre">tools/train.py</span></code>，提供 float -&gt; calibration -&gt; qat 三个阶段的功能。此入口对应上述训练流程的运行命令为：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="n">tools</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">step</span> <span class="nb">float</span> <span class="o">--</span><span class="n">config</span> <span class="n">configs</span><span class="o">/</span><span class="n">detection</span><span class="o">/</span><span class="n">fcos</span><span class="o">/</span><span class="n">fcos_efficientnetb0_mscoco</span><span class="o">.</span><span class="n">py</span>
<span class="n">python3</span> <span class="n">tools</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">step</span> <span class="n">calibration</span> <span class="o">--</span><span class="n">config</span> <span class="n">configs</span><span class="o">/</span><span class="n">detection</span><span class="o">/</span><span class="n">fcos</span><span class="o">/</span><span class="n">fcos_efficientnetb0_mscoco</span><span class="o">.</span><span class="n">py</span>
<span class="n">python3</span> <span class="n">tools</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">step</span> <span class="n">qat</span> <span class="o">--</span><span class="n">config</span> <span class="n">configs</span><span class="o">/</span><span class="n">detection</span><span class="o">/</span><span class="n">fcos</span><span class="o">/</span><span class="n">fcos_efficientnetb0_mscoco</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>注意：calibration与qat模型相比于float模型多了更多的参数，如果发生显存不足的情况，需要将配置文件中的batch_size_per_gpu改小一些。以3090显卡为例, fcos_efficientnetb0推荐使用的QAT batch size为24，fcos_efficientnetb2推荐使用的QAT batch size为6，fcos_efficientnetb3推荐使用的QAT batch size为2。如果需要更大的batch size提升训练速度，那么需要选用显存更大的硬件。</p>
</div>
<div class="section" id="id2">
<h2>配置文件<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">configs/detection/fcos/fcos_efficientnetb0_mscoco.py</span></code> 提供了一个配置文件样例。使用社区qat需要在配置中添加 <code class="docutils literal notranslate"><span class="pre">calibration_trainer</span></code>， <code class="docutils literal notranslate"><span class="pre">calibration_solver</span></code>， <code class="docutils literal notranslate"><span class="pre">qat_trainer</span></code> 和 <code class="docutils literal notranslate"><span class="pre">qat_solver</span></code>。 添加的方式与浮点的trainer和solver一致，重点解释独有的字段 <code class="docutils literal notranslate"><span class="pre">custom_config_dict</span></code>。<code class="docutils literal notranslate"><span class="pre">custom_config_dict</span></code> 中包含五部分内容 <code class="docutils literal notranslate"><span class="pre">modules</span></code>, <code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code>，<code class="docutils literal notranslate"><span class="pre">prepare_custom_config_dict</span></code>， <code class="docutils literal notranslate"><span class="pre">quantize_input</span></code> 和 <code class="docutils literal notranslate"><span class="pre">quantize_output</span></code> 。<code class="docutils literal notranslate"><span class="pre">modules</span></code> 用于指定需要prepare的模块，如果不指定，则默认会prepare整个模型，每个需要prepare的模块都可以设置自己的 <code class="docutils literal notranslate"><span class="pre">custom_config_dict</span></code>。<code class="docutils literal notranslate"><span class="pre">modules</span></code> 不能与其余四项同时出现，因为如果指定了 <code class="docutils literal notranslate"><span class="pre">modules</span></code> ，那么当前模型不需要整体prepare。<code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code> 和 <code class="docutils literal notranslate"><span class="pre">prepare_custom_config_dict</span></code> 与pytorch中的prepare接口参数完全一致。</p>
<div class="section" id="calibration">
<h3>calibration配置<a class="headerlink" href="#calibration" title="Permalink to this headline">¶</a></h3>
<p>以fcos-efficientnetb0为例, 做calibration时，需要prepare <code class="docutils literal notranslate"><span class="pre">backbone</span></code> , <code class="docutils literal notranslate"><span class="pre">neck</span></code> , <code class="docutils literal notranslate"><span class="pre">head</span></code> 三部分。neck的输入不需要量化，因为neck的输入直接来自于backbone，而backbone的输出已经量化过了，head同理也不需要量化输入。head的输出不需要量化，因为芯片支持高精度输出。<code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code> 使用hemat提供的calibration qconfig。<code class="docutils literal notranslate"><span class="pre">prepare_custom_config_dict</span></code> 使用hemat提供的HorizonPrepareCustomConfigDict。这些配置已为用户处理了一些特殊情况，也将部分算子的量化方式与BPU对齐，如无特殊需求，不需要用户修改。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">custom_config_dict</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;backbone&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;qconfig_dict&quot;</span><span class="p">:</span> <span class="n">calibration_qconfig_dict</span><span class="p">,</span>
            <span class="s2">&quot;prepare_custom_config_dict&quot;</span><span class="p">:</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;neck&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;qconfig_dict&quot;</span><span class="p">:</span> <span class="n">calibration_qconfig_dict</span><span class="p">,</span>
            <span class="s2">&quot;prepare_custom_config_dict&quot;</span><span class="p">:</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
            <span class="s2">&quot;quantize_input&quot;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;qconfig_dict&quot;</span><span class="p">:</span> <span class="n">calibration_qconfig_dict</span><span class="p">,</span>
            <span class="s2">&quot;prepare_custom_config_dict&quot;</span><span class="p">:</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
            <span class="s2">&quot;quantize_input&quot;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
            <span class="s2">&quot;quantize_output&quot;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="p">},</span>
</pre></div>
</div>
</div>
<div class="section" id="custom-config-dict">
<h3>配置custom_config_dict的高阶技巧<a class="headerlink" href="#custom-config-dict" title="Permalink to this headline">¶</a></h3>
<p>如果对模型进行合理的模块划分，绝大部分情况下是可以通过简单的配置完成QAT流程的，如果需要更复杂的功能，参考下方内容对 <code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code> 和 <code class="docutils literal notranslate"><span class="pre">prepare_custom_config_dict</span></code> 进行更高级的配置。</p>
<p><code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code> 支持对象类型，模块名正则，模块名，全局四种配置方式，优先级从高到低。用户可以通过这种方式指定哪个模块用哪个qconfig。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">qconfig</span><span class="p">,</span>
    <span class="s2">&quot;object_type&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">),</span>
        <span class="o">...</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;module_name&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">bar</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">)</span>
        <span class="o">...</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;module_name_regex&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">bar</span><span class="o">.</span><span class="n">conv</span><span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="mi">9</span><span class="p">]</span><span class="o">+</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">)</span>
        <span class="o">...</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
<p>用户可以参考hemat提供的HorizonPrepareCustomConfigDict添加自己需要的配置，具体来说，<code class="docutils literal notranslate"><span class="pre">prepare_custom_config_dict</span></code> 用法如下，</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># 会被fx trace，但不会进入的module</span>
    <span class="s2">&quot;standalone_module_name&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;standalone_module_class&quot;</span><span class="p">:</span> <span class="p">[],</span>

    <span class="c1"># 不会被fx trace的module</span>
    <span class="s2">&quot;non_traceable_module_name&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;non_traceable_module_class&quot;</span><span class="p">:</span> <span class="p">[],</span>

    <span class="c1"># 额外的fuse方法</span>
    <span class="s2">&quot;additional_fuser_method_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">)</span> <span class="n">fuse_conv_bn</span>
    <span class="p">},</span>

    <span class="c1"># 额外的模块替换</span>
    <span class="s2">&quot;additional_qat_module_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">intrinsic</span><span class="o">.</span><span class="n">ConvBn2d</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">qat</span><span class="o">.</span><span class="n">ConvBn2d</span>
    <span class="p">},</span>

    <span class="c1"># 额外的fuse pattern</span>
    <span class="s2">&quot;additional_fusion_pattern&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="n">ConvReluFusionhandler</span>
    <span class="p">},</span>

    <span class="c1"># 额外的quantize pattern</span>
    <span class="s2">&quot;additional_quant_pattern&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span> <span class="n">ConvReluQuantizeHandler</span><span class="p">,</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span> <span class="n">ConvReluQuantizeHandler</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># 没有在forward中使用的属性会被丢掉，使用这一项保留必要的属性</span>
    <span class="s2">&quot;preserved_attributes&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;preserved_attr&quot;</span><span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h3>qat配置<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>在做qat时，需要固定calibration得到的activation fake quantize，这里``qconfig_dict`` 使用hemat提供的HorizonCalibratedQConfig。其余配置与calibration完全一致。如果未使用calibration，则``qconfig_dict`` 使用hemat提供的HorizonQConfig。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">custom_config_dict</span><span class="o">=</span><span class="p">{</span>
    <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;backbone&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;qconfig_dict&quot;</span><span class="p">:</span> <span class="n">qat_qconfig_dict</span><span class="p">,</span>
            <span class="s2">&quot;prepare_custom_config_dict&quot;</span><span class="p">:</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;neck&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;qconfig_dict&quot;</span><span class="p">:</span> <span class="n">qat_qconfig_dict</span><span class="p">,</span>
            <span class="s2">&quot;prepare_custom_config_dict&quot;</span><span class="p">:</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
            <span class="s2">&quot;quantize_input&quot;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">&quot;head&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;qconfig_dict&quot;</span><span class="p">:</span> <span class="n">qat_qconfig_dict</span><span class="p">,</span>
            <span class="s2">&quot;prepare_custom_config_dict&quot;</span><span class="p">:</span> <span class="n">HorizonPrepareCustomConfigDict</span><span class="p">,</span>
            <span class="s2">&quot;quantize_input&quot;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
            <span class="s2">&quot;quantize_output&quot;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="p">},</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id4">
<h2>模型结构<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>fx graph模式的社区qat不需要编写fuse_model和set_qconfig，但由于fx自身的局限性，用户在编写模型的forward方法时，需要注意与fx的适配。HAT中的fcos已为用户处理了模型与pytorch fx的适配问题，但如果用户有自定义模型的需求，那么需要在编写模型的时候注意这些与fx有关的适配问题，并且在prepare完模型之后检查伪量化节点是否已经正确插入。</p>
<div class="section" id="id5">
<h3>编写模型注意事项<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>避免在forward中编写不运行在training状态的逻辑。社区的prepare_qat方法需要模型处于training状态，如果此时进行trace，那么生成的graph module会丢失这一部分与training无关的逻辑。当然，如果确认这一部分逻辑丢掉不会对evaluation等其他阶段产生影响，不修改也没有关系。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="c1"># 这一部分逻辑不会存在于prepare之后的graph module中</span>
        <span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p>fx不支持动态控制流，避免在forward中使用与动态输入有关的if、for、assert… 社区关于这个问题的讨论见<a class="reference external" href="https://pytorch.org/docs/stable/fx.html#dynamic-control-flow">这里</a>。事实上，绝大部分与控制流相关的动态输入都并非真正的动态，比如：height, width … 这些都可以以成员变量的形式预先存储在模型中。如果实在无法避免与动态输入相关的控制流，可以将这部分逻辑写为一个函数，使用wrap方法装饰起来，用法详见<a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.wrap">这里</a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)</span>

<span class="c1"># assert中用到了inputs，会报如下错误：</span>
 <span class="n">File</span> <span class="n">pytorch</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">fx</span><span class="o">/</span><span class="n">proxy</span><span class="o">.</span><span class="n">py</span><span class="p">,</span> <span class="n">line</span> <span class="mi">85</span><span class="p">,</span> <span class="ow">in</span> <span class="n">to_bool</span>
    <span class="k">raise</span> <span class="n">TraceError</span><span class="p">(</span><span class="s1">&#39;symbolically traced variables cannot be used as inputs to control flow&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">proxy</span><span class="o">.</span><span class="n">TraceError</span> <span class="n">symbolically</span> <span class="n">traced</span> <span class="n">variables</span> <span class="n">cannot</span> <span class="n">be</span> <span class="n">used</span> <span class="k">as</span> <span class="n">inputs</span> <span class="n">to</span> <span class="n">control</span> <span class="n">flow</span>
</pre></div>
</div>
<ul class="simple">
<li><p>不支持trace部分python内置方法，比如：len。可以使用wrap()修饰不需要被trace的方法，社区关于这个问题的讨论见<a class="reference external" href="https://pytorch.org/docs/stable/fx.html#non-torch-functions">这里</a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 不用wrap的话，fx会尝试trace内置的len方法</span>
<span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="s1">&#39;len&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># 报如下错误：</span>
 <span class="n">File</span> <span class="n">pytorch</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">fx</span><span class="o">/</span><span class="n">proxy</span><span class="o">.</span><span class="n">py</span><span class="p">,</span> <span class="n">line</span> <span class="mi">161</span><span class="p">,</span> <span class="ow">in</span> <span class="fm">__len__</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;len&#39;</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">supported</span> <span class="ow">in</span> <span class="n">symbolic</span> <span class="n">tracing</span> <span class="n">by</span> <span class="n">default</span><span class="o">.</span> <span class="n">If</span> <span class="n">you</span> <span class="n">want</span>
<span class="ne">RuntimeError</span> <span class="s1">&#39;len&#39;</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">supported</span> <span class="ow">in</span> <span class="n">symbolic</span> <span class="n">tracing</span> <span class="n">by</span> <span class="n">default</span><span class="o">.</span> <span class="n">If</span> <span class="n">you</span> <span class="n">want</span> <span class="n">this</span> <span class="n">call</span> <span class="n">to</span> <span class="n">be</span> <span class="n">recorded</span><span class="p">,</span> <span class="n">please</span> <span class="n">call</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">wrap</span><span class="p">(</span><span class="s1">&#39;len&#39;</span><span class="p">)</span> <span class="n">at</span> <span class="n">module</span> <span class="n">scope</span>
</pre></div>
</div>
<ul class="simple">
<li><p>参数共享问题。以一个简单的例子说明：假设我们有一个conv被两个BN共享，prepare QAT模型时，会先做fuse操作，在匹配到conv+bn的模式之后，conv和第一个BN会被标记为matched，第二个bn将无法被conv+bn的pattern匹配，模型变为convBN和一个单独的BN，所以第二个BN的分支会出现一些问题，导致模型预测完全错误。pytorch在fuse这里的代码中留了todo，后续版本应该会解决这个问题。在当前版本，为了避免这个问题，推荐重点检查参数共享中有无处理完一个分支后影响另一个分支的情况。以上述例子来说，这类参数共享问题的解决方法有两种：1. 编写fuse pattern，并通过custom_config_dict传入，详细做法见社区QAT源码。2. 将QAT模型的conv拆开，这样两个BN都有各自对应conv。实际使用中更推荐第二种做法，只需要在代码中加上少量deepcopy即可解决问题。</p></li>
</ul>
</div>
<div class="section" id="id6">
<h3>模型结构检查<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>为避免踩坑未知错误，训练模型之前最好进行如下检查：</p>
<ul class="simple">
<li><p>打印模型，检查量化节点的位置是否正确。</p></li>
<li><p>查看输出的量化节点是否已经禁用。如果没有禁用，需要将对应模块的``quantize_output`` 置为false。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 输出量化节点的fake_quant_enabled应为0</span>
<span class="p">(</span><span class="n">conv_centerness_4_activation_post_process_0</span><span class="p">):</span> <span class="n">FixedActivationFakeQuantize</span><span class="p">(</span>
    <span class="n">fake_quant_enabled</span><span class="o">=</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span> <span class="n">observer_enabled</span><span class="o">=</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span> <span class="n">quant_min</span><span class="o">=-</span><span class="mi">128</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span><span class="p">,</span> <span class="n">ch_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">]),</span> <span class="n">zero_point</span><span class="o">=</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
    <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">min_val</span><span class="o">=</span><span class="n">inf</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=-</span><span class="n">inf</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>检查非必要模块是否插入了量化节点。loss,post_process等与inference无关或没有量化需求的模块不需要插入量化节点，否则会引入不必要的误差。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 这些模块均没有插入量化节点</span>
<span class="p">(</span><span class="n">loss_cls</span><span class="p">):</span> <span class="n">FocalLoss</span><span class="p">()</span>
<span class="p">(</span><span class="n">loss_reg</span><span class="p">):</span> <span class="n">GIoULoss</span><span class="p">()</span>
<span class="p">(</span><span class="n">loss_centerness</span><span class="p">):</span> <span class="n">CrossEntropyLossV2</span><span class="p">()</span>
<span class="p">(</span><span class="n">post_process</span><span class="p">):</span> <span class="n">FCOSDecoder</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id7">
<h2>调参技巧<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>batch size 尽可能大一些，最好打满显存。</p></li>
<li><p>确保输入数据分布合理，均值为0，最好是均匀分布，其次为高斯分布，避免长尾分布。</p></li>
<li><p>weight decay一般设置为5e-5，可根据实际实验情况调整。</p></li>
<li><p>learning rate一般从0.001左右开始设置，可根据实际实验情况调整。一般可以搭配StepLrUpdater做1-2次scale=0.1的decay。</p></li>
<li><p>learning rate的最小值最好不要小于1e-6</p></li>
<li><p>不推荐使用warmup。warmup初期学习率过小，对QAT几乎没有加成，甚至会降低QAT精度。</p></li>
<li><p>epoch长度不固定，一般选为float epoch大小的十分之一到二分之一不等。</p></li>
<li><p>最好的QAT模型一般在第一个epoch结果的基础上提升不超过3个点，如果第一个epoch的指标较低，那么基本可以断定最后模型的结果不会很好。</p></li>
<li><p>减弱data augmentation有时效果较好。</p></li>
<li><p>多选用不同epoch的浮点模型做QAT，有时并非最好的浮点模型就能训出最好的QAT模型。最好的浮点模型往往处于过拟合的边缘，此时进行QAT不一定最好。</p></li>
<li><p>调大averaging_constant，这样可以增加当前观察值对scale的影响，在scale初始化不靠谱的情况下，这种方法往往非常有效。</p></li>
<li><p>calibration对QAT效果的提升非常大，一般是有益无害的。</p></li>
<li><p>如果单次训练的batch size较小，固定住BN的均值和方差可能取得意想不到的效果。</p></li>
</ul>
</div>
<div class="section" id="id8">
<h2>转换流程<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>得到QAT模型之后，需要对模型进行转换和编译才能上板，我们提供了配套的工具，入口为 <code class="docutils literal notranslate"><span class="pre">tools/convert.py</span></code>。运行如下命令即可转换fcos qat模型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">tools</span><span class="o">/</span><span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config</span> <span class="n">configs</span><span class="o">/</span><span class="n">detection</span><span class="o">/</span><span class="n">fcos</span><span class="o">/</span><span class="n">fcos_efficientnetb0_mscoco</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">qat_model_path</span> <span class="n">tmp_models</span><span class="o">/</span><span class="n">fcos_efficentnetb0_mscoco</span><span class="o">/</span><span class="n">qat</span><span class="o">-</span><span class="n">checkpoint</span><span class="o">-</span><span class="n">best</span><span class="o">.</span><span class="n">pth</span><span class="o">.</span><span class="n">tar</span> <span class="o">--</span><span class="n">output_dir</span> <span class="n">converted_model</span> <span class="o">--</span><span class="n">march</span> <span class="n">bernoulli2</span>
</pre></div>
</div>
<p>该工具需要指定模型的配置文件，需要转换的qat模型路径，定点模型的输出路径以及芯片架构。如果使用的是XJ3芯片，march选择bernoulli2，J5芯片的march选择bayes。</p>
<p>转换成功后可以在输出路径下看到QAT模型导出的ONNX模型，转换后的ONNX模型，编译之后的bin以及perf文件。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="unet.html" class="btn btn-neutral float-left" title="Unet分割模型训练" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, HAT Developers.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>