

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>hat.models &mdash; HAT 1.5.5 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script >mermaid.initialize({startOnLoad:true});</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="hat.metrics" href="metrics.html" />
    <link rel="prev" title="hat.engine" href="engine.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> HAT
          

          
          </a>

          
            
            
              <div class="version">
                1.5.5
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/introduction.html">简介</a></li>
</ul>
<p class="caption"><span class="caption-text">Framework</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../framework/framework.html">框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="../framework/engine.html">执行引擎</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/registry.html">注册机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/config.html">config 文件介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/opts.html">通过命令行覆盖config参数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/calops.html">计算量工具</a></li>
</ul>
<p class="caption"><span class="caption-text">ModelZoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/model_zoo.html">ModelZoo</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data.html">hat.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="callbacks.html">hat.callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="engine.html">hat.engine</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">hat.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#backbones">backbones</a></li>
<li class="toctree-l3"><a class="reference internal" href="#losses">losses</a></li>
<li class="toctree-l3"><a class="reference internal" href="#necks">necks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#structures">structures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#detectors">detectors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fcos">fcos</a></li>
<li class="toctree-l4"><a class="reference internal" href="#seg">seg</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deeplab">deeplab</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fcn">fcn</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-hat.models.backbones">API Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">hat.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualize.html">hat.visualize</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/scripts.html">执行脚本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/classification.html">VargConvNet分类模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/fcos.html">FCOS检测模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/deeplab.html">Deeplab分割模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/fastscnn.html">Fastscnn分割模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/unet.html">Unet分割模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/community_qat.html">社区QAT</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">HAT</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>hat.models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/api_reference/models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hat-models">
<h1>hat.models<a class="headerlink" href="#hat-models" title="Permalink to this headline">¶</a></h1>
<p>Models widely used in upper module in HAT.</p>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="backbones">
<h3>backbones<a class="headerlink" href="#backbones" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.backbones.EfficientNet" title="hat.models.backbones.EfficientNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EfficientNet</span></code></a></p></td>
<td><p>A module of EfficientNet.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.backbones.efficientnet" title="hat.models.backbones.efficientnet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet</span></code></a></p></td>
<td><p>A module of efficientnet.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.backbones.efficientnet_lite" title="hat.models.backbones.efficientnet_lite"><code class="xref py py-obj docutils literal notranslate"><span class="pre">efficientnet_lite</span></code></a></p></td>
<td><p>A module of efficientnet_lite.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.backbones.MobileNetV1" title="hat.models.backbones.MobileNetV1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MobileNetV1</span></code></a></p></td>
<td><p>A module of mobilenetv1.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.backbones.ResNet18" title="hat.models.backbones.ResNet18"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ResNet18</span></code></a></p></td>
<td><p>A module of resnet18.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.backbones.ResNet50" title="hat.models.backbones.ResNet50"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ResNet50</span></code></a></p></td>
<td><p>A module of resnet50.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.backbones.VarGDarkNet53" title="hat.models.backbones.VarGDarkNet53"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VarGDarkNet53</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.backbones.ResNet50V2" title="hat.models.backbones.ResNet50V2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ResNet50V2</span></code></a></p></td>
<td><p>A module of resnet50V2.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.backbones.VargNetV2" title="hat.models.backbones.VargNetV2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VargNetV2</span></code></a></p></td>
<td><p>A module of vargnetv2.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.backbones.TinyVargNetV2" title="hat.models.backbones.TinyVargNetV2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TinyVargNetV2</span></code></a></p></td>
<td><p>A module of TinyVargNetv2.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.backbones.get_vargnetv2_stride2channels" title="hat.models.backbones.get_vargnetv2_stride2channels"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_vargnetv2_stride2channels</span></code></a></p></td>
<td><p>Get vargnet v2 stride to channel dict with giving channels and strides.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.backbones.VargConvNet" title="hat.models.backbones.VargConvNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VargConvNet</span></code></a></p></td>
<td><p>A module of vargconvnet.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="losses">
<h3>losses<a class="headerlink" href="#losses" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.CEWithLabelSmooth" title="hat.models.losses.CEWithLabelSmooth"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CEWithLabelSmooth</span></code></a></p></td>
<td><p>The losses of cross-entropy with label smooth.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.CrossEntropyLoss" title="hat.models.losses.CrossEntropyLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a></p></td>
<td><p>Calculate cross entropy loss of multi stride output.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.CrossEntropyLossV2" title="hat.models.losses.CrossEntropyLossV2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CrossEntropyLossV2</span></code></a></p></td>
<td><p>Calculate cross entropy loss of multi stride output.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.SoftTargetCrossEntropy" title="hat.models.losses.SoftTargetCrossEntropy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SoftTargetCrossEntropy</span></code></a></p></td>
<td><p>The losses of cross-entropy with soft target.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.CEWithWeightMap" title="hat.models.losses.CEWithWeightMap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CEWithWeightMap</span></code></a></p></td>
<td><p>Crossentropy loss with image-specfic class weighted map within batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.FocalLoss" title="hat.models.losses.FocalLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FocalLoss</span></code></a></p></td>
<td><p>Sigmoid focal loss.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.FocalLossV2" title="hat.models.losses.FocalLossV2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FocalLossV2</span></code></a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1708.02002">Focal Loss</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.SoftmaxFocalLoss" title="hat.models.losses.SoftmaxFocalLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SoftmaxFocalLoss</span></code></a></p></td>
<td><p>Focal Loss.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.GaussianFocalLoss" title="hat.models.losses.GaussianFocalLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GaussianFocalLoss</span></code></a></p></td>
<td><p>Guassian focal loss.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.GIoULoss" title="hat.models.losses.GIoULoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GIoULoss</span></code></a></p></td>
<td><p>Generalized Intersection over Union Loss.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.SegLoss" title="hat.models.losses.SegLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SegLoss</span></code></a></p></td>
<td><p>Segmentation loss wrapper.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.SmoothL1Loss" title="hat.models.losses.SmoothL1Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code></a></p></td>
<td><p>Smooth L1 Loss.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.YOLOV3Loss" title="hat.models.losses.YOLOV3Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">YOLOV3Loss</span></code></a></p></td>
<td><p>The loss module of YOLOv3.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.losses.MixSegLossMultipreds" title="hat.models.losses.MixSegLossMultipreds"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MixSegLossMultipreds</span></code></a></p></td>
<td><p>Calculate multi-losses with multi-preds and correspondence targets.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.losses.MixSegLoss" title="hat.models.losses.MixSegLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MixSegLoss</span></code></a></p></td>
<td><p>Calculate multi-losses with same prediction and target.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="necks">
<h3>necks<a class="headerlink" href="#necks" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.necks.BiFPN" title="hat.models.necks.BiFPN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BiFPN</span></code></a></p></td>
<td><p>Weighted Bi-directional Feature Pyramid Network(BiFPN).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.necks.DwUnet" title="hat.models.necks.DwUnet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DwUnet</span></code></a></p></td>
<td><p>Unet segmentation neck structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.necks.FPN" title="hat.models.necks.FPN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FPN</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.necks.RetinaNetFPN" title="hat.models.necks.RetinaNetFPN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RetinaNetFPN</span></code></a></p></td>
<td><p>FPN for RetinaNet.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.necks.Unet" title="hat.models.necks.Unet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Unet</span></code></a></p></td>
<td><p>Unet neck module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.necks.YOLOV3Neck" title="hat.models.necks.YOLOV3Neck"><code class="xref py py-obj docutils literal notranslate"><span class="pre">YOLOV3Neck</span></code></a></p></td>
<td><p>Necks module of yolov3.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.necks.PAFPN" title="hat.models.necks.PAFPN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PAFPN</span></code></a></p></td>
<td><p>Path Aggregation Network for Instance Segmentation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.necks.FastSCNNNeck" title="hat.models.necks.FastSCNNNeck"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FastSCNNNeck</span></code></a></p></td>
<td><p>Upper neck module for segmentation.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="structures">
<h3>structures<a class="headerlink" href="#structures" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.structures.Classifier" title="hat.models.structures.Classifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Classifier</span></code></a></p></td>
<td><p>The basic structure of classifier.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.structures.Segmentor" title="hat.models.structures.Segmentor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Segmentor</span></code></a></p></td>
<td><p>The basic structure of segmentor.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="detectors">
<h4>detectors<a class="headerlink" href="#detectors" title="Permalink to this headline">¶</a></h4>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.structures.detectors.RetinaNet" title="hat.models.structures.detectors.RetinaNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RetinaNet</span></code></a></p></td>
<td><p>The basic structure of retinanet.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.structures.detectors.YOLOV3" title="hat.models.structures.detectors.YOLOV3"><code class="xref py py-obj docutils literal notranslate"><span class="pre">YOLOV3</span></code></a></p></td>
<td><p>The basic structure of yolov3.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.structures.detectors.FCOS" title="hat.models.structures.detectors.FCOS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCOS</span></code></a></p></td>
<td><p>The basic structure of fcos.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="fcos">
<h4>fcos<a class="headerlink" href="#fcos" title="Permalink to this headline">¶</a></h4>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.FCOSDecoder" title="hat.models.task_modules.fcos.FCOSDecoder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCOSDecoder</span></code></a></p></td>
<td><p><dl class="field-list simple">
<dt class="field-odd">param num_classes</dt>
<dd class="field-odd"><p>Number of categories excluding the background</p>
</dd>
</dl>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.FCOSHead" title="hat.models.task_modules.fcos.FCOSHead"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCOSHead</span></code></a></p></td>
<td><p>Anchor-free head used in <cite>FCOS &lt;https://arxiv.org/abs/1904.01355&gt;</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.FCOSTarget" title="hat.models.task_modules.fcos.FCOSTarget"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCOSTarget</span></code></a></p></td>
<td><p>Generate cls and reg targets for FCOS in training stage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.DynamicFcosTarget" title="hat.models.task_modules.fcos.DynamicFcosTarget"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DynamicFcosTarget</span></code></a></p></td>
<td><p>Generate cls and reg targets for FCOS in training stage base on dynamic losses.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.multiclass_nms" title="hat.models.task_modules.fcos.multiclass_nms"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiclass_nms</span></code></a></p></td>
<td><p>NMS for multi-class bboxes.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.get_points" title="hat.models.task_modules.fcos.get_points"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_points</span></code></a></p></td>
<td><p>Generate points according to feat_sizes.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.distance2bbox" title="hat.models.task_modules.fcos.distance2bbox"><code class="xref py py-obj docutils literal notranslate"><span class="pre">distance2bbox</span></code></a></p></td>
<td><p>Decode distance prediction to bounding box.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.fcos.BBoxUpscaler" title="hat.models.task_modules.fcos.BBoxUpscaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BBoxUpscaler</span></code></a></p></td>
<td><p><dl class="field-list simple">
<dt class="field-odd">param strides</dt>
<dd class="field-odd"><p>A list contains the strides of fcos_head</p>
</dd>
</dl>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="seg">
<h4>seg<a class="headerlink" href="#seg" title="Permalink to this headline">¶</a></h4>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.seg.SegDecoder" title="hat.models.task_modules.seg.SegDecoder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SegDecoder</span></code></a></p></td>
<td><p>Semantic Segmentation Decoder.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.seg.SegHead" title="hat.models.task_modules.seg.SegHead"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SegHead</span></code></a></p></td>
<td><p>Head Module for segmentation task.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.seg.SegTarget" title="hat.models.task_modules.seg.SegTarget"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SegTarget</span></code></a></p></td>
<td><p>Generate training targets for Seg task.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.seg.FRCNNSegHead" title="hat.models.task_modules.seg.FRCNNSegHead"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FRCNNSegHead</span></code></a></p></td>
<td><p>FRCNNSegHead module for segmentation task.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="deeplab">
<h4>deeplab<a class="headerlink" href="#deeplab" title="Permalink to this headline">¶</a></h4>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.deeplab.Deeplabv3plusHead" title="hat.models.task_modules.deeplab.Deeplabv3plusHead"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Deeplabv3plusHead</span></code></a></p></td>
<td><p>Head Module for FCN.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="fcn">
<h4>fcn<a class="headerlink" href="#fcn" title="Permalink to this headline">¶</a></h4>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.fcn.FCNHead" title="hat.models.task_modules.fcn.FCNHead"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCNHead</span></code></a></p></td>
<td><p>Head Module for FCN.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.fcn.DepthwiseSeparableFCNHead" title="hat.models.task_modules.fcn.DepthwiseSeparableFCNHead"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DepthwiseSeparableFCNHead</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#hat.models.task_modules.fcn.FCNTarget" title="hat.models.task_modules.fcn.FCNTarget"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCNTarget</span></code></a></p></td>
<td><p>Generate Target for FCN.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#hat.models.task_modules.fcn.FCNDecoder" title="hat.models.task_modules.fcn.FCNDecoder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FCNDecoder</span></code></a></p></td>
<td><p>FCN Decoder.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="module-hat.models.backbones">
<span id="api-reference"></span><h2>API Reference<a class="headerlink" href="#module-hat.models.backbones" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="hat.models.backbones.EfficientNet">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">EfficientNet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model_type</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">coefficient_params</span><span class="p">:</span> <span class="n">tuple</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">drop_connect_rate</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.2</span></em>, <em class="sig-param"><span class="n">depth_division</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">8</span></em>, <em class="sig-param"><span class="n">activation</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'relu'</span></em>, <em class="sig-param"><span class="n">use_se_block</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">blocks_args</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(BlockArgs(kernel_size=3, num_repeat=1, in_filters=32, out_filters=16, expand_ratio=1, id_skip=True, strides=1, se_ratio=0.25), BlockArgs(kernel_size=3, num_repeat=2, in_filters=16, out_filters=24, expand_ratio=6, id_skip=True, strides=2, se_ratio=0.25), BlockArgs(kernel_size=5, num_repeat=2, in_filters=24, out_filters=40, expand_ratio=6, id_skip=True, strides=2, se_ratio=0.25), BlockArgs(kernel_size=3, num_repeat=3, in_filters=40, out_filters=80, expand_ratio=6, id_skip=True, strides=2, se_ratio=0.25), BlockArgs(kernel_size=5, num_repeat=3, in_filters=80, out_filters=112, expand_ratio=6, id_skip=True, strides=1, se_ratio=0.25), BlockArgs(kernel_size=5, num_repeat=4, in_filters=112, out_filters=192, expand_ratio=6, id_skip=True, strides=2, se_ratio=0.25), BlockArgs(kernel_size=3, num_repeat=1, in_filters=192, out_filters=320, expand_ratio=6, id_skip=True, strides=1, se_ratio=0.25))</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">resolution</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">use_drop_connect</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.EfficientNet" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of EfficientNet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_type</strong> (<em>str</em>) – Select to use which EfficientNet(B0-B7 or lite0-4),             for EfficientNet model, model_type must be one of:               [‘b0’, ‘b1’, ‘b2’, ‘b3’, ‘b4’, ‘b5’, ‘b6’, ‘b7’],             for EfficientNet-lite model, model_type must be one of:               [‘lite0’, ‘lite1’, ‘lite2’, ‘lite3’, ‘lite4’].</p></li>
<li><p><strong>coefficient_params</strong> (<em>tuple</em>) – Parameter coefficients of EfficientNet,             include:               width_coefficient(float): scaling coefficient for net width.               depth_coefficient(float): scaling coefficient for net depth.               default_resolution(int): default input image size.               dropout_rate(float): dropout rate for final classifier layer.         num_classes (int): Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for Bn layer.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>drop_connect_rate</strong> (<em>float</em>) – Dropout rate at skip connections.</p></li>
<li><p><strong>depth_division</strong> (<em>int</em>) – Depth division, Defaults to 8.</p></li>
<li><p><strong>activation</strong> (<em>str</em>) – Activation layer, defaults to ‘relu’.</p></li>
<li><p><strong>use_se_block</strong> (<em>bool</em>) – Whether to use SEBlock in module.</p></li>
<li><p><strong>blocks_args</strong> (<em>list</em>) – A list of BlockArgs to MBConvBlock modules.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
<li><p><strong>use_drop_connect</strong> (<em>bool</em>) – Whether to use drop connect.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.backbones.EfficientNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.EfficientNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.MobileNetV1">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">MobileNetV1</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">dw_with_relu</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.MobileNetV1" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of mobilenetv1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for BN layer.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – Alpha for mobilenetv1.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>dw_with_relu</strong> (<em>bool</em>) – Whether to use relu in dw conv.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.backbones.MobileNetV1.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.MobileNetV1.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.ResNet18">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">ResNet18</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.ResNet18" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of resnet18.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for BN layer.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.ResNet50">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">ResNet50</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.ResNet50" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of resnet50.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for BN layer.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.ResNet50V2">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">ResNet50V2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">group_base</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">extend_features</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.ResNet50V2" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of resnet50V2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> – Num classes of output layer.</p></li>
<li><p><strong>group_base</strong> – Group base for ExtendVarGNetFeatures.</p></li>
<li><p><strong>bn_kwargs</strong> – Dict for BN layer.</p></li>
<li><p><strong>bias</strong> – Whether to use bias in module.</p></li>
<li><p><strong>extend_features</strong> – Whether to extend features.</p></li>
<li><p><strong>include_top</strong> – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> – Whether to view the output tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.TinyVargNetV2">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">TinyVargNetV2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">group_base</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">8</span></em>, <em class="sig-param"><span class="n">factor</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">extend_features</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">disable_quanti_input</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">input_channels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">3</span></em>, <em class="sig-param"><span class="n">input_sequence_length</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">head_factor</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">input_resize_scale</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.TinyVargNetV2" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of TinyVargNetv2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for BN layer.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – Alpha for tinyvargnetv2.</p></li>
<li><p><strong>group_base</strong> (<em>int</em>) – Group base for tinyvargnetv2.</p></li>
<li><p><strong>factor</strong> (<em>int</em>) – Factor for channel expansion in basic block.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>extend_features</strong> (<em>bool</em>) – Whether to extend features.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
<li><p><strong>input_channels</strong> (<em>int</em>) – Input channels of first conv.</p></li>
<li><p><strong>input_sequence_length</strong> (<em>int</em>) – Length of input sequence.</p></li>
<li><p><strong>head_factor</strong> (<em>int</em>) – Factor for channels expansion of stage1(mod2).</p></li>
<li><p><strong>input_resize_scale</strong> (<em>int</em>) – Narrow_model need resize input 0.65 scale,
While int_infer or visualize or eval</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.VarGDarkNet53">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">VarGDarkNet53</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">max_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.VarGDarkNet53" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="hat.models.backbones.VarGDarkNet53.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.VarGDarkNet53.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.VargConvNet">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">VargConvNet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">channels_list</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">repeats</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">group_list</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">factor_list</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1024</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">input_channels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">3</span></em>, <em class="sig-param"><span class="n">deep_stem</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.VargConvNet" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of vargconvnet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for BN layer.</p></li>
<li><p><strong>channels_list</strong> (<em>list</em>) – List for output channels</p></li>
<li><p><strong>repeats</strong> (<em>list</em>) – Depth of each stage.</p></li>
<li><p><strong>group_list</strong> (<em>list</em>) – Group of each stage.</p></li>
<li><p><strong>factor_list</strong> (<em>list</em>) – Factor for each stage.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Output channels.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
<li><p><strong>input_channels</strong> (<em>int</em>) – Input channels of first conv.</p></li>
<li><p><strong>deep_stem</strong> (<em>bool</em>) – Whether use deep stem.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.backbones.VargConvNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.VargConvNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.backbones.VargNetV2">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">VargNetV2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">model_type</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'VargNetV2'</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">group_base</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">8</span></em>, <em class="sig-param"><span class="n">factor</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">extend_features</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">disable_quanti_input</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">flat_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">input_channels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">3</span></em>, <em class="sig-param"><span class="n">input_sequence_length</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">head_factor</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">input_resize_scale</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.VargNetV2" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of vargnetv2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of output layer.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Dict for BN layer.</p></li>
<li><p><strong>model_type</strong> (<em>str</em>) – Choose to use <cite>VargNetV2</cite> or <cite>TinyVargNetV2</cite>.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – Alpha for vargnetv2.</p></li>
<li><p><strong>group_base</strong> (<em>int</em>) – Group base for vargnetv2.</p></li>
<li><p><strong>factor</strong> (<em>int</em>) – Factor for channel expansion in basic block.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
<li><p><strong>extend_features</strong> (<em>bool</em>) – Whether to extend features.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – Whether to include output layer.</p></li>
<li><p><strong>flat_output</strong> (<em>bool</em>) – Whether to view the output tensor.</p></li>
<li><p><strong>input_channels</strong> (<em>int</em>) – Input channels of first conv.</p></li>
<li><p><strong>input_sequence_length</strong> (<em>int</em>) – Length of input sequence.</p></li>
<li><p><strong>head_factor</strong> (<em>int</em>) – Factor for channels expansion of stage1(mod2).</p></li>
<li><p><strong>input_resize_scale</strong> (<em>int</em>) – Narrow_model need resize input 0.65 scale,
While int_infer or visualize or eval</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.backbones.VargNetV2.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.VargNetV2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hat.models.backbones.VargNetV2.process_sequence_input">
<code class="sig-name descname">process_sequence_input</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n">List</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#hat.models.backbones.VargNetV2.process_sequence_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Process sequence input with cat.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="hat.models.backbones.efficientnet">
<code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">efficientnet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model_type</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.efficientnet" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of efficientnet.</p>
</dd></dl>

<dl class="py function">
<dt id="hat.models.backbones.efficientnet_lite">
<code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">efficientnet_lite</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model_type</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.backbones.efficientnet_lite" title="Permalink to this definition">¶</a></dt>
<dd><p>A module of efficientnet_lite.</p>
</dd></dl>

<dl class="py function">
<dt id="hat.models.backbones.get_vargnetv2_stride2channels">
<code class="sig-prename descclassname">hat.models.backbones.</code><code class="sig-name descname">get_vargnetv2_stride2channels</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">channels</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">strides</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; Dict<a class="headerlink" href="#hat.models.backbones.get_vargnetv2_stride2channels" title="Permalink to this definition">¶</a></dt>
<dd><p>Get vargnet v2 stride to channel dict with giving channels and strides.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – channel multipler.</p></li>
<li><p><strong>channels</strong> – base channel of each stride.</p></li>
<li><p><strong>strides</strong> – stride list corresponding to channels.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>strides2channels: a stride to channel dict.</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-hat.models.losses"></span><dl class="py class">
<dt id="hat.models.losses.CEWithLabelSmooth">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">CEWithLabelSmooth</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">smooth_alpha</span><span class="o">=</span><span class="default_value">0.1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CEWithLabelSmooth" title="Permalink to this definition">¶</a></dt>
<dd><p>The losses of cross-entropy with label smooth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>smooth_alpha</strong> (<em>float</em>) – Alpha of label smooth.</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.CEWithLabelSmooth.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">target</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CEWithLabelSmooth.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.CEWithWeightMap">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">CEWithWeightMap</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weight_min</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CEWithWeightMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Crossentropy loss with image-specfic class weighted map within batch.</p>
<dl class="py method">
<dt id="hat.models.losses.CEWithWeightMap.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">target</span></em>, <em class="sig-param"><span class="n">weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">avg_factor</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CEWithWeightMap.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.CrossEntropyLoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate cross entropy loss of multi stride output.</p>
<p>This class will depracated in future, use CrossEntropyLossV2 instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_name</strong> (<em>str</em>) – The key of loss in return dict.</p></li>
<li><p><strong>preds_name</strong> (<em>str</em>) – The key of pred in pred dict.</p></li>
<li><p><strong>label_name</strong> (<em>str</em>) – The key of label in target dict.</p></li>
<li><p><strong>weight_name</strong> (<em>str</em>) – The key of weight in target dict.</p></li>
<li><p><strong>avg_factor_name</strong> (<em>str</em>) – The key of avg_factor in target dict.</p></li>
<li><p><strong>use_sigmoid</strong> (<em>bool</em>) – Whether logits tensor is converted to probability
through sigmoid, Defaults to False.
If <cite>True</cite>, use <cite>F.binary_cross_entropy_with_logits</cite>.
If <cite>False</cite>, use <cite>F.cross_entropy</cite>.</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – The method used to reduce the loss. Options are
[<cite>none</cite>, <cite>mean</cite>, <cite>sum</cite>].</p></li>
<li><p><strong>class_weight</strong> (<em>list</em><em>[</em><em>float</em><em>]</em>) – Weight of each class. Defaults is None.</p></li>
<li><p><strong>loss_weight</strong> (<em>float</em>) – Global weight of loss. Defaults is 1.</p></li>
<li><p><strong>ignore_index</strong> (<em>int</em>) – Only works when using cross_entropy.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict containing the calculated loss, the key of loss is
loss_name.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.CrossEntropyLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred_dict</span><span class="p">:</span> <span class="n">Mapping</span></em>, <em class="sig-param"><span class="n">target_dict</span><span class="p">:</span> <span class="n">Mapping</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CrossEntropyLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.CrossEntropyLossV2">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">CrossEntropyLossV2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">use_sigmoid</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em>, <em class="sig-param"><span class="n">class_weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>float<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_weight</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">ignore_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">loss_name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">auto_class_weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">weight_min</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_noobj</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_class</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CrossEntropyLossV2" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate cross entropy loss of multi stride output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_sigmoid</strong> (<em>bool</em>) – Whether logits tensor is converted to probability
through sigmoid, Defaults to False.
If <cite>True</cite>, use <cite>F.binary_cross_entropy_with_logits</cite>.
If <cite>False</cite>, use <cite>F.cross_entropy</cite>.</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – The method used to reduce the loss. Options are
[<cite>none</cite>, <cite>mean</cite>, <cite>sum</cite>].</p></li>
<li><p><strong>class_weight</strong> (<em>list</em><em>[</em><em>float</em><em>]</em>) – Weight of each class. Defaults is None.</p></li>
<li><p><strong>loss_weight</strong> (<em>float</em>) – Global weight of loss. Defaults is 1.</p></li>
<li><p><strong>ignore_index</strong> (<em>int</em>) – Only works when using cross_entropy.</p></li>
<li><p><strong>loss_name</strong> (<em>str</em>) – The key of loss in return dict. If None, return loss
directly.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>cross entropy loss</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.CrossEntropyLossV2.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">target</span></em>, <em class="sig-param"><span class="n">weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">avg_factor</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.CrossEntropyLossV2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.FocalLoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">FocalLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss_name</span></em>, <em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.25</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="o">=</span><span class="default_value">2.0</span></em>, <em class="sig-param"><span class="n">loss_weight</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-12</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.FocalLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Sigmoid focal loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_name</strong> (<em>str</em>) – The key of loss in return dict.</p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – Num_classes including background, C+1, C is number
of foreground categories.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – A weighting factor for pos-sample, (1-alpha) is for
neg-sample.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Gamma used in focal loss to compress the contribution
of easy examples.</p></li>
<li><p><strong>loss_weight</strong> (<em>float</em>) – Global weight of loss. Defaults is 1.0.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – A small value to avoid zero denominator.</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – The method used to reduce the loss. Options are
[<cite>none</cite>, <cite>mean</cite>, <cite>sum</cite>].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict containing the calculated loss, the key of loss is
loss_name.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.FocalLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">target</span></em>, <em class="sig-param"><span class="n">weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">avg_factor</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">points_per_strides</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">valid_classes_list</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.FocalLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> (<em>Tensor</em>) – Cls pred, with shape(N, C), C is num_classes of
foreground.</p></li>
<li><p><strong>target</strong> (<em>Tensor</em>) – Cls target, with shape(N,), values in [0, C-1]
represent the foreground, C or negative value represent the
background.</p></li>
<li><p><strong>weight</strong> (<em>Tensor</em>) – The weight of loss for each prediction. Default
is None.</p></li>
<li><p><strong>avg_factor</strong> (<em>float</em>) – Normalized factor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.FocalLossV2">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">FocalLossV2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.25</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">2.0</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-12</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.FocalLossV2" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://arxiv.org/abs/1708.02002">Focal Loss</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em>) – A weighting factor for pos-sample, (1-alpha) is for
neg-sample.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Gamma used in focal loss to compress the contribution
of easy examples.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – A small value to avoid zero denominator.</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – The method to reduce the loss.
Options are “none”, “mean” and “sum”. Defaults to “mean”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.FocalLossV2.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">target</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">avg_factor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>float<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.FocalLossV2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> (<em>Tensor</em>) – cls pred, with shape (B, N, C), C is num_classes of
foreground.</p></li>
<li><p><strong>target</strong> (<em>Tensor</em>) – cls target, with shape (B, N, C), C is num_classes
of foreground.</p></li>
<li><p><strong>weight</strong> (<em>Tensor</em>) – The weight of loss for each prediction. It is
mainly used to filter the ignored box. Default is None.</p></li>
<li><p><strong>avg_factor</strong> (<em>float</em>) – Normalized factor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.GIoULoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">GIoULoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss_name</span></em>, <em class="sig-param"><span class="n">loss_weight</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-06</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.GIoULoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Generalized Intersection over Union Loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_name</strong> (<em>str</em>) – The key of loss in return dict.</p></li>
<li><p><strong>loss_weight</strong> (<em>float</em>) – Global weight of loss. Defaults is 1.0.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – A small value to avoid zero denominator.</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – The method used to reduce the loss. Options are
[<cite>none</cite>, <cite>mean</cite>, <cite>sum</cite>].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict containing the calculated loss, the key of loss is
loss_name.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.GIoULoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">target</span></em>, <em class="sig-param"><span class="n">weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">avg_factor</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.GIoULoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> (<em>torch.Tensor</em>) – Predicted bboxes of format (x1, y1, x2, y2),
represent upper-left and lower-right point, with shape(N, 4).</p></li>
<li><p><strong>target</strong> (<em>torch.Tensor</em>) – Corresponding gt_boxes, the same shape as
pred.</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – Element-wise weight loss weight, with
shape(N,).</p></li>
<li><p><strong>avg_factor</strong> (<em>float</em>) – Average factor that is used to average the
loss.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.GaussianFocalLoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">GaussianFocalLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">2.0</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">4.0</span></em>, <em class="sig-param"><span class="n">loss_weight</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.GaussianFocalLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Guassian focal loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – A weighting factor for positive sample.</p></li>
<li><p><strong>gamma</strong> – Used in focal loss to balance contribution
of easy examples and hard examples.</p></li>
<li><p><strong>loss_weight</strong> – Weight factor for guassian focal loss.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.GaussianFocalLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="n">labels</span></em>, <em class="sig-param"><span class="n">grad_tensor</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.GaussianFocalLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> (<em>torch.Tensor</em>) – The prediction.</p></li>
<li><p><strong>target</strong> (<em>torch.Tensor</em>) – The learning target of the prediction
in gaussian distribution.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.MixSegLoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">MixSegLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">losses_weight</span><span class="p">:</span> <span class="n">List<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.MixSegLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate multi-losses with same prediction and target.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>losses</strong> – List of losses with the same input pred and target.</p></li>
<li><p><strong>losses_weight</strong> – List of weights used for loss calculation.
Default: None</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.MixSegLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">target</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.MixSegLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.MixSegLossMultipreds">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">MixSegLossMultipreds</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">losses_weight</span><span class="p">:</span> <span class="n">List<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.MixSegLossMultipreds" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate multi-losses with multi-preds and correspondence targets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>losses</strong> – List of losses with different prediction and target.</p></li>
<li><p><strong>losses_weight</strong> – List of weights used for loss calculation.
Default: None</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.MixSegLossMultipreds.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">target</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.MixSegLossMultipreds.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.SegLoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">SegLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SegLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Segmentation loss wrapper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<em>dict</em>) – loss config.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class is not universe. Make sure you know this class limit before
using it.</p>
</div>
<dl class="py method">
<dt id="hat.models.losses.SegLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">target</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Dict<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Dict<a class="headerlink" href="#hat.models.losses.SegLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.SmoothL1Loss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">SmoothL1Loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">beta</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Smooth L1 Loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>optional</em>) – The threshold in the piecewise function.
Defaults to 1.0.</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – The method to reduce the loss.
Options are “none”, “mean” and “sum”. Defaults to “mean”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.SmoothL1Loss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">target</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">weight</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">avg_factor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>float<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SmoothL1Loss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> (<em>torch.Tensor</em>) – The prediction.</p></li>
<li><p><strong>target</strong> (<em>torch.Tensor</em>) – The learning target of the prediction.</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – The weight of loss for each
prediction. Defaults to None.</p></li>
<li><p><strong>avg_factor</strong> (<em>float</em>) – Normalized factor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.SoftTargetCrossEntropy">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">SoftTargetCrossEntropy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SoftTargetCrossEntropy" title="Permalink to this definition">¶</a></dt>
<dd><p>The losses of cross-entropy with soft target.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss_name</strong> (<em>str</em>) – The name of returned losses.</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.SoftTargetCrossEntropy.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">target</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SoftTargetCrossEntropy.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.SoftmaxFocalLoss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">SoftmaxFocalLoss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss_name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.25</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">2.0</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em>, <em class="sig-param"><span class="n">weight</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>Sequence<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SoftmaxFocalLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Focal Loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_name</strong> (<em>str</em>) – The key of loss in return dict.</p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – Class number.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – Alpha. Defaults to 0.25.</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em>) – Gamma. Defaults to 2.0.</p></li>
<li><p><strong>reduction</strong> (<em>str</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>.</p></li>
<li><p><strong>weight</strong> (<em>Union</em><em>[</em><em>float</em><em>, </em><em>Sequence</em><em>]</em><em>, </em><em>optional</em>) – Weight to be applied to
the loss of each input. Defaults to 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.SoftmaxFocalLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="n">labels</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.SoftmaxFocalLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.losses.YOLOV3Loss">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.losses.</code><code class="sig-name descname">YOLOV3Loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">anchors</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">strides</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">ignore_thresh</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">loss_xy</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">loss_wh</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">loss_conf</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">loss_cls</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">lambda_loss</span><span class="p">:</span> <span class="n">list</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.YOLOV3Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The loss module of YOLOv3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Num classes of class branch.</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – The anchors of YOLOv3.</p></li>
<li><p><strong>strides</strong> (<em>list</em>) – The strides of feature maps.</p></li>
<li><p><strong>ignore_thresh</strong> (<em>float</em>) – Ignore thresh of target.</p></li>
<li><p><strong>loss_xy</strong> (<em>dict</em>) – Losses of xy.</p></li>
<li><p><strong>loss_wh</strong> (<em>dict</em>) – Losses of wh.</p></li>
<li><p><strong>loss_conf</strong> (<em>dict</em>) – Losses of conf.</p></li>
<li><p><strong>loss_cls</strong> (<em>dict</em>) – Losses of cls.</p></li>
<li><p><strong>lambda_loss</strong> (<em>list</em>) – The list of weighted losses.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.losses.YOLOV3Loss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">target</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.losses.YOLOV3Loss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<span class="target" id="module-hat.models.necks"></span><dl class="py class">
<dt id="hat.models.necks.BiFPN">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">BiFPN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_strides</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">stride2channels</span></em>, <em class="sig-param"><span class="n">out_channels</span></em>, <em class="sig-param"><span class="n">num_outs</span></em>, <em class="sig-param"><span class="n">stack</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">start_level</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">end_level</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">fpn_name</span><span class="o">=</span><span class="default_value">'bifpn_sum'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.BiFPN" title="Permalink to this definition">¶</a></dt>
<dd><p>Weighted Bi-directional Feature Pyramid Network(BiFPN).</p>
<p>This is an implementation of - EfficientDet: Scalable and Efficient Object
Detection (<a class="reference external" href="https://arxiv.org/abs/1911.09070">https://arxiv.org/abs/1911.09070</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_strides</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – Stride of input feature map</p></li>
<li><p><strong>out_strides</strong> (<em>int</em>) – Stride of output feature map</p></li>
<li><p><strong>stride2channels</strong> (<em>dict</em>) – The key:value is stride:channel ,
the channles have been multipified by alpha</p></li>
<li><p><strong>out_channels</strong> (<em>int|dict</em>) – Channel number of output layer, the key:value
is stride:channel.</p></li>
<li><p><strong>num_outs</strong> (<em>int</em>) – Number of BifpnLayer’s input, the value is must 5,
because the bifpn layer is fixed</p></li>
<li><p><strong>stack</strong> (<em>int</em>) – Number of BifpnLayer</p></li>
<li><p><strong>start_level</strong> (<em>int</em>) – Index of the start input backbone level
used to build the feature pyramid. Default: 0.</p></li>
<li><p><strong>end_level</strong> (<em>int</em>) – Index of the end input backbone level (exclusive)
to build the feature pyramid. Default: -1, means the last level.</p></li>
<li><p><strong>fpn_name</strong> (<em>str</em>) – the value is mutst between with ‘bifpn_sum’, ‘bifpn_fa’</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.BiFPN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.BiFPN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>list</em><em>[</em><em>tensor</em><em>]</em>) – Input tensors</p>
</dd>
</dl>
<p>Returns (list[tensor]): Output tensors</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.DwUnet">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">DwUnet</code><span class="sig-paren">(</span><em class="sig-param">base_channels: int</em>, <em class="sig-param">bn_kwargs: Dict = None</em>, <em class="sig-param">act_type: torch.nn.modules.module.Module = &lt;class 'torch.nn.modules.activation.ReLU'&gt;</em>, <em class="sig-param">use_deconv: bool = False</em>, <em class="sig-param">dw_with_act: bool = False</em>, <em class="sig-param">output_scales: Sequence = (4</em>, <em class="sig-param">8</em>, <em class="sig-param">16</em>, <em class="sig-param">32</em>, <em class="sig-param">64)</em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.DwUnet" title="Permalink to this definition">¶</a></dt>
<dd><p>Unet segmentation neck structure.</p>
<p>Built with separable convolution layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_channels</strong> (<em>int</em>) – Output channel number of the output layer of scale 1.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>Dict</em><em>, </em><em>optional</em>) – Keyword arguments for BN layer.
Defaults to {}.</p></li>
<li><p><strong>use_deconv</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether user deconv for upsampling layer.
Defaults to False.</p></li>
<li><p><strong>dw_with_act</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether user relu after the depthwise conv in SeparableConv.
Defaults to False.</p></li>
<li><p><strong>output_scales</strong> (<em>Sequence</em><em>, </em><em>optional</em>) – The scale of each output layer.
Defaults to (4, 8, 16, 32, 64).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.DwUnet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.DwUnet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.FPN">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">FPN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">out_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">fix_out_channel</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.FPN" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="hat.models.necks.FPN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="headerlink" href="#hat.models.necks.FPN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.FastSCNNNeck">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">FastSCNNNeck</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">feat_channels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">indexes</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.FastSCNNNeck" title="Permalink to this definition">¶</a></dt>
<dd><p>Upper neck module for segmentation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – channels of each input feature map</p></li>
<li><p><strong>feat_channels</strong> – channels for featture maps.</p></li>
<li><p><strong>indexes</strong> – indexes of inputs.</p></li>
<li><p><strong>bn_kwargs</strong> – Dict for Bn layer.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.FastSCNNNeck.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.FastSCNNNeck.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.PAFPN">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">PAFPN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span></em>, <em class="sig-param"><span class="n">out_channels</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">num_outs</span></em>, <em class="sig-param"><span class="n">start_level</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">end_level</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">add_extra_convs</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">relu_before_extra_convs</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.PAFPN" title="Permalink to this definition">¶</a></dt>
<dd><p>Path Aggregation Network for Instance Segmentation.</p>
<p>This is an implementation of the <cite>PAFPN in Path Aggregation Network
&lt;https://arxiv.org/abs/1803.01534&gt;</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – Number of input channels per scale.</p></li>
<li><p><strong>out_channels</strong> (<em>int | Dict</em>) – Output channels of each scale</p></li>
<li><p><strong>out_strides</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – Stride of output feature map</p></li>
<li><p><strong>num_outs</strong> (<em>int</em>) – Number of output scales.</p></li>
<li><p><strong>start_level</strong> (<em>int</em>) – Index of the start input backbone level used to            build the feature pyramid. Default: 0.</p></li>
<li><p><strong>end_level</strong> (<em>int</em>) – Index of the end input backbone level (exclusive) to            build the feature pyramid. Default: -1, which means the last level.</p></li>
<li><p><strong>add_extra_convs</strong> (<em>bool | str</em>) – <p>If bool, it decides whether to add conv            layers on top of the original feature maps. Default to False.
If True, it is equivalent to <cite>add_extra_convs=’on_input’</cite>.
If str, it specifies the source feature map of the extra convs.
Only the following options are allowed:</p>
<ul>
<li><p>’on_input’: Last feat map of neck inputs (i.e. backbone feature).</p></li>
<li><p>’on_lateral’:  Last feature map after lateral convs.</p></li>
<li><p>’on_output’: The last output feature map after fpn convs.</p></li>
</ul>
</p></li>
<li><p><strong>relu_before_extra_convs</strong> (<em>bool</em>) – Whether to apply relu before the extra            conv. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.PAFPN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.PAFPN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward function.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.RetinaNetFPN">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">RetinaNetFPN</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">out_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">fix_out_channel</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.RetinaNetFPN" title="Permalink to this definition">¶</a></dt>
<dd><p>FPN for RetinaNet.</p>
<p>The difference with FPN is that RetinaNetFPN
has two extra convs correspond to stride 64 and stride 128 except
the lateral convs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_strides</strong> (<em>list</em>) – strides of each input feature map</p></li>
<li><p><strong>in_channels</strong> (<em>list</em>) – channels of each input feature map,
the length of in_channels should be equal to in_strides</p></li>
<li><p><strong>out_strides</strong> (<em>list</em>) – strides of each output feature map,
should be a subset of in_strides, and continuous (any
subsequence of 2, 4, 8, 16, 32, 64 …). The largest
stride in in_strides and out_strides should be equal</p></li>
<li><p><strong>out_channels</strong> (<em>list</em>) – channels of each output feature maps
the length of out_channels should be equal to out_strides</p></li>
<li><p><strong>fix_out_channel</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional) – if set, there will be
a 1x1 conv following each output feature map so that each
final output has fix_out_channel channels</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.RetinaNetFPN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="headerlink" href="#hat.models.necks.RetinaNetFPN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hat.models.necks.RetinaNetFPN.init_weights">
<code class="sig-name descname">init_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.RetinaNetFPN.init_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the weights of FPN module.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.Unet">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">Unet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">out_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">stride2channels</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>int<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">factor</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">2</span></em>, <em class="sig-param"><span class="n">use_bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">group_base</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">8</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.Unet" title="Permalink to this definition">¶</a></dt>
<dd><p>Unet neck module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_strides</strong> – contains the strides of feature maps from backbone.</p></li>
<li><p><strong>out_strides</strong> – contains the strides of feature maps the neck output.</p></li>
<li><p><strong>stride2channels</strong> – stride to channel dict</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.Unet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.Unet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.necks.YOLOV3Neck">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.necks.</code><code class="sig-name descname">YOLOV3Neck</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backbone_idx</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">in_channels_list</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">out_channels_list</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.YOLOV3Neck" title="Permalink to this definition">¶</a></dt>
<dd><p>Necks module of yolov3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone_idx</strong> (<em>list</em>) – Index of backbone output for necks.</p></li>
<li><p><strong>in_channels_list</strong> (<em>list</em>) – List of input channels.</p></li>
<li><p><strong>out_channels_list</strong> (<em>list</em>) – List of output channels.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Config dict for BN layer.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – Whether to use bias in module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.necks.YOLOV3Neck.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.necks.YOLOV3Neck.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<span class="target" id="module-hat.models.structures"></span><dl class="py class">
<dt id="hat.models.structures.Classifier">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.structures.</code><code class="sig-name descname">Classifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backbone</span></em>, <em class="sig-param"><span class="n">losses</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.Classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>The basic structure of classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone</strong> (<em>torch.nn.Module</em>) – Backbone module.</p></li>
<li><p><strong>losses</strong> (<em>torch.nn.Module</em>) – Losses module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.structures.Classifier.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.Classifier.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.structures.Segmentor">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.structures.</code><code class="sig-name descname">Segmentor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backbone</span></em>, <em class="sig-param"><span class="n">neck</span></em>, <em class="sig-param"><span class="n">head</span></em>, <em class="sig-param"><span class="n">losses</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.Segmentor" title="Permalink to this definition">¶</a></dt>
<dd><p>The basic structure of segmentor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone</strong> (<em>torch.nn.Module</em>) – Backbone module.</p></li>
<li><p><strong>neck</strong> (<em>torch.nn.Module</em>) – Neck module.</p></li>
<li><p><strong>head</strong> (<em>torch.nn.Module</em>) – Head module.</p></li>
<li><p><strong>losses</strong> (<em>torch.nn.Module</em>) – Losses module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.structures.Segmentor.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">dict</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.Segmentor.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<span class="target" id="module-hat.models.structures.detectors"></span><dl class="py class">
<dt id="hat.models.structures.detectors.FCOS">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.structures.detectors.</code><code class="sig-name descname">FCOS</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backbone</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">neck</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">targets</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">post_process</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_cls</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_reg</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_centerness</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.FCOS" title="Permalink to this definition">¶</a></dt>
<dd><p>The basic structure of fcos.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone</strong> – Backbone module.</p></li>
<li><p><strong>neck</strong> – Neck module.</p></li>
<li><p><strong>head</strong> – Head module.</p></li>
<li><p><strong>targets</strong> – Target module.</p></li>
<li><p><strong>loss_cls</strong> – Classification loss module.</p></li>
<li><p><strong>loss_reg</strong> – Regiression loss module.</p></li>
<li><p><strong>loss_centerness</strong> – Centerness loss module</p></li>
<li><p><strong>postprocess</strong> – Postprocess module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.structures.detectors.FCOS.extract_feat">
<code class="sig-name descname">extract_feat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">img</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.FCOS.extract_feat" title="Permalink to this definition">¶</a></dt>
<dd><p>Directly extract features from the backbone + neck.</p>
</dd></dl>

<dl class="py method">
<dt id="hat.models.structures.detectors.FCOS.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Dict</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.FCOS.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.structures.detectors.RetinaNet">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.structures.detectors.</code><code class="sig-name descname">RetinaNet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backbone</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">neck</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">filter_module</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">anchors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">targets</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">post_process</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_cls</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss_reg</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.nn.modules.module.Module<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.RetinaNet" title="Permalink to this definition">¶</a></dt>
<dd><p>The basic structure of retinanet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone</strong> – backbone module or dict for building backbone module.</p></li>
<li><p><strong>neck</strong> – neck module or dict for building neck module.</p></li>
<li><p><strong>head</strong> – head module or dict for building head module.</p></li>
<li><p><strong>anchors</strong> – anchors module or dict for building anchors module.</p></li>
<li><p><strong>targets</strong> – targets module or dict for building target module.</p></li>
<li><p><strong>post_process</strong> – post_process module or dict for building
post_process module.</p></li>
<li><p><strong>loss_cls</strong> – loss_cls module or dict for building loss_cls module.</p></li>
<li><p><strong>loss_reg</strong> – loss_reg module or dict for building loss_reg module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.structures.detectors.RetinaNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Dict</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.RetinaNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.structures.detectors.YOLOV3">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.structures.detectors.</code><code class="sig-name descname">YOLOV3</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backbone</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">neck</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">filter_module</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">anchor_generator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">target_generator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">postprocess</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.YOLOV3" title="Permalink to this definition">¶</a></dt>
<dd><p>The basic structure of yolov3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone</strong> (<em>torch.nn.Module</em>) – Backbone module.</p></li>
<li><p><strong>neck</strong> (<em>torch.nn.Module</em>) – Neck module.</p></li>
<li><p><strong>head</strong> (<em>torch.nn.Module</em>) – Head module.</p></li>
<li><p><strong>anchor_generator</strong> (<em>torch.nn.Module</em>) – Anchor generator module.</p></li>
<li><p><strong>target_generator</strong> (<em>torch.nn.Module</em>) – Target generator module.</p></li>
<li><p><strong>loss</strong> (<em>torch.nn.Module</em>) – Loss module.</p></li>
<li><p><strong>postprocess</strong> (<em>torch.nn.Module</em>) – Postprocess module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.structures.detectors.YOLOV3.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.structures.detectors.YOLOV3.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<span class="target" id="module-hat.models.task_modules.fcos"></span><dl class="py class">
<dt id="hat.models.task_modules.fcos.BBoxUpscaler">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">BBoxUpscaler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">nhwc_output</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.BBoxUpscaler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>strides</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – A list contains the strides of fcos_head
output.</p></li>
<li><p><strong>nhwc_output</strong> (<em>bool</em>) – transpose output layout to nhwc.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcos.BBoxUpscaler.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">meta_data</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.BBoxUpscaler.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Do post process for model predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – Prediction tensors.</p></li>
<li><p><strong>meta_data</strong> – Meta data used in post processor, e.g. image width,
height.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcos.DynamicFcosTarget">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">DynamicFcosTarget</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">topK</span></em>, <em class="sig-param"><span class="n">loss_cls</span></em>, <em class="sig-param"><span class="n">loss_reg</span></em>, <em class="sig-param"><span class="n">cls_out_channels</span></em>, <em class="sig-param"><span class="n">background_label</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.DynamicFcosTarget" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate cls and reg targets for FCOS in training stage base on dynamic losses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>strides</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Strides of points in multiple feature levels.</p></li>
<li><p><strong>topK</strong> (<em>int</em>) – Number of postive sample for each ground trouth to keep.</p></li>
<li><p><strong>cls_out_channels</strong> (<em>int</em>) – Out_channels of cls_score.</p></li>
<li><p><strong>background_label</strong> (<em>int</em>) – Label ID of background, set as num_classes.</p></li>
<li><p><strong>loss_cls</strong> (<em>nn.Module</em>) – Loss for cls to choose positive target.</p></li>
<li><p><strong>loss_reg</strong> (<em>nn.Module</em>) – Loss for reg to choose positive target.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcos.DynamicFcosTarget.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span></em>, <em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.DynamicFcosTarget.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcos.FCOSDecoder">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">FCOSDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">transforms</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inverse_transform_key</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">nms_use_centerness</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">nms_sqrt</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">rescale</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">test_cfg</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_resize_scale</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">truncate_bbox</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">filter_score_mul_centerness</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">upscale_bbox_pred</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.FCOSDecoder" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of categories excluding the background
category.</p></li>
<li><p><strong>strides</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – A list contains the strides of fcos_head
output.</p></li>
<li><p><strong>transforms</strong> (<em>Sequence</em><em>[</em><em>dict</em><em>]</em>) – A list contains the transform config.</p></li>
<li><p><strong>inverse_transform_key</strong> (<em>Sequence</em><em>[</em><em>str</em><em>]</em>) – A list contains the inverse
transform info key.</p></li>
<li><p><strong>nms_use_centerness</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, use centerness as a
factor in nms post-processing.</p></li>
<li><p><strong>nms_sqrt</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, sqrt(score_thr * score_factors).</p></li>
<li><p><strong>rescale</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to map the prediction result to the
orig img.</p></li>
<li><p><strong>test_cfg</strong> (<em>dict</em><em>, </em><em>optional</em>) – Cfg dict, including some configurations of
nms.</p></li>
<li><p><strong>truncate_bbox</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, truncate the predictive bbox
out of image boundary. Default True.</p></li>
<li><p><strong>filter_score_mul_centerness</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, filter out bbox
by score multiply centerness, else filter out bbox by score.
Default False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcos.FCOSDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">meta_data</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.FCOSDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Do post process for model predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – Prediction tensors.</p></li>
<li><p><strong>meta_data</strong> – Meta data used in post processor, e.g. image width,
height.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcos.FCOSHead">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">FCOSHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">in_strides</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">stride2channels</span></em>, <em class="sig-param"><span class="n">upscale_bbox_pred</span></em>, <em class="sig-param"><span class="n">feat_channels</span><span class="o">=</span><span class="default_value">256</span></em>, <em class="sig-param"><span class="n">stacked_convs</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">use_sigmoid</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">share_bn</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">dequant_output</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">int8_output</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">int16_output</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">share_conv</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">deepcopy_share_conv</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.FCOSHead" title="Permalink to this definition">¶</a></dt>
<dd><p>Anchor-free head used in <cite>FCOS &lt;https://arxiv.org/abs/1904.01355&gt;</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of categories excluding the background
category.</p></li>
<li><p><strong>in_strides</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – A list contains the strides of feature
maps from backbone or neck.</p></li>
<li><p><strong>out_strides</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – A list contains the strides of this head
will output.</p></li>
<li><p><strong>stride2channels</strong> (<em>dict</em>) – A stride to channel dict.</p></li>
<li><p><strong>feat_channels</strong> (<em>int</em>) – Number of hidden channels.</p></li>
<li><p><strong>stacked_convs</strong> (<em>int</em>) – Number of stacking convs of the head.</p></li>
<li><p><strong>use_sigmoid</strong> (<em>bool</em>) – Whether the classification output is obtained
using sigmoid.</p></li>
<li><p><strong>share_bn</strong> (<em>bool</em>) – Whether to share bn between multiple levels, default
is share_bn.</p></li>
<li><p><strong>upscale_bbox_pred</strong> (<em>bool</em>) – If true, upscale bbox pred by FPN strides.</p></li>
<li><p><strong>dequant_output</strong> (<em>bool</em>) – Whether to dequant output. Default: True</p></li>
<li><p><strong>int8_output</strong> (<em>bool</em>) – If True, output int8, otherwise output int32.
Default: True</p></li>
<li><p><strong>int16_output</strong> (<em>bool</em>) – If True, output int16, otherwise output int32.
Default: True</p></li>
<li><p><strong>share_conv</strong> (<em>bool</em>) – Only the number of all stride channels is the same,
share_conv can be True, branches share conv, otherwise not.
Default: True</p></li>
<li><p><strong>deepcopy_share_conv</strong> (<em>bool</em>) – Whether use deepcopy for share conv.
Default: False</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcos.FCOSHead.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feats</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.FCOSHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hat.models.task_modules.fcos.FCOSHead.forward_single">
<code class="sig-name descname">forward_single</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">stride</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.FCOSHead.forward_single" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward features of a single scale levle.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – FPN feature maps of the specified stride.</p></li>
<li><p><strong>i</strong> (<em>int</em>) – Index of feature level.</p></li>
<li><p><strong>stride</strong> (<em>int</em>) – The corresponding stride for feature maps, only
used to upscale bbox pred when self.upscale_bbox_pred
is True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcos.FCOSTarget">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">FCOSTarget</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">regress_ranges</span></em>, <em class="sig-param"><span class="n">cls_out_channels</span></em>, <em class="sig-param"><span class="n">background_label</span></em>, <em class="sig-param"><span class="n">norm_on_bbox</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">center_sampling</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">center_sample_radius</span><span class="o">=</span><span class="default_value">1.5</span></em>, <em class="sig-param"><span class="n">use_iou_replace_ctrness</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">task_batch_list</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.FCOSTarget" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate cls and reg targets for FCOS in training stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>strides</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Strides of points in multiple feature levels.</p></li>
<li><p><strong>regress_ranges</strong> (<em>tuple</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>int</em><em>]</em><em>]</em>) – Regress range of multiple
level points.</p></li>
<li><p><strong>cls_out_channels</strong> (<em>int</em>) – Out_channels of cls_score.</p></li>
<li><p><strong>background_label</strong> (<em>int</em>) – Label ID of background, set as num_classes.</p></li>
<li><p><strong>center_sampling</strong> (<em>bool</em>) – If true, use center sampling.</p></li>
<li><p><strong>center_sample_radius</strong> – Radius of center sampling. Default: 1.5.</p></li>
<li><p><strong>norm_on_bbox</strong> (<em>bool</em>) – If true, normalize the regression targets
with FPN strides.</p></li>
<li><p><strong>use_iou_replace_ctrness</strong> (<em>bool</em>) – If true, use iou as box quality
assessment method, else use ctrness. Default: false.</p></li>
<li><p><strong>task_batch_list</strong> (<em>[</em><em>int</em><em>, </em><em>int</em><em>]</em>) – two datasets use same head, so we
generate mask</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="hat.models.task_modules.fcos.distance2bbox">
<code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">distance2bbox</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">points</span></em>, <em class="sig-param"><span class="n">distance</span></em>, <em class="sig-param"><span class="n">max_shape</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.distance2bbox" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode distance prediction to bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>points</strong> (<em>torch.Tensor</em>) – Shape (n, 2), [x, y].</p></li>
<li><p><strong>distance</strong> (<em>torch.Tensor</em>) – Distance from the given point to 4 boundaries
(left, top, right, bottom).</p></li>
<li><p><strong>max_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – Shape of the image, used to clamp decoded
bbox in max_shape range.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Decoded bbox, with shape (n, 4).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="hat.models.task_modules.fcos.get_points">
<code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">get_points</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feat_sizes</span></em>, <em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">dtype</span></em>, <em class="sig-param"><span class="n">device</span></em>, <em class="sig-param"><span class="n">flatten</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.get_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate points according to feat_sizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>feat_sizes</strong> (<em>list</em><em>[</em><em>tuple</em><em>]</em>) – Multi-level feature map sizes, the value is
the HW of a certain layer.</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em>) – Type of points should be.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device of points should be.</p></li>
<li><p><strong>flatten</strong> (<em>bool</em>) – Whether to flatten 2D coordinates into 1D dimension.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Points of multiple levels belong to each image,</dt><dd><p>the value in mlvl_points is [Tensor(H1W1, 2), Tensor(H2W2, 2), …]</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list[torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="hat.models.task_modules.fcos.multiclass_nms">
<code class="sig-prename descclassname">hat.models.task_modules.fcos.</code><code class="sig-name descname">multiclass_nms</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">multi_bboxes</span></em>, <em class="sig-param"><span class="n">multi_scores</span></em>, <em class="sig-param"><span class="n">score_thr</span></em>, <em class="sig-param"><span class="n">nms</span></em>, <em class="sig-param"><span class="n">iou_threshold</span></em>, <em class="sig-param"><span class="n">max_per_img</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">score_factors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">nms_sqrt</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">filter_score_mul_centerness</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcos.multiclass_nms" title="Permalink to this definition">¶</a></dt>
<dd><p>NMS for multi-class bboxes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>multi_bboxes</strong> (<em>Tensor</em>) – shape (n, #class*4) or (n, 4)</p></li>
<li><p><strong>multi_scores</strong> (<em>Tensor</em>) – shape (n, #class), where the last column
contains scores of the background class, but this will be ignored.</p></li>
<li><p><strong>score_thr</strong> (<em>float</em>) – bbox threshold, bboxes with scores lower than it
will not be considered.</p></li>
<li><p><strong>nms</strong> (<em>str</em>) – nms type, candidate values are [‘nms’, ‘soft_nms’].</p></li>
<li><p><strong>iou_threshold</strong> (<em>float</em>) – NMS IoU threshold</p></li>
<li><p><strong>max_per_img</strong> (<em>int</em>) – if there are more than max_num bboxes after NMS,
only top max_num will be kept.</p></li>
<li><p><strong>score_factors</strong> (<em>Tensor</em>) – The factors multiplied to scores before
applying NMS</p></li>
<li><p><strong>nms_sqrt</strong> (<em>bool</em>) – If True, sqrt(score_thr * score_factors)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-hat.models.task_modules.seg"></span><dl class="py class">
<dt id="hat.models.task_modules.seg.FRCNNSegHead">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.seg.</code><code class="sig-name descname">FRCNNSegHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">group_base</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">in_strides</span><span class="p">:</span> <span class="n">List</span></em>, <em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">List</span></em>, <em class="sig-param"><span class="n">out_strides</span><span class="p">:</span> <span class="n">List</span></em>, <em class="sig-param"><span class="n">out_channels</span><span class="p">:</span> <span class="n">List</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">Dict</span></em>, <em class="sig-param"><span class="n">with_extra_conv</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">linear_out</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">argmax_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">dequant_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">int8_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.seg.FRCNNSegHead" title="Permalink to this definition">¶</a></dt>
<dd><p>FRCNNSegHead module for segmentation task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>group_base</strong> (<em>int</em>) – Group base of group conv</p></li>
<li><p><strong>in_strides</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – The strides corresponding to the inputs of
seg_head, the inputs usually come from backbone or neck.</p></li>
<li><p><strong>in_channels</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – Number of channels of each input stride.</p></li>
<li><p><strong>out_strides</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – List of output strides.</p></li>
<li><p><strong>out_channels</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – Number of channels of each output stride.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Extra keyword arguments for bn layers.</p></li>
<li><p><strong>with_extra_conv</strong> (<em>bool</em>) – Whether to use extra conv module.</p></li>
<li><p><strong>use_bias</strong> (<em>bool</em>) – Whether to use bias in conv module.</p></li>
<li><p><strong>linear_out</strong> (<em>bool</em>) – Whether NOT to use to act of pw.</p></li>
<li><p><strong>argmax_output</strong> (<em>bool</em>) – Whether conduct argmax on output.</p></li>
<li><p><strong>dequant_output</strong> (<em>bool</em>) – Whether to dequant output.</p></li>
<li><p><strong>int8_output</strong> (<em>bool</em>) – If True, output int8, otherwise output int32.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.seg.FRCNNSegHead.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="headerlink" href="#hat.models.task_modules.seg.FRCNNSegHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.seg.SegDecoder">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.seg.</code><code class="sig-name descname">SegDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">decode_strides</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">upscale_times</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">transforms</span><span class="p">:</span> <span class="n">List<span class="p">[</span>dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inverse_transform_key</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_names</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'pred_seg'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.seg.SegDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Semantic Segmentation Decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_strides</strong> – List of output strides, represents the strides
of the output from seg_head.</p></li>
<li><p><strong>output_names</strong> – Keys of returned results dict.</p></li>
<li><p><strong>decode_strides</strong> – Strides that need to be decoded,
should be a subset of out_strides.</p></li>
<li><p><strong>upscale_times</strong> – Bilinear upscale times for each
decode stride, default to None, which means same as decode stride.</p></li>
<li><p><strong>transforms</strong> – A list contains the transform config.</p></li>
<li><p><strong>inverse_transform_key</strong> – A list contains the inverse
transform info key.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.seg.SegHead">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.seg.</code><code class="sig-name descname">SegHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span></em>, <em class="sig-param"><span class="n">in_strides</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">stride2channels</span></em>, <em class="sig-param"><span class="n">feat_channels</span><span class="o">=</span><span class="default_value">256</span></em>, <em class="sig-param"><span class="n">stride_loss_weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stacked_convs</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">argmax_output</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">dequant_output</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">int8_output</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">upscale</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">upscale_stride</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">output_with_bn</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">upsample_output_scale</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.seg.SegHead" title="Permalink to this definition">¶</a></dt>
<dd><p>Head Module for segmentation task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of classes.</p></li>
<li><p><strong>in_strides</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – The strides corresponding to the inputs of
seg_head, the inputs usually come from backbone or neck.</p></li>
<li><p><strong>out_strides</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – List of output strides.</p></li>
<li><p><strong>stride2channels</strong> (<em>dict</em>) – A stride to channel dict.</p></li>
<li><p><strong>feat_channels</strong> (<em>int</em><em> or </em><em>list</em><em>[</em><em>int</em><em>]</em>) – Number of hidden channels (of each output stride).</p></li>
<li><p><strong>stride_loss_weights</strong> (<em>list</em><em>[</em><em>int</em><em>]</em>) – loss weight of each stride.</p></li>
<li><p><strong>stacked_convs</strong> (<em>int</em>) – Number of stacking convs of head.</p></li>
<li><p><strong>argmax_output</strong> (<em>bool</em>) – Whether conduct argmax on output. Default: False</p></li>
<li><p><strong>dequant_output</strong> (<em>bool</em>) – Whether to dequant output. Default: True</p></li>
<li><p><strong>int8_output</strong> (<em>bool</em>) – If True, output int8, otherwise output int32.
Default: True</p></li>
<li><p><strong>upscale</strong> (<em>bool</em>) – If True, stride{x}’s feature map
is upsampled by 2x, then the upsampled feature is adding
supervisory signal. Default is False.</p></li>
<li><p><strong>upscale_stride</strong> (<em>int</em>) – Specify which stride’s feature need to
be upsampled when upscale is True.</p></li>
<li><p><strong>output_with_bn</strong> (<em>bool</em>) – Whether add bn layer to the output conv.</p></li>
<li><p><strong>bn_kwargs</strong> (<em>dict</em>) – Extra keyword arguments for bn layers.</p></li>
<li><p><strong>upsample_output_scale</strong> (<em>int</em>) – Output upsample scale, only used in
qat model, default is None.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.seg.SegHead.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feats</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.seg.SegHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="hat.models.task_modules.seg.SegHead.forward_single">
<code class="sig-name descname">forward_single</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">stride_index</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.seg.SegHead.forward_single" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward features of a single scale level.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – feature maps of the specified stride.</p></li>
<li><p><strong>stride_index</strong> (<em>int</em>) – stride index of input feature map.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>seg predictions of input feature maps.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.seg.SegTarget">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.seg.</code><code class="sig-name descname">SegTarget</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ignore_index</span><span class="o">=</span><span class="default_value">255</span></em>, <em class="sig-param"><span class="n">label_name</span><span class="o">=</span><span class="default_value">'gt_seg'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.seg.SegTarget" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate training targets for Seg task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ignore_index</strong> (<em>int</em><em>, </em><em>optional</em>) – Index of ignore class.</p></li>
<li><p><strong>label_name</strong> (<em>str</em><em>, </em><em>optional</em>) – The key corresponding to the gt seg in
label.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-hat.models.task_modules.deeplab"></span><dl class="py class">
<dt id="hat.models.task_modules.deeplab.Deeplabv3plusHead">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.deeplab.</code><code class="sig-name descname">Deeplabv3plusHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">c1_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">c1_in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">feat_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">dilations</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">num_repeats</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">argmax_output</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">dequant_output</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">int8_output</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dropout_ratio</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">upsample_output_scale</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">upsample_decode_scale</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">4</span></em>, <em class="sig-param"><span class="n">bias</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.deeplab.Deeplabv3plusHead" title="Permalink to this definition">¶</a></dt>
<dd><p>Head Module for FCN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> – Input channels.</p></li>
<li><p><strong>c1_index</strong> – Index for c1 input.</p></li>
<li><p><strong>c1_in_channels</strong> – In channels of c1.</p></li>
<li><p><strong>feat_channels</strong> – Channels for the module.</p></li>
<li><p><strong>num_classes</strong> – Number of classes.</p></li>
<li><p><strong>dilations</strong> – List of dilations for aspp.</p></li>
<li><p><strong>num_repeats</strong> – List of repeat for each branch of ASPP.</p></li>
<li><p><strong>argmax_output</strong> – Whether conduct argmax on output. Default: False.</p></li>
<li><p><strong>dequant_output</strong> – Whether to dequant output. Default: True</p></li>
<li><p><strong>int8_output</strong> – If True, output int8, otherwise output int32.
Default: False.</p></li>
<li><p><strong>bn_kwargs</strong> – Extra keyword arguments for bn layers. Default: None.</p></li>
<li><p><strong>dropout_ratio</strong> – Ratio for dropout during training. Default: 0.1.</p></li>
<li><p><strong>upsample_decode_scale</strong> – upsample scale to c1. Default is 4.</p></li>
<li><p><strong>upsample_output_scale</strong> – Output upsample scale, only used in
qat model, default is None.</p></li>
<li><p><strong>bias</strong> – Whether has bias. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.deeplab.Deeplabv3plusHead.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.deeplab.Deeplabv3plusHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<span class="target" id="module-hat.models.task_modules.fcn"></span><dl class="py class">
<dt id="hat.models.task_modules.fcn.DepthwiseSeparableFCNHead">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcn.</code><code class="sig-name descname">DepthwiseSeparableFCNHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_channels</span></em>, <em class="sig-param"><span class="n">feat_channels</span></em>, <em class="sig-param"><span class="n">num_convs</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcn.DepthwiseSeparableFCNHead" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcn.FCNDecoder">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcn.</code><code class="sig-name descname">FCNDecoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">upsample_output_scale</span><span class="o">=</span><span class="default_value">8</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcn.FCNDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>FCN Decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>upsample_output_scale</strong> – Output upsample scale. Default: 8.</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcn.FCNDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcn.FCNDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcn.FCNHead">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcn.</code><code class="sig-name descname">FCNHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">in_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">feat_channels</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">dropout_ratio</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">int8_output</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">argmax_output</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">dequant_output</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">upsample_output_scale</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_convs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">2</span></em>, <em class="sig-param"><span class="n">bn_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcn.FCNHead" title="Permalink to this definition">¶</a></dt>
<dd><p>Head Module for FCN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_index</strong> – Index of inputs.</p></li>
<li><p><strong>in_channels</strong> – Input channels.</p></li>
<li><p><strong>feat_channels</strong> – Channels for the module.</p></li>
<li><p><strong>num_classes</strong> – Number of classes.</p></li>
<li><p><strong>dropout_ratio</strong> – Ratio for dropout during training. Default: 0.1.</p></li>
<li><p><strong>int8_output</strong> – If True, output int8, otherwise output int32.
Default: False.</p></li>
<li><p><strong>argmax_output</strong> – Whether conduct argmax on output. Default: False.</p></li>
<li><p><strong>dequant_output</strong> – Whether to dequant output. Default: True.</p></li>
<li><p><strong>upsample_output_scale</strong> – Output upsample scale. Default: None.</p></li>
<li><p><strong>num_convs</strong> – number of convs in head. Default: 2.</p></li>
<li><p><strong>bn_kwargs</strong> – Extra keyword arguments for bn layers. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcn.FCNHead.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcn.FCNHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="hat.models.task_modules.fcn.FCNTarget">
<em class="property">class </em><code class="sig-prename descclassname">hat.models.task_modules.fcn.</code><code class="sig-name descname">FCNTarget</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">19</span></em><span class="sig-paren">)</span><a class="headerlink" href="#hat.models.task_modules.fcn.FCNTarget" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate Target for FCN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_classes</strong> – Number of classes. Defualt: 19.</p>
</dd>
</dl>
<dl class="py method">
<dt id="hat.models.task_modules.fcn.FCNTarget.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; dict<a class="headerlink" href="#hat.models.task_modules.fcn.FCNTarget.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>label</strong> – data Tenser.(n, h, w)</p></li>
<li><p><strong>pred</strong> – Output Tenser. (n, c, h, w).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss inputs.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="metrics.html" class="btn btn-neutral float-right" title="hat.metrics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="engine.html" class="btn btn-neutral float-left" title="hat.engine" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, HAT Developers.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>